{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "af842a4b-30e4-469d-b315-e5405730df14",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af842a4b-30e4-469d-b315-e5405730df14",
        "outputId": "27b5603e-40f4-41f7-ef12-76808f966178"
      },
      "outputs": [],
      "source": [
        "# !pip install einops torch maze-dataset --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cfa8f76c-2435-45e0-92af-e1c548bf0390",
      "metadata": {
        "id": "cfa8f76c-2435-45e0-92af-e1c548bf0390"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from torch.nn import functional as F\n",
        "\n",
        "now = datetime.now()\n",
        "formatted_time = now.strftime(\"%Y%m%d_%H%M%S\")\n",
        "loss_curves_folder = f\"../data/loss_curves_{formatted_time}/stable_diffusion_diffusion_models\"\n",
        "if not os.path.exists(loss_curves_folder):\n",
        "    os.makedirs(loss_curves_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bf0f5bb1-2c86-4f19-bafe-7b7bec3a57ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf0f5bb1-2c86-4f19-bafe-7b7bec3a57ff",
        "outputId": "5d16aa39-38b8-429f-f379-7d699ed534cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /Users/I749793/Desktop/NUS/CS5340 - Uncertainty Modeling in AI/project/diffusion-based-environment-generator\n"
          ]
        }
      ],
      "source": [
        "os.chdir(\"..\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "543d1c3c-8f5e-403d-b9a1-ce031a9587c7",
      "metadata": {
        "id": "543d1c3c-8f5e-403d-b9a1-ce031a9587c7"
      },
      "source": [
        "## Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e7448583-863f-4043-9e54-c36788bf8b30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7448583-863f-4043-9e54-c36788bf8b30",
        "outputId": "3833d816-19ce-4bde-fce1-8c6d53584d7a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from generator.maze.grid_world_generator import generate_multiple_grid_worlds\n",
        "from generator.maze.solvers.a_star_l1 import main as a_star_l1_paths\n",
        "from generator.maze.solvers.bfs import main as bfs_paths\n",
        "\n",
        "# parent_directory = \"./data\"\n",
        "# if not os.path.exists(parent_directory):\n",
        "#     os.makedirs(parent_directory)\n",
        "# # generates the mazes\n",
        "# mazes = generate_multiple_grid_worlds(num_worlds=25000, parent_directory=parent_directory)\n",
        "# # generate path travrsals\n",
        "# a_star_l1_paths(parent_directory)\n",
        "# bfs_paths(parent_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6407b36e-2b1a-49da-8fda-dc83f7bb3481",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "6407b36e-2b1a-49da-8fda-dc83f7bb3481",
        "outputId": "e65862df-98a3-4d88-9261-c7466ce34052"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def preprocess_image(image, target_size=32):\n",
        "#     image = np.array(image)\n",
        "\n",
        "#     scale_factor = target_size // image.shape[0]\n",
        "#     image = np.kron(image, np.ones((scale_factor, scale_factor, 1)))\n",
        "\n",
        "#     image = image.astype(np.float32) / 127.5 - 1\n",
        "#     image = torch.tensor(image).permute(2, 0, 1)\n",
        "#     return image\n",
        "\n",
        "# def load_dataset_from_npy(directory=\"./data\", target_size=32):\n",
        "#     images = []\n",
        "#     path_lengths = []\n",
        "\n",
        "#     files = sorted([f for f in os.listdir(directory) if f.endswith(\".npy\")])\n",
        "\n",
        "#     for file in files:\n",
        "#         img = np.load(os.path.join(directory, file))\n",
        "\n",
        "#         mask = np.all(img == [0, 0, 255], axis=-1)\n",
        "#         img[mask] = [255, 255, 255]\n",
        "#         img = img[:-1, :-1]\n",
        "\n",
        "#         image = preprocess_image(img, target_size)\n",
        "\n",
        "#         base_name = os.path.splitext(file)[0]\n",
        "#         len_filename = base_name + \"_len.txt\"\n",
        "#         len_path = os.path.join(directory, len_filename)\n",
        "\n",
        "#         with open(len_path, \"r\") as f:\n",
        "#             maze_length = int(f.read().strip())\n",
        "\n",
        "#         images.append(image)\n",
        "#         path_lengths.append(maze_length)\n",
        "\n",
        "#     return images, path_lengths\n",
        "\n",
        "# images, path_lengths = load_dataset_from_npy(\"./data\", target_size=32)\n",
        "\n",
        "# plt.imshow(images[0].permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "83a74713",
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image(image, target_size=32):\n",
        "    image = np.array(image)\n",
        "    scale_factor = target_size // image.shape[0] \n",
        "    # image = np.kron(image, np.ones((scale_factor, scale_factor, 1))) \n",
        "    \n",
        "    # image = image.astype(np.float32) / 127.5 - 1\n",
        "    image = image.astype(np.float32)\n",
        "    image = torch.tensor(image).permute(2, 0, 1)\n",
        "    image = F.interpolate(image.unsqueeze(0), size=(target_size, target_size), mode='nearest').squeeze(0)  # (3, 32, 32)\n",
        "\n",
        "    return image\n",
        "\n",
        "def plot_grid_world(grid):\n",
        "    \"\"\"\n",
        "    Plots the given grid world.\n",
        "    \"\"\"\n",
        "    wall = grid[:,:,0] == 0\n",
        "    source = grid[:,:,1] == 1\n",
        "    destination = grid[:,:,2] == 1\n",
        "\n",
        "    img = np.ones((*wall.shape, 3), dtype=np.float32)  # White background\n",
        "    img[wall] = np.array([0, 0, 0])  # Walls → Black\n",
        "    img[source] = np.array([1, 0, 0])  # Source → Red\n",
        "    img[destination] = np.array([0, 1, 0])  # Destination → Green\n",
        "\n",
        "    return img\n",
        "\n",
        "def load_dataset_from_npy(parent_directory=\"./data\", target_size=32):\n",
        "    images = []\n",
        "    path_lengths = []\n",
        "    num_nodes_traversed_astar = []\n",
        "    num_nodes_traversed_bfs = []\n",
        "    \n",
        "    mazes_directory = os.path.join(parent_directory, \"mazes\")\n",
        "    files = sorted([f for f in os.listdir(mazes_directory) if f.endswith(\".npy\")])\n",
        "    \n",
        "    for file in files:\n",
        "        img = np.load(os.path.join(mazes_directory, file))\n",
        "        if(img.shape != (10,10,3)):\n",
        "            continue\n",
        "        # mask = np.all(img == [0, 0, 255], axis=-1)\n",
        "        # img[mask] = [255, 255, 255]\n",
        "        # img = img[:-1, :-1]\n",
        "        # image = preprocess_image(img, target_size)\n",
        "\n",
        "        image = plot_grid_world(img)\n",
        "        mask = np.all(image == [0, 0, 255], axis=-1)\n",
        "        image[mask] = [255, 255, 255]\n",
        "        image = preprocess_image(image, target_size)\n",
        "\n",
        "        pattern = r'maze_(\\d+)'\n",
        "        match = re.search(pattern, file)\n",
        "        num = 0\n",
        "        if match:\n",
        "            num = int(match.group(1))\n",
        "        else:\n",
        "            continue\n",
        "        \n",
        "        # base_name = os.path.splitext(file)[0]\n",
        "        # len_filename = base_name + \"_len.txt\"\n",
        "        len_filename = f\"path_length_{num}\" + \".npy\"\n",
        "        len_path = os.path.join(mazes_directory, len_filename)\n",
        "        astar_traversal_filename = f\"a_star_{num}\" + \".npy\"\n",
        "        astar_traversal_path = os.path.join(parent_directory, \"a_star_l1_results\" ,astar_traversal_filename)\n",
        "        bfs_traversal_filename = f\"bfs_{num}\" + \".npy\"\n",
        "        bfs_traversal_path = os.path.join(parent_directory, \"bfs_results\" ,bfs_traversal_filename)\n",
        "        \n",
        "        # with open(len_path, \"r\") as f:\n",
        "        #     maze_length = int(f.read().strip())\n",
        "        maze_length = np.load(len_path)\n",
        "        astar_traversal = np.load(astar_traversal_path)\n",
        "        bfs_traversal = np.load(bfs_traversal_path)\n",
        "        \n",
        "        images.append(image)\n",
        "        path_lengths.append(int(maze_length))\n",
        "        num_nodes_traversed_astar.append(int(astar_traversal))\n",
        "        num_nodes_traversed_bfs.append(int(bfs_traversal))\n",
        "    \n",
        "    return images, path_lengths, num_nodes_traversed_astar, num_nodes_traversed_bfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "73f89a5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "mazes_data_path = \"./data/\"\n",
        "images, org_path_lengths, num_nodes_astar, num_nodes_bfs = load_dataset_from_npy(mazes_data_path, target_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9364d0ee",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(images[0].permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e2a04378-6293-4c79-95d0-fa6e305cba28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2a04378-6293-4c79-95d0-fa6e305cba28",
        "outputId": "1e1d158e-a42a-453d-ea90-ff6a716979fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /Users/I749793/Desktop/NUS/CS5340 - Uncertainty Modeling in AI/project/diffusion-based-environment-generator/diffuser\n"
          ]
        }
      ],
      "source": [
        "# print(f\"Current working directory: {os.getcwd()}\")\n",
        "os.chdir(\"./diffuser\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "742a775a-18e1-4d73-a304-0d951dfc05c2",
      "metadata": {
        "id": "742a775a-18e1-4d73-a304-0d951dfc05c2",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### VAE Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0b8328c6-fdf1-4599-9140-b1ee9c6cc029",
      "metadata": {
        "id": "0b8328c6-fdf1-4599-9140-b1ee9c6cc029"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "335fe85b-942c-4707-89fe-fd5e10b8214d",
      "metadata": {
        "id": "335fe85b-942c-4707-89fe-fd5e10b8214d"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 25\n",
        "LATENT_CHANNELS = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f1bd601d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# class MazeTensorDataset(Dataset):\n",
        "#     def __init__(self, images, path_lengths, num_nodes_astar, num_nodes_bfs):\n",
        "#         self.images = images\n",
        "#         self.path_lengths = path_lengths\n",
        "#         self.num_nodes_astar = num_nodes_astar\n",
        "#         self.num_nodes_bfs = num_nodes_bfs\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return len(self.images)\n",
        "    \n",
        "#     def __getitem__(self, idx):\n",
        "#         return self.images[idx], self.path_lengths[idx], self.num_nodes_astar[idx], self.num_nodes_bfs[idx]\n",
        "    \n",
        "class MazeTensorDataset(Dataset):\n",
        "    def __init__(self, images, path_lengths, num_nodes_astar, num_nodes_bfs):\n",
        "        self.images = images\n",
        "        self.path_lengths = path_lengths\n",
        "        self.num_nodes_astar = num_nodes_astar\n",
        "        self.num_nodes_bfs = num_nodes_bfs\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        prompt = \"a maze with path length \" + str(self.path_lengths[idx]) + \", and number of nodes traversed using astar algorithm \" + str(self.num_nodes_astar[idx])\n",
        "        return self.images[idx], prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5984963c-d4d5-402c-bae4-4b79276425f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5984963c-d4d5-402c-bae4-4b79276425f3",
        "outputId": "0c7d727d-5507-4e0b-c336-168d6ced665b"
      },
      "outputs": [],
      "source": [
        "# print(\"Total images:\", len(images))\n",
        "# print(\"Total path_lengths:\", len(path_lengths))\n",
        "\n",
        "# total = len(images)\n",
        "# test_size = int(0.2 * total)\n",
        "# all_indices = list(range(total))\n",
        "# random.shuffle(all_indices)\n",
        "\n",
        "# test_indices = all_indices[:test_size]\n",
        "# train_indices = all_indices[test_size:]\n",
        "\n",
        "# train_images = [images[i] for i in train_indices]\n",
        "# train_path_lengths = [path_lengths[i] for i in train_indices]\n",
        "\n",
        "# test_images = [images[i] for i in test_indices]\n",
        "# test_path_lengths = [path_lengths[i] for i in test_indices]\n",
        "\n",
        "# dataset = MazeTensorDataset(train_images, train_path_lengths)\n",
        "# test_dataset = MazeTensorDataset(test_images, test_path_lengths)\n",
        "\n",
        "# print(\"Train dataset length:\", len(dataset))\n",
        "# print(\"Test dataset length:\", len(test_dataset))\n",
        "\n",
        "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# unique_train_paths = set(train_path_lengths)\n",
        "# print(\"Unique training path lengths:\", unique_train_paths)\n",
        "# print(\"Number of unique training paths:\", len(unique_train_paths))\n",
        "\n",
        "# unique_test_paths = set(test_path_lengths)\n",
        "# print(\"Unique test path lengths:\", unique_test_paths)\n",
        "# print(\"Number of unique test paths:\", len(unique_test_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "960b6cda",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total images: 1000\n",
            "Total path_lengths: 1000\n",
            "Train dataset length: 800\n",
            "Test dataset length: 200\n",
            "Unique training path lengths: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20}\n",
            "Number of unique training paths: 19\n",
            "Unique test path lengths: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 19, 20, 22}\n",
            "Number of unique test paths: 19\n"
          ]
        }
      ],
      "source": [
        "path_lengths = org_path_lengths\n",
        "\n",
        "## use the scaled path lengths based on the number of nodes traversed\n",
        "# path_lengths = [a / b if b!=0 else a for a, b in zip(path_lengths, num_nodes_astar)]\n",
        "# path_lengths = [a / b if b!=0 else a for a, b in zip(path_lengths, num_nodes_bfs)]\n",
        "\n",
        "print(\"Total images:\", len(images))\n",
        "print(\"Total path_lengths:\", len(path_lengths))\n",
        "\n",
        "total = len(images)\n",
        "test_size = int(0.2 * total)\n",
        "all_indices = list(range(total))\n",
        "random.shuffle(all_indices)\n",
        "\n",
        "test_indices = all_indices[:test_size]\n",
        "train_indices = all_indices[test_size:]\n",
        "\n",
        "train_images = [images[i] for i in train_indices]\n",
        "train_path_lengths = [path_lengths[i] for i in train_indices]\n",
        "train_num_nodes_astar = [num_nodes_astar[i] for i in train_indices]\n",
        "train_num_nodes_bfs = [num_nodes_bfs[i] for i in train_indices]\n",
        "\n",
        "test_images = [images[i] for i in test_indices]\n",
        "test_path_lengths = [path_lengths[i] for i in test_indices]\n",
        "test_num_nodes_astar = [num_nodes_astar[i] for i in test_indices]\n",
        "test_num_nodes_bfs = [num_nodes_bfs[i] for i in test_indices]\n",
        "\n",
        "dataset = MazeTensorDataset(train_images, train_path_lengths, train_num_nodes_astar, train_num_nodes_bfs)\n",
        "test_dataset = MazeTensorDataset(test_images, test_path_lengths, test_num_nodes_astar, test_num_nodes_bfs)\n",
        "\n",
        "print(\"Train dataset length:\", len(dataset))\n",
        "print(\"Test dataset length:\", len(test_dataset))\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "unique_train_paths = set(train_path_lengths)\n",
        "print(\"Unique training path lengths:\", unique_train_paths)\n",
        "print(\"Number of unique training paths:\", len(unique_train_paths))\n",
        "\n",
        "unique_test_paths = set(test_path_lengths)\n",
        "print(\"Unique test path lengths:\", unique_test_paths)\n",
        "print(\"Number of unique test paths:\", len(unique_test_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0e98f779",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import safetensors\n",
        "# print(safetensors.__file__)\n",
        "\n",
        "# from safetensors.torch import save_file, load_file\n",
        "# print(\"Success!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "37e80c38",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from diffusers import AutoencoderKL\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load Stable Diffusion's pre-trained VAE\n",
        "vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\").to(device)\n",
        "# vae_checkpoint = torch.load(\"/Users/I749793/Desktop/NUS/CS5340 - Uncertainty Modeling in AI/project/diffusion-based-environment-generator/data/stablediffusion_weights.pth\", map_location=device)  # or \"cuda\" if you want\n",
        "vae_checkpoint = torch.load(\"../data/vae_models/stablediffusion_vae_weights.pth\", map_location=device)  # or \"cuda\" if you want\n",
        "vae_state_dict = vae_checkpoint['diffusion_state_dict']\n",
        "vae.load_state_dict(vae_state_dict)\n",
        "# optimizer = optim.Adam(vae.parameters(), lr=1e-4)\n",
        "# criterion = nn.MSELoss()\n",
        "# loading pretrained vae instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "qd7ioKYDO15-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd7ioKYDO15-",
        "outputId": "00753a60-c6d7-4d6b-c878-343ffc7c2bb2"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "# ---- Training Loop ---- #\n",
        "def train_vae(dataloader, epochs=50):\n",
        "    vae.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for i, (images, _, _, _) in enumerate(dataloader):\n",
        "            images = images.to(device)  # Input maze images (shape: B x 3 x 256 x 256)\n",
        "\n",
        "            # Forward pass\n",
        "            latent_dist = vae.encode(images).latent_dist\n",
        "            latent_sample = latent_dist.sample()\n",
        "            reconstructed_images = vae.decode(latent_sample).sample\n",
        "\n",
        "            # Loss computation\n",
        "            loss = criterion(reconstructed_images, images)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.6f}\")\n",
        "\n",
        "# train_vae(dataloader, epochs=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "Y7ye83uDSOUQ",
      "metadata": {
        "id": "Y7ye83uDSOUQ"
      },
      "outputs": [],
      "source": [
        "# torch.save({\n",
        "#     'diffusion_state_dict': vae.state_dict(),\n",
        "#     'optimizer_state_dict': optimizer.state_dict(),\n",
        "#     'train_losses': train_losses\n",
        "# }, 'diffusion_weights.pth')\n",
        "# print(\"Diffusion weights saved to diffusion_weights.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c87ee58e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Save Model Weights\n",
        "# if not os.path.exists(f\"../data/loss_curves_{formatted_time}/stable_diffusion_vae_models/\"):\n",
        "#     os.makedirs(f\"../data/loss_curves_{formatted_time}/stable_diffusion_vae_models/\")\n",
        "# torch.save({\n",
        "#     'vae_state_dict': vae.state_dict(),\n",
        "#     'optimizer_state_dict': optimizer.state_dict(),\n",
        "#     'train_losses': train_losses,\n",
        "# }, f'../data/loss_curves_{formatted_time}/stable_diffusion_vae_models/vae_weights_multi_feat.pth')\n",
        "\n",
        "# print(\"Model weights saved to vae_weights_multi_feat.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd28e29c",
      "metadata": {},
      "source": [
        "### VAE Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "SlkhuVYMSspV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "SlkhuVYMSspV",
        "outputId": "dc812d14-0cd7-4948-e9dc-6bc2fe0d00df"
      },
      "outputs": [],
      "source": [
        "# vae.eval()\n",
        "# def generate_maze_from_test(sample_idx=None):\n",
        "\n",
        "#     if sample_idx is None:\n",
        "#         sample_idx = random.randint(0, len(test_dataset) - 1)\n",
        "\n",
        "#     test_img, test_path_length, _, _ = test_dataset[sample_idx]  # Load test maze\n",
        "#     test_img = test_img.unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "#     # Encode test maze image into latent space\n",
        "#     with torch.no_grad():\n",
        "#         latent_dist = vae.encode(test_img).latent_dist\n",
        "#         latent_sample = latent_dist.sample()  # Sample from the distribution\n",
        "\n",
        "#         # Decode latent back to an image\n",
        "#         generated_image = vae.decode(latent_sample).sample\n",
        "\n",
        "#     return generated_image, test_img, test_path_length\n",
        "\n",
        "# # Run test generation\n",
        "# generated, original, test_path_length = generate_maze_from_test()\n",
        "\n",
        "# # ---- Visualization ---- #\n",
        "# plt.figure(figsize=(10, 5))\n",
        "\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.imshow(generated.squeeze(0).permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "# plt.title(f\"Generated Maze (Path Length: {test_path_length})\")\n",
        "# plt.axis(\"off\")\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.imshow(original.squeeze(0).permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "# plt.title(\"Original Test Maze\")\n",
        "# plt.axis(\"off\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1821b8bd",
      "metadata": {},
      "source": [
        "# Diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "848710e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import UNet2DConditionModel, DDIMScheduler\n",
        "from peft import LoraConfig\n",
        "from model import Diffusion as CustomDiffusionModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f567312",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparams\n",
        "NUM_TIMESTEPS = 1000\n",
        "GUIDANCE_SCALE = 7.5\n",
        "LATENT_SHAPE = (4, 64, 64)  # For SD 1.5\n",
        "EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "62efeb99",
      "metadata": {},
      "outputs": [],
      "source": [
        "unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\").to(device)\n",
        "unet.requires_grad_(False)\n",
        "lora_config = LoraConfig(\n",
        "        r=4,\n",
        "        lora_alpha=16,\n",
        "        init_lora_weights=\"gaussian\",\n",
        "        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
        "    )\n",
        "unet.add_adapter(lora_config)\n",
        "lora_layers = filter(lambda p: p.requires_grad, unet.parameters())\n",
        "optimizer = torch.optim.Adam(\n",
        "    lora_layers, lr=1e-5, betas=(0.9, 0.999), weight_decay=1e-2, eps=1e-08)\n",
        "scheduler = DDIMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
        "custom_diffusion_model = CustomDiffusionModel(input_size=2).to(device)\n",
        "scheduler.set_timesteps(NUM_TIMESTEPS)\n",
        "vae.eval()\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", subfolder=\"tokenizer\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", subfolder=\"text_encoder\").to(device)\n",
        "# optimizer = torch.optim.Adam(list(unet.parameters()), lr=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aedc139",
      "metadata": {},
      "source": [
        "### Stable Diffusion Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c19b54",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training epoch [1/1]: 100%|██████████| 7/7 [03:54<00:00, 33.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Loss: 0.947589\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0.0\n",
        "    count = 0\n",
        "\n",
        "    # for batch_idx, (images, path_lengths, nodes_astar, nodes_bfs) in enumerate(tqdm(dataloader, desc=f\"Training epoch [{epoch+1}/{EPOCHS}]\")):\n",
        "    for batch_idx, (images, prompt) in enumerate(tqdm(dataloader, desc=f\"Training epoch [{epoch+1}/{EPOCHS}]\")):\n",
        "        unet.train()\n",
        "        images = images.to(device)\n",
        "        # path_lengths = path_lengths.float().to(device)\n",
        "        # nodes_astar = nodes_astar.float().to(device)\n",
        "\n",
        "        # Step 1: Encode images to latents using VAE\n",
        "        with torch.no_grad():\n",
        "            z = vae.encode(images).latent_dist.sample()  # shape: (B, 4, 64, 64)\n",
        "\n",
        "        # Step 2: Sample timesteps & add noise\n",
        "        timesteps = torch.randint(0, NUM_TIMESTEPS, (z.size(0),), device=device).long()\n",
        "        noise = torch.randn_like(z)\n",
        "        noisy_z = scheduler.add_noise(z, noise, timesteps)\n",
        "\n",
        "        # # Step 3: Prepare conditional and unconditional embeddings\n",
        "        # cond_input = torch.stack([path_lengths, nodes_astar], dim=-1)\n",
        "        # cond_embed = custom_diffusion_model.condition_multidimensional_embedding(cond_input)\n",
        "   \n",
        "        # zero_input = torch.zeros_like(cond_input)\n",
        "        # uncond_embed = custom_diffusion_model.condition_multidimensional_embedding(zero_input)\n",
        "\n",
        "        # context = torch.cat([uncond_embed, cond_embed], dim=0)       \n",
        "\n",
        "        inputs = tokenizer(prompt, padding=\"max_length\", max_length=77, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            cond_embed = text_encoder(**inputs).last_hidden_state\n",
        "\n",
        "        # For unconditional context\n",
        "        uncond_prompt = [\"\"] * images.size(0)\n",
        "        uncond_inputs = tokenizer(uncond_prompt, padding=\"max_length\", max_length=77, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            uncond_embed = text_encoder(**uncond_inputs).last_hidden_state\n",
        "\n",
        "        # Combine for classifier-free guidance\n",
        "        context = torch.cat([uncond_embed, cond_embed], dim=0)\n",
        "\n",
        "        # Step 4: Forward pass with classifier-free guidance\n",
        "        noisy_z = noisy_z.repeat(2, 1, 1, 1)                          \n",
        "        timesteps = timesteps.repeat_interleave(2)                   \n",
        "\n",
        "        noise_pred = unet(noisy_z, timesteps, encoder_hidden_states=context).sample\n",
        "        noise_uncond, noise_cond = noise_pred.chunk(2)\n",
        "\n",
        "        guided_noise = noise_uncond + GUIDANCE_SCALE * (noise_cond - noise_uncond)\n",
        "\n",
        "        # Step 5: Loss and optimization\n",
        "        # loss = F.smooth_l1_loss(guided_noise, noise)\n",
        "        loss = F.mse_loss(guided_noise, noise)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item() * images.size(0)\n",
        "        count += images.size(0)\n",
        "\n",
        "    avg_loss = epoch_loss / count\n",
        "    train_losses.append(avg_loss)\n",
        "    print(f\"[Epoch {epoch+1}] Loss: {avg_loss:.6f}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, EPOCHS + 1), train_losses, marker='o', label='Train Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Graph')\n",
        "plt.legend()\n",
        "file_name = f\"loss_curve_diffusion_multi_feat_{formatted_time}\"\n",
        "plt.savefig(os.path.join(loss_curves_folder, file_name))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "96439c5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline, DiffusionPipeline\n",
        "from diffusers.utils import convert_state_dict_to_diffusers\n",
        "from peft import get_peft_model_state_dict\n",
        "\n",
        "if not os.path.exists(f\"../data/loss_curves_{formatted_time}/lora/\"):\n",
        "    os.makedirs(f\"../data/loss_curves_{formatted_time}/lora/\")\n",
        "# if not os.path.exists(f\"../data/lora_test/lora/\"):\n",
        "#     os.makedirs(f\"../data/lora_test/lora/\")\n",
        "unet_lora_state_dict = convert_state_dict_to_diffusers(\n",
        "    get_peft_model_state_dict(unet)\n",
        ")\n",
        "StableDiffusionPipeline.save_lora_weights(\n",
        "    save_directory=f\"../data/loss_curves_{formatted_time}/lora/lora_weights\",\n",
        "    # save_directory=f\"../data/lora_test/lora/lora_weights\",\n",
        "    unet_lora_layers=unet_lora_state_dict,\n",
        "    safe_serialization=True,\n",
        ")\n",
        "torch.save(unet.state_dict(), f\"../data/loss_curves_{formatted_time}/lora/diffusion_weights_multi_feat_lora.pth\")\n",
        "# torch.save(unet.state_dict(), f\"../data/lora_test/lora/diffusion_weights_multi_feat_lora.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "97eb8ffb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e6475a222d5451fb5f43c66ccea1c9f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/I749793/.pyenv/versions/3.11.0/envs/classifierGuidance_3.11.0_venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    unet=unet,\n",
        "    torch_dtype=torch.float16,  # or torch.float32\n",
        ")\n",
        "pipe.load_lora_weights(\n",
        "    pretrained_model_name_or_path_or_dict=f\"../data/loss_curves_{formatted_time}/lora/lora_weights\",\n",
        "    # pretrained_model_name_or_path_or_dict=f\"../data/lora_test/lora/lora_weights\",\n",
        "    adapter_name=\"custom_lora\",  # optional name\n",
        "    from_local=True\n",
        ")\n",
        "unet = pipe.unet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "172368e1",
      "metadata": {},
      "source": [
        "### Stable Diffusion Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "702fdd80",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def generate_maze_from_test(sample_idx=None, num_steps=50):\n",
        "    unet.eval()\n",
        "    scheduler.set_timesteps(num_steps)\n",
        "    \n",
        "    if sample_idx is None:\n",
        "        sample_idx = random.randint(0, len(test_dataset) - 1)\n",
        "    \n",
        "    # test_img, test_path_length, nodes_astar, nodes_bfs = test_dataset[sample_idx]\n",
        "    test_img, prompt = test_dataset[sample_idx]\n",
        "    # path_tensor = torch.tensor([test_path_length]).float().to(device)\n",
        "    # nodes_astar_tensor = torch.tensor([nodes_astar]).float().to(device)\n",
        "    # combined_features = torch.stack((path_tensor, nodes_astar_tensor), dim=-1)\n",
        "    # context = diffusion_model.condition_multidimensional_embedding(\n",
        "    #     torch.tensor([test_path_length], device=device).float()\n",
        "    # )\n",
        "\n",
        "    # context = custom_diffusion_model.condition_multidimensional_embedding(combined_features)\n",
        "\n",
        "    inputs = tokenizer(prompt, padding=\"max_length\", max_length=77, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        context = text_encoder(**inputs).last_hidden_state\n",
        "    \n",
        "    # latent = torch.randn((1, 4, 8, 8), device=device)\n",
        "    latent = torch.randn((1, 4, 8, 8), device=device)\n",
        "    \n",
        "    for t in scheduler.timesteps:\n",
        "        timestep = torch.tensor([t], device=device)\n",
        "        with torch.no_grad():\n",
        "            pred = unet(latent, timestep, encoder_hidden_states=context).sample\n",
        "        latent = scheduler.step(pred, t, latent).prev_sample\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        generated_image = vae.decode(latent/0.18215).sample\n",
        "        # generated_image = decoder(latent / 0.18215)\n",
        "    \n",
        "    # return generated_image, test_img, test_path_length\n",
        "    return generated_image, test_img, prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ff2084bb",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'unet' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m generated, original, test_path_length = \u001b[43mgenerate_maze_from_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m image = generated[\u001b[32m0\u001b[39m].detach().cpu()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Convert from [-1, 1] to [0, 1]\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mgenerate_maze_from_test\u001b[39m\u001b[34m(sample_idx, num_steps)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_maze_from_test\u001b[39m(sample_idx=\u001b[38;5;28;01mNone\u001b[39;00m, num_steps=\u001b[32m50\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43munet\u001b[49m.eval()\n\u001b[32m      3\u001b[39m     scheduler.set_timesteps(num_steps)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sample_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[31mNameError\u001b[39m: name 'unet' is not defined"
          ]
        }
      ],
      "source": [
        "generated, original, test_path_length = generate_maze_from_test(num_steps=100)\n",
        "image = generated[0].detach().cpu()\n",
        "\n",
        "# Convert from [-1, 1] to [0, 1]\n",
        "image = (image + 1.0) / 2.0\n",
        "generated = torch.clamp(image, 0.0, 1.0)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(generated.squeeze(0).permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "# plt.imshow(generated.squeeze(0).cpu().numpy(), cmap='gray')\n",
        "plt.title(f\"Generated Maze, {test_path_length}\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# plt.imshow(original.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "plt.imshow(original.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "plt.title(\"Original Test Maze\")\n",
        "plt.axis(\"off\")\n",
        "file_name = f\"diffusion_image_generation_multi_feat_{formatted_time}\"\n",
        "plt.savefig(os.path.join(loss_curves_folder, file_name))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a01ba929",
      "metadata": {},
      "outputs": [],
      "source": [
        "generated, original, test_path_length = generate_maze_from_test(num_steps=100)\n",
        "image = generated[0].detach().cpu()\n",
        "\n",
        "# Convert from [-1, 1] to [0, 1]\n",
        "image = (image + 1.0) / 2.0\n",
        "generated = torch.clamp(image, 0.0, 1.0)\n",
        "\n",
        "generated = (generated > 0.5).float()\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(generated.squeeze(0).permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "# plt.imshow(generated.squeeze(0).cpu().numpy(), cmap='gray')\n",
        "plt.title(f\"Generated Maze, {test_path_length}\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# plt.imshow(original.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "plt.imshow(original.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "plt.title(\"Original Test Maze\")\n",
        "plt.axis(\"off\")\n",
        "file_name = f\"diffusion_image_generation_multi_feat_{formatted_time}_greater_05\"\n",
        "plt.savefig(os.path.join(loss_curves_folder, file_name))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2853145",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "classifierGuidance_3.11.0_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
