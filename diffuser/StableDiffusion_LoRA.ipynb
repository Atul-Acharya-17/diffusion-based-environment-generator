{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "af842a4b-30e4-469d-b315-e5405730df14",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af842a4b-30e4-469d-b315-e5405730df14",
        "outputId": "27b5603e-40f4-41f7-ef12-76808f966178"
      },
      "outputs": [],
      "source": [
        "# !pip install einops torch maze-dataset --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "cfa8f76c-2435-45e0-92af-e1c548bf0390",
      "metadata": {
        "id": "cfa8f76c-2435-45e0-92af-e1c548bf0390"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from torch.nn import functional as F\n",
        "\n",
        "now = datetime.now()\n",
        "formatted_time = now.strftime(\"%Y%m%d_%H%M%S\")\n",
        "loss_curves_folder = f\"../data/loss_curves_{formatted_time}/stable_diffusion_diffusion_models\"\n",
        "if not os.path.exists(loss_curves_folder):\n",
        "    os.makedirs(loss_curves_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bf0f5bb1-2c86-4f19-bafe-7b7bec3a57ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf0f5bb1-2c86-4f19-bafe-7b7bec3a57ff",
        "outputId": "5d16aa39-38b8-429f-f379-7d699ed534cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /Users/I749793/Desktop/NUS/CS5340 - Uncertainty Modeling in AI/project/diffusion-based-environment-generator\n"
          ]
        }
      ],
      "source": [
        "os.chdir(\"..\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "543d1c3c-8f5e-403d-b9a1-ce031a9587c7",
      "metadata": {
        "id": "543d1c3c-8f5e-403d-b9a1-ce031a9587c7"
      },
      "source": [
        "## Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e7448583-863f-4043-9e54-c36788bf8b30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7448583-863f-4043-9e54-c36788bf8b30",
        "outputId": "3833d816-19ce-4bde-fce1-8c6d53584d7a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from generator.maze.grid_world_generator import generate_multiple_grid_worlds\n",
        "from generator.maze.solvers.a_star_l1 import main as a_star_l1_paths\n",
        "from generator.maze.solvers.bfs import main as bfs_paths\n",
        "\n",
        "# parent_directory = \"./data\"\n",
        "# if not os.path.exists(parent_directory):\n",
        "#     os.makedirs(parent_directory)\n",
        "# # generates the mazes\n",
        "# mazes = generate_multiple_grid_worlds(num_worlds=25000, parent_directory=parent_directory)\n",
        "# # generate path travrsals\n",
        "# a_star_l1_paths(parent_directory)\n",
        "# bfs_paths(parent_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6407b36e-2b1a-49da-8fda-dc83f7bb3481",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "6407b36e-2b1a-49da-8fda-dc83f7bb3481",
        "outputId": "e65862df-98a3-4d88-9261-c7466ce34052"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def preprocess_image(image, target_size=32):\n",
        "#     image = np.array(image)\n",
        "\n",
        "#     scale_factor = target_size // image.shape[0]\n",
        "#     image = np.kron(image, np.ones((scale_factor, scale_factor, 1)))\n",
        "\n",
        "#     image = image.astype(np.float32) / 127.5 - 1\n",
        "#     image = torch.tensor(image).permute(2, 0, 1)\n",
        "#     return image\n",
        "\n",
        "# def load_dataset_from_npy(directory=\"./data\", target_size=32):\n",
        "#     images = []\n",
        "#     path_lengths = []\n",
        "\n",
        "#     files = sorted([f for f in os.listdir(directory) if f.endswith(\".npy\")])\n",
        "\n",
        "#     for file in files:\n",
        "#         img = np.load(os.path.join(directory, file))\n",
        "\n",
        "#         mask = np.all(img == [0, 0, 255], axis=-1)\n",
        "#         img[mask] = [255, 255, 255]\n",
        "#         img = img[:-1, :-1]\n",
        "\n",
        "#         image = preprocess_image(img, target_size)\n",
        "\n",
        "#         base_name = os.path.splitext(file)[0]\n",
        "#         len_filename = base_name + \"_len.txt\"\n",
        "#         len_path = os.path.join(directory, len_filename)\n",
        "\n",
        "#         with open(len_path, \"r\") as f:\n",
        "#             maze_length = int(f.read().strip())\n",
        "\n",
        "#         images.append(image)\n",
        "#         path_lengths.append(maze_length)\n",
        "\n",
        "#     return images, path_lengths\n",
        "\n",
        "# images, path_lengths = load_dataset_from_npy(\"./data\", target_size=32)\n",
        "\n",
        "# plt.imshow(images[0].permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "83a74713",
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image(image, target_size=32):\n",
        "    image = np.array(image)\n",
        "    scale_factor = target_size // image.shape[0] \n",
        "    # image = np.kron(image, np.ones((scale_factor, scale_factor, 1))) \n",
        "    \n",
        "    # image = image.astype(np.float32) / 127.5 - 1\n",
        "    image = image.astype(np.float32)\n",
        "    image = torch.tensor(image).permute(2, 0, 1)\n",
        "    image = F.interpolate(image.unsqueeze(0), size=(target_size, target_size), mode='nearest').squeeze(0)  # (3, 32, 32)\n",
        "\n",
        "    return image\n",
        "\n",
        "def plot_grid_world(grid):\n",
        "    \"\"\"\n",
        "    Plots the given grid world.\n",
        "    \"\"\"\n",
        "    wall = grid[:,:,0] == 0\n",
        "    source = grid[:,:,1] == 1\n",
        "    destination = grid[:,:,2] == 1\n",
        "\n",
        "    img = np.ones((*wall.shape, 3), dtype=np.float32)  # White background\n",
        "    img[wall] = np.array([0, 0, 0])  # Walls → Black\n",
        "    img[source] = np.array([1, 0, 0])  # Source → Red\n",
        "    img[destination] = np.array([0, 1, 0])  # Destination → Green\n",
        "\n",
        "    return img\n",
        "\n",
        "def load_dataset_from_npy(parent_directory=\"./data\", target_size=32):\n",
        "    images = []\n",
        "    path_lengths = []\n",
        "    num_nodes_traversed_astar = []\n",
        "    num_nodes_traversed_bfs = []\n",
        "    \n",
        "    mazes_directory = os.path.join(parent_directory, \"mazes\")\n",
        "    files = sorted([f for f in os.listdir(mazes_directory) if f.endswith(\".npy\")])\n",
        "    \n",
        "    for file in files:\n",
        "        img = np.load(os.path.join(mazes_directory, file))\n",
        "        if(img.shape != (10,10,3)):\n",
        "            continue\n",
        "        # mask = np.all(img == [0, 0, 255], axis=-1)\n",
        "        # img[mask] = [255, 255, 255]\n",
        "        # img = img[:-1, :-1]\n",
        "        # image = preprocess_image(img, target_size)\n",
        "\n",
        "        image = plot_grid_world(img)\n",
        "        mask = np.all(image == [0, 0, 255], axis=-1)\n",
        "        image[mask] = [255, 255, 255]\n",
        "        image = preprocess_image(image, target_size)\n",
        "\n",
        "        pattern = r'maze_(\\d+)'\n",
        "        match = re.search(pattern, file)\n",
        "        num = 0\n",
        "        if match:\n",
        "            num = int(match.group(1))\n",
        "        else:\n",
        "            continue\n",
        "        \n",
        "        # base_name = os.path.splitext(file)[0]\n",
        "        # len_filename = base_name + \"_len.txt\"\n",
        "        len_filename = f\"path_length_{num}\" + \".npy\"\n",
        "        len_path = os.path.join(mazes_directory, len_filename)\n",
        "        astar_traversal_filename = f\"a_star_{num}\" + \".npy\"\n",
        "        astar_traversal_path = os.path.join(parent_directory, \"a_star_l1_results\" ,astar_traversal_filename)\n",
        "        bfs_traversal_filename = f\"bfs_{num}\" + \".npy\"\n",
        "        bfs_traversal_path = os.path.join(parent_directory, \"bfs_results\" ,bfs_traversal_filename)\n",
        "        \n",
        "        # with open(len_path, \"r\") as f:\n",
        "        #     maze_length = int(f.read().strip())\n",
        "        maze_length = np.load(len_path)\n",
        "        astar_traversal = np.load(astar_traversal_path)\n",
        "        bfs_traversal = np.load(bfs_traversal_path)\n",
        "        \n",
        "        images.append(image)\n",
        "        path_lengths.append(int(maze_length))\n",
        "        num_nodes_traversed_astar.append(int(astar_traversal))\n",
        "        num_nodes_traversed_bfs.append(int(bfs_traversal))\n",
        "    \n",
        "    return images, path_lengths, num_nodes_traversed_astar, num_nodes_traversed_bfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "73f89a5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "mazes_data_path = \"./data/\"\n",
        "images, org_path_lengths, num_nodes_astar, num_nodes_bfs = load_dataset_from_npy(mazes_data_path, target_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9364d0ee",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(images[0].permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e2a04378-6293-4c79-95d0-fa6e305cba28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2a04378-6293-4c79-95d0-fa6e305cba28",
        "outputId": "1e1d158e-a42a-453d-ea90-ff6a716979fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /Users/I749793/Desktop/NUS/CS5340 - Uncertainty Modeling in AI/project/diffusion-based-environment-generator/diffuser\n"
          ]
        }
      ],
      "source": [
        "# print(f\"Current working directory: {os.getcwd()}\")\n",
        "os.chdir(\"./diffuser\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "742a775a-18e1-4d73-a304-0d951dfc05c2",
      "metadata": {
        "id": "742a775a-18e1-4d73-a304-0d951dfc05c2",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### VAE Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0b8328c6-fdf1-4599-9140-b1ee9c6cc029",
      "metadata": {
        "id": "0b8328c6-fdf1-4599-9140-b1ee9c6cc029"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "335fe85b-942c-4707-89fe-fd5e10b8214d",
      "metadata": {
        "id": "335fe85b-942c-4707-89fe-fd5e10b8214d"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 25\n",
        "LATENT_CHANNELS = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f1bd601d",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MazeTensorDataset(Dataset):\n",
        "    def __init__(self, images, path_lengths, num_nodes_astar, num_nodes_bfs):\n",
        "        self.images = images\n",
        "        self.path_lengths = path_lengths\n",
        "        self.num_nodes_astar = num_nodes_astar\n",
        "        self.num_nodes_bfs = num_nodes_bfs\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.path_lengths[idx], self.num_nodes_astar[idx], self.num_nodes_bfs[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5984963c-d4d5-402c-bae4-4b79276425f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5984963c-d4d5-402c-bae4-4b79276425f3",
        "outputId": "0c7d727d-5507-4e0b-c336-168d6ced665b"
      },
      "outputs": [],
      "source": [
        "# print(\"Total images:\", len(images))\n",
        "# print(\"Total path_lengths:\", len(path_lengths))\n",
        "\n",
        "# total = len(images)\n",
        "# test_size = int(0.2 * total)\n",
        "# all_indices = list(range(total))\n",
        "# random.shuffle(all_indices)\n",
        "\n",
        "# test_indices = all_indices[:test_size]\n",
        "# train_indices = all_indices[test_size:]\n",
        "\n",
        "# train_images = [images[i] for i in train_indices]\n",
        "# train_path_lengths = [path_lengths[i] for i in train_indices]\n",
        "\n",
        "# test_images = [images[i] for i in test_indices]\n",
        "# test_path_lengths = [path_lengths[i] for i in test_indices]\n",
        "\n",
        "# dataset = MazeTensorDataset(train_images, train_path_lengths)\n",
        "# test_dataset = MazeTensorDataset(test_images, test_path_lengths)\n",
        "\n",
        "# print(\"Train dataset length:\", len(dataset))\n",
        "# print(\"Test dataset length:\", len(test_dataset))\n",
        "\n",
        "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# unique_train_paths = set(train_path_lengths)\n",
        "# print(\"Unique training path lengths:\", unique_train_paths)\n",
        "# print(\"Number of unique training paths:\", len(unique_train_paths))\n",
        "\n",
        "# unique_test_paths = set(test_path_lengths)\n",
        "# print(\"Unique test path lengths:\", unique_test_paths)\n",
        "# print(\"Number of unique test paths:\", len(unique_test_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "960b6cda",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total images: 1000\n",
            "Total path_lengths: 1000\n",
            "Train dataset length: 800\n",
            "Test dataset length: 200\n",
            "Unique training path lengths: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22}\n",
            "Number of unique training paths: 21\n",
            "Unique test path lengths: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18}\n",
            "Number of unique test paths: 17\n"
          ]
        }
      ],
      "source": [
        "path_lengths = org_path_lengths\n",
        "\n",
        "## use the scaled path lengths based on the number of nodes traversed\n",
        "# path_lengths = [a / b if b!=0 else a for a, b in zip(path_lengths, num_nodes_astar)]\n",
        "# path_lengths = [a / b if b!=0 else a for a, b in zip(path_lengths, num_nodes_bfs)]\n",
        "\n",
        "print(\"Total images:\", len(images))\n",
        "print(\"Total path_lengths:\", len(path_lengths))\n",
        "\n",
        "total = len(images)\n",
        "test_size = int(0.2 * total)\n",
        "all_indices = list(range(total))\n",
        "random.shuffle(all_indices)\n",
        "\n",
        "test_indices = all_indices[:test_size]\n",
        "train_indices = all_indices[test_size:]\n",
        "\n",
        "train_images = [images[i] for i in train_indices]\n",
        "train_path_lengths = [path_lengths[i] for i in train_indices]\n",
        "train_num_nodes_astar = [num_nodes_astar[i] for i in train_indices]\n",
        "train_num_nodes_bfs = [num_nodes_bfs[i] for i in train_indices]\n",
        "\n",
        "test_images = [images[i] for i in test_indices]\n",
        "test_path_lengths = [path_lengths[i] for i in test_indices]\n",
        "test_num_nodes_astar = [num_nodes_astar[i] for i in test_indices]\n",
        "test_num_nodes_bfs = [num_nodes_bfs[i] for i in test_indices]\n",
        "\n",
        "dataset = MazeTensorDataset(train_images, train_path_lengths, train_num_nodes_astar, train_num_nodes_bfs)\n",
        "test_dataset = MazeTensorDataset(test_images, test_path_lengths, test_num_nodes_astar, test_num_nodes_bfs)\n",
        "\n",
        "print(\"Train dataset length:\", len(dataset))\n",
        "print(\"Test dataset length:\", len(test_dataset))\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "unique_train_paths = set(train_path_lengths)\n",
        "print(\"Unique training path lengths:\", unique_train_paths)\n",
        "print(\"Number of unique training paths:\", len(unique_train_paths))\n",
        "\n",
        "unique_test_paths = set(test_path_lengths)\n",
        "print(\"Unique test path lengths:\", unique_test_paths)\n",
        "print(\"Number of unique test paths:\", len(unique_test_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0e98f779",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import safetensors\n",
        "# print(safetensors.__file__)\n",
        "\n",
        "# from safetensors.torch import save_file, load_file\n",
        "# print(\"Success!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "37e80c38",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from diffusers import UNet2DConditionModel, DDIMScheduler, AutoencoderKL\n",
        "from peft import LoraConfig\n",
        "from model import Diffusion as CustomDiffusionModel\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load Stable Diffusion's pre-trained VAE\n",
        "vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\").to(device)\n",
        "# vae_checkpoint = torch.load(\"/Users/I749793/Desktop/NUS/CS5340 - Uncertainty Modeling in AI/project/diffusion-based-environment-generator/data/stablediffusion_weights.pth\", map_location=device)  # or \"cuda\" if you want\n",
        "vae_checkpoint = torch.load(\"../data/vae_models/stablediffusion_vae_weights.pth\", map_location=device)  # or \"cuda\" if you want\n",
        "vae_state_dict = vae_checkpoint['diffusion_state_dict']\n",
        "vae.load_state_dict(vae_state_dict)\n",
        "# optimizer = optim.Adam(vae.parameters(), lr=1e-4)\n",
        "# criterion = nn.MSELoss()\n",
        "# loading pretrained vae instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "qd7ioKYDO15-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd7ioKYDO15-",
        "outputId": "00753a60-c6d7-4d6b-c878-343ffc7c2bb2"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "# ---- Training Loop ---- #\n",
        "def train_vae(dataloader, epochs=50):\n",
        "    vae.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for i, (images, _, _, _) in enumerate(dataloader):\n",
        "            images = images.to(device)  # Input maze images (shape: B x 3 x 256 x 256)\n",
        "\n",
        "            # Forward pass\n",
        "            latent_dist = vae.encode(images).latent_dist\n",
        "            latent_sample = latent_dist.sample()\n",
        "            reconstructed_images = vae.decode(latent_sample).sample\n",
        "\n",
        "            # Loss computation\n",
        "            loss = criterion(reconstructed_images, images)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.6f}\")\n",
        "\n",
        "# train_vae(dataloader, epochs=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "Y7ye83uDSOUQ",
      "metadata": {
        "id": "Y7ye83uDSOUQ"
      },
      "outputs": [],
      "source": [
        "# torch.save({\n",
        "#     'diffusion_state_dict': vae.state_dict(),\n",
        "#     'optimizer_state_dict': optimizer.state_dict(),\n",
        "#     'train_losses': train_losses\n",
        "# }, 'diffusion_weights.pth')\n",
        "# print(\"Diffusion weights saved to diffusion_weights.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c87ee58e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Save Model Weights\n",
        "# if not os.path.exists(f\"../data/loss_curves_{formatted_time}/stable_diffusion_vae_models/\"):\n",
        "#     os.makedirs(f\"../data/loss_curves_{formatted_time}/stable_diffusion_vae_models/\")\n",
        "# torch.save({\n",
        "#     'vae_state_dict': vae.state_dict(),\n",
        "#     'optimizer_state_dict': optimizer.state_dict(),\n",
        "#     'train_losses': train_losses,\n",
        "# }, f'../data/loss_curves_{formatted_time}/stable_diffusion_vae_models/vae_weights_multi_feat.pth')\n",
        "\n",
        "# print(\"Model weights saved to vae_weights_multi_feat.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd28e29c",
      "metadata": {},
      "source": [
        "### VAE Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "SlkhuVYMSspV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "SlkhuVYMSspV",
        "outputId": "dc812d14-0cd7-4948-e9dc-6bc2fe0d00df"
      },
      "outputs": [],
      "source": [
        "# vae.eval()\n",
        "# def generate_maze_from_test(sample_idx=None):\n",
        "\n",
        "#     if sample_idx is None:\n",
        "#         sample_idx = random.randint(0, len(test_dataset) - 1)\n",
        "\n",
        "#     test_img, test_path_length, _, _ = test_dataset[sample_idx]  # Load test maze\n",
        "#     test_img = test_img.unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "#     # Encode test maze image into latent space\n",
        "#     with torch.no_grad():\n",
        "#         latent_dist = vae.encode(test_img).latent_dist\n",
        "#         latent_sample = latent_dist.sample()  # Sample from the distribution\n",
        "\n",
        "#         # Decode latent back to an image\n",
        "#         generated_image = vae.decode(latent_sample).sample\n",
        "\n",
        "#     return generated_image, test_img, test_path_length\n",
        "\n",
        "# # Run test generation\n",
        "# generated, original, test_path_length = generate_maze_from_test()\n",
        "\n",
        "# # ---- Visualization ---- #\n",
        "# plt.figure(figsize=(10, 5))\n",
        "\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.imshow(generated.squeeze(0).permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "# plt.title(f\"Generated Maze (Path Length: {test_path_length})\")\n",
        "# plt.axis(\"off\")\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.imshow(original.squeeze(0).permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "# plt.title(\"Original Test Maze\")\n",
        "# plt.axis(\"off\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1821b8bd",
      "metadata": {},
      "source": [
        "# Diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f567312",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparams\n",
        "NUM_TIMESTEPS = 1000\n",
        "GUIDANCE_SCALE = 7.5\n",
        "LATENT_SHAPE = (4, 64, 64)  # For SD 1.5\n",
        "EPOCHS = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "62efeb99",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AutoencoderKL(\n",
              "  (encoder): Encoder(\n",
              "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (down_blocks): ModuleList(\n",
              "      (0): DownEncoderBlock2D(\n",
              "        (resnets): ModuleList(\n",
              "          (0-1): 2 x ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "        )\n",
              "        (downsamplers): ModuleList(\n",
              "          (0): Downsample2D(\n",
              "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): DownEncoderBlock2D(\n",
              "        (resnets): ModuleList(\n",
              "          (0): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (1): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "        )\n",
              "        (downsamplers): ModuleList(\n",
              "          (0): Downsample2D(\n",
              "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): DownEncoderBlock2D(\n",
              "        (resnets): ModuleList(\n",
              "          (0): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (1): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "        )\n",
              "        (downsamplers): ModuleList(\n",
              "          (0): Downsample2D(\n",
              "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): DownEncoderBlock2D(\n",
              "        (resnets): ModuleList(\n",
              "          (0-1): 2 x ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (mid_block): UNetMidBlock2D(\n",
              "      (attentions): ModuleList(\n",
              "        (0): Attention(\n",
              "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (to_out): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "    (conv_act): SiLU()\n",
              "    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (up_blocks): ModuleList(\n",
              "      (0-1): 2 x UpDecoderBlock2D(\n",
              "        (resnets): ModuleList(\n",
              "          (0-2): 3 x ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "        )\n",
              "        (upsamplers): ModuleList(\n",
              "          (0): Upsample2D(\n",
              "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): UpDecoderBlock2D(\n",
              "        (resnets): ModuleList(\n",
              "          (0): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (1-2): 2 x ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "        )\n",
              "        (upsamplers): ModuleList(\n",
              "          (0): Upsample2D(\n",
              "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): UpDecoderBlock2D(\n",
              "        (resnets): ModuleList(\n",
              "          (0): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (1-2): 2 x ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (mid_block): UNetMidBlock2D(\n",
              "      (attentions): ModuleList(\n",
              "        (0): Attention(\n",
              "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (to_out): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "    (conv_act): SiLU()\n",
              "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\").to(device)\n",
        "unet.requires_grad_(False)\n",
        "lora_config = LoraConfig(\n",
        "        r=4,\n",
        "        lora_alpha=16,\n",
        "        init_lora_weights=\"gaussian\",\n",
        "        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
        "    )\n",
        "unet.add_adapter(lora_config)\n",
        "lora_layers = filter(lambda p: p.requires_grad, unet.parameters())\n",
        "optimizer = torch.optim.Adam(\n",
        "    lora_layers, lr=1e-5, betas=(0.9, 0.999), weight_decay=1e-2, eps=1e-08)\n",
        "scheduler = DDIMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
        "custom_diffusion_model = CustomDiffusionModel(input_size=2).to(device)\n",
        "scheduler.set_timesteps(NUM_TIMESTEPS)\n",
        "vae.eval()\n",
        "# optimizer = torch.optim.Adam(list(unet.parameters()), lr=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aedc139",
      "metadata": {},
      "source": [
        "### Stable Diffusion Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "32c19b54",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training epoch [1/1]: 100%|██████████| 7/7 [03:18<00:00, 28.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Loss: 0.955663\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0.0\n",
        "    count = 0\n",
        "\n",
        "    for batch_idx, (images, path_lengths, nodes_astar, nodes_bfs) in enumerate(tqdm(dataloader, desc=f\"Training epoch [{epoch+1}/{EPOCHS}]\")):\n",
        "        unet.train()\n",
        "        images = images.to(device)\n",
        "        path_lengths = path_lengths.float().to(device)\n",
        "        nodes_astar = nodes_astar.float().to(device)\n",
        "\n",
        "        # Step 1: Encode images to latents using VAE\n",
        "        with torch.no_grad():\n",
        "            z = vae.encode(images).latent_dist.sample()  # shape: (B, 4, 64, 64)\n",
        "\n",
        "        # Step 2: Sample timesteps & add noise\n",
        "        timesteps = torch.randint(0, NUM_TIMESTEPS, (z.size(0),), device=device).long()\n",
        "        noise = torch.randn_like(z)\n",
        "        noisy_z = scheduler.add_noise(z, noise, timesteps)\n",
        "\n",
        "        # Step 3: Prepare conditional and unconditional embeddings\n",
        "        cond_input = torch.stack([path_lengths, nodes_astar], dim=-1)  \n",
        "        cond_embed = custom_diffusion_model.condition_multidimensional_embedding(cond_input)                \n",
        "\n",
        "        zero_input = torch.zeros_like(cond_input)\n",
        "        uncond_embed = custom_diffusion_model.condition_multidimensional_embedding(zero_input)\n",
        "\n",
        "        context = torch.cat([uncond_embed, cond_embed], dim=0)       \n",
        "\n",
        "        # Step 4: Forward pass with classifier-free guidance\n",
        "        noisy_z = noisy_z.repeat(2, 1, 1, 1)                          \n",
        "        timesteps = timesteps.repeat_interleave(2)                   \n",
        "\n",
        "        noise_pred = unet(noisy_z, timesteps, encoder_hidden_states=context).sample\n",
        "        noise_uncond, noise_cond = noise_pred.chunk(2)\n",
        "\n",
        "        guided_noise = noise_uncond + GUIDANCE_SCALE * (noise_cond - noise_uncond)\n",
        "\n",
        "        # Step 5: Loss and optimization\n",
        "        loss = F.smooth_l1_loss(guided_noise, noise)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item() * images.size(0)\n",
        "        count += images.size(0)\n",
        "\n",
        "    avg_loss = epoch_loss / count\n",
        "    train_losses.append(avg_loss)\n",
        "    print(f\"[Epoch {epoch+1}] Loss: {avg_loss:.6f}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, EPOCHS + 1), train_losses, marker='o', label='Train Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Graph')\n",
        "plt.legend()\n",
        "file_name = f\"loss_curve_diffusion_multi_feat_{formatted_time}\"\n",
        "plt.savefig(os.path.join(loss_curves_folder, file_name))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "96439c5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline, DiffusionPipeline\n",
        "from diffusers.utils import convert_state_dict_to_diffusers\n",
        "from peft import get_peft_model_state_dict\n",
        "\n",
        "if not os.path.exists(f\"../data/loss_curves_{formatted_time}/lora/\"):\n",
        "    os.makedirs(f\"../data/loss_curves_{formatted_time}/lora/\")\n",
        "unet_lora_state_dict = convert_state_dict_to_diffusers(\n",
        "    get_peft_model_state_dict(unet)\n",
        ")\n",
        "StableDiffusionPipeline.save_lora_weights(\n",
        "    save_directory=f\"../data/loss_curves_{formatted_time}/lora/lora_weights\",\n",
        "    # save_directory=f\"../data/lora_test/lora/lora_weights\",\n",
        "    unet_lora_layers=unet_lora_state_dict,\n",
        "    safe_serialization=True,\n",
        ")\n",
        "torch.save(unet.state_dict(), f\"../data/loss_curves_{formatted_time}/lora/diffusion_weights_multi_feat_lora.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "97eb8ffb",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    unet=unet,\n",
        "    torch_dtype=torch.float16,  # or torch.float32\n",
        ")\n",
        "pipe.load_lora_weights(\n",
        "    pretrained_model_name_or_path_or_dict=f\"../data/loss_curves_{formatted_time}/lora/lora_weights\",\n",
        "    adapter_name=\"custom_lora\",  # optional name\n",
        "    from_local=True\n",
        ")\n",
        "unet = pipe.unet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "172368e1",
      "metadata": {},
      "source": [
        "### Stable Diffusion Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "702fdd80",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def generate_maze_from_test(sample_idx=None, num_steps=50):\n",
        "    unet.eval()\n",
        "    scheduler.set_timesteps(num_steps)\n",
        "    \n",
        "    if sample_idx is None:\n",
        "        sample_idx = random.randint(0, len(test_dataset) - 1)\n",
        "    \n",
        "    test_img, test_path_length, nodes_astar, nodes_bfs = test_dataset[sample_idx]\n",
        "    path_tensor = torch.tensor([test_path_length]).float().to(device)\n",
        "    nodes_astar_tensor = torch.tensor([nodes_astar]).float().to(device)\n",
        "    combined_features = torch.stack((path_tensor, nodes_astar_tensor), dim=-1)\n",
        "    # context = diffusion_model.condition_multidimensional_embedding(\n",
        "    #     torch.tensor([test_path_length], device=device).float()\n",
        "    # )\n",
        "\n",
        "    context = custom_diffusion_model.condition_multidimensional_embedding(combined_features)\n",
        "    \n",
        "    latent = torch.randn((1, 4, 8, 8), device=device)\n",
        "    \n",
        "    for t in scheduler.timesteps:\n",
        "        timestep = torch.tensor([t], device=device)\n",
        "        with torch.no_grad():\n",
        "            pred = unet(latent, timestep, encoder_hidden_states=context).sample\n",
        "        latent = scheduler.step(pred, t, latent).prev_sample\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        generated_image = vae.decode(latent/0.18215).sample\n",
        "        # generated_image = decoder(latent / 0.18215)\n",
        "    \n",
        "    return generated_image, test_img, test_path_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "ff2084bb",
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../data/loss_curves_20250420_185835/stable_diffusion_diffusion_models/diffusion_image_generation_multi_feat_20250420_185835.png'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m plt.axis(\u001b[33m\"\u001b[39m\u001b[33moff\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m file_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdiffusion_image_generation_multi_feat_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformatted_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_curves_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m plt.show()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.0/envs/classifierGuidance_3.11.0_venv/lib/python3.11/site-packages/matplotlib/pyplot.py:1243\u001b[39m, in \u001b[36msavefig\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1240\u001b[39m fig = gcf()\n\u001b[32m   1241\u001b[39m \u001b[38;5;66;03m# savefig default implementation has no return, so mypy is unhappy\u001b[39;00m\n\u001b[32m   1242\u001b[39m \u001b[38;5;66;03m# presumably this is here because subclasses can return?\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m res = \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[func-returns-value]\u001b[39;00m\n\u001b[32m   1244\u001b[39m fig.canvas.draw_idle()  \u001b[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.0/envs/classifierGuidance_3.11.0_venv/lib/python3.11/site-packages/matplotlib/figure.py:3490\u001b[39m, in \u001b[36mFigure.savefig\u001b[39m\u001b[34m(self, fname, transparent, **kwargs)\u001b[39m\n\u001b[32m   3488\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.axes:\n\u001b[32m   3489\u001b[39m         _recursively_make_axes_transparent(stack, ax)\n\u001b[32m-> \u001b[39m\u001b[32m3490\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcanvas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.0/envs/classifierGuidance_3.11.0_venv/lib/python3.11/site-packages/matplotlib/backend_bases.py:2184\u001b[39m, in \u001b[36mFigureCanvasBase.print_figure\u001b[39m\u001b[34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[39m\n\u001b[32m   2180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2181\u001b[39m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[32m   2182\u001b[39m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[32m   2183\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m cbook._setattr_cm(\u001b[38;5;28mself\u001b[39m.figure, dpi=dpi):\n\u001b[32m-> \u001b[39m\u001b[32m2184\u001b[39m         result = \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2185\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2187\u001b[39m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2188\u001b[39m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[43m=\u001b[49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2189\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2190\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2191\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2192\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.0/envs/classifierGuidance_3.11.0_venv/lib/python3.11/site-packages/matplotlib/backend_bases.py:2040\u001b[39m, in \u001b[36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   2036\u001b[39m     optional_kws = {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[32m   2037\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdpi\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfacecolor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33medgecolor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33morientation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2038\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbbox_inches_restore\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m   2039\u001b[39m     skip = optional_kws - {*inspect.signature(meth).parameters}\n\u001b[32m-> \u001b[39m\u001b[32m2040\u001b[39m     print_method = functools.wraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m *args, **kwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2041\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2042\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[32m   2043\u001b[39m     print_method = meth\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.0/envs/classifierGuidance_3.11.0_venv/lib/python3.11/site-packages/matplotlib/backends/backend_agg.py:481\u001b[39m, in \u001b[36mFigureCanvasAgg.print_png\u001b[39m\u001b[34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, *, metadata=\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    435\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    436\u001b[39m \u001b[33;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[32m    437\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    479\u001b[39m \u001b[33;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpng\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.0/envs/classifierGuidance_3.11.0_venv/lib/python3.11/site-packages/matplotlib/backends/backend_agg.py:430\u001b[39m, in \u001b[36mFigureCanvasAgg._print_pil\u001b[39m\u001b[34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[33;03mDraw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[32m    427\u001b[39m \u001b[33;03m*pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    429\u001b[39m FigureCanvasAgg.draw(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[43mmpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimsave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuffer_rgba\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupper\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfigure\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.0/envs/classifierGuidance_3.11.0_venv/lib/python3.11/site-packages/matplotlib/image.py:1644\u001b[39m, in \u001b[36mimsave\u001b[39m\u001b[34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[39m\n\u001b[32m   1642\u001b[39m pil_kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m)\n\u001b[32m   1643\u001b[39m pil_kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mdpi\u001b[39m\u001b[33m\"\u001b[39m, (dpi, dpi))\n\u001b[32m-> \u001b[39m\u001b[32m1644\u001b[39m \u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.0/envs/classifierGuidance_3.11.0_venv/lib/python3.11/site-packages/PIL/Image.py:2591\u001b[39m, in \u001b[36mImage.save\u001b[39m\u001b[34m(self, fp, format, **params)\u001b[39m\n\u001b[32m   2589\u001b[39m         fp = builtins.open(filename, \u001b[33m\"\u001b[39m\u001b[33mr+b\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2590\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2591\u001b[39m         fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw+b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2593\u001b[39m     fp = cast(IO[\u001b[38;5;28mbytes\u001b[39m], fp)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/loss_curves_20250420_185835/stable_diffusion_diffusion_models/diffusion_image_generation_multi_feat_20250420_185835.png'"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "generated, original, test_path_length = generate_maze_from_test(num_steps=100)\n",
        "image = generated[0].detach().cpu()\n",
        "\n",
        "# Convert from [-1, 1] to [0, 1]\n",
        "image = (image + 1.0) / 2.0\n",
        "generated = torch.clamp(image, 0.0, 1.0)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(generated.squeeze(0).permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "# plt.imshow(generated.squeeze(0).cpu().numpy(), cmap='gray')\n",
        "plt.title(f\"Generated Maze, {test_path_length}\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# plt.imshow(original.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "plt.imshow(original.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "plt.title(\"Original Test Maze\")\n",
        "plt.axis(\"off\")\n",
        "file_name = f\"diffusion_image_generation_multi_feat_{formatted_time}\"\n",
        "plt.savefig(os.path.join(loss_curves_folder, file_name))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a01ba929",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2853145",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "classifierGuidance_3.11.0_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
