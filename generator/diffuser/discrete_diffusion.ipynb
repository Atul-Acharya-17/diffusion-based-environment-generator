{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from maze_dataset.plotting import MazePlot\n",
    "import random\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/atul/diffusion-based-environment-generator/generator\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/home/atul/diffusion-based-environment-generator/generator\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/grid/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Grid Worlds: 100%|██████████| 1/1 [00:00<00:00, 920.61it/s]\n"
     ]
    }
   ],
   "source": [
    "from maze.grid_world_generator import generate_multiple_grid_worlds\n",
    "\n",
    "generate_multiple_grid_worlds(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        # print(directory)\n",
    "        self.files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.npy')]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # print(self.files[idx])\n",
    "        grid = np.load(self.files[idx])\n",
    "        return torch.tensor(grid, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_world(grid):\n",
    "    \"\"\"\n",
    "    Plots the given grid world.\n",
    "    \"\"\"\n",
    "    wall = grid[:,:,0] == 0\n",
    "    source = grid[:,:,1] == 1\n",
    "    destination = grid[:,:,2] == 1\n",
    "\n",
    "    # Below is ChatGPT   \n",
    "    img = np.ones((*wall.shape, 3), dtype=np.float32)  # White background\n",
    "    img[wall] = np.array([0, 0, 0])  # Walls → Black\n",
    "    img[source] = np.array([0, 0, 1])  # Source → Blue\n",
    "    img[destination] = np.array([0, 1, 0])  # Destination → Green\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GridWorldDataset(\"/home/atul/diffusion-based-environment-generator/generator/data/grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_grid_text(grid):\n",
    "    \"\"\"\n",
    "    Displays the grid world in text format.\n",
    "    \"\"\"\n",
    "    height, width = grid.shape[1], grid.shape[2]\n",
    "    char_grid = np.full((height, width), '-', dtype=str)\n",
    "    \n",
    "    wall = grid[:, :, 0] == 0\n",
    "    source = grid[:, :, 1] == 1\n",
    "    destination = grid[:, :, 2] == 1\n",
    "    \n",
    "    char_grid = np.full(wall.shape, '-', dtype=str)  # Default to empty space\n",
    "    char_grid[wall] = '#'  # Walls\n",
    "    char_grid[source] = 'S'  # Source\n",
    "    char_grid[destination] = 'E'  # Destination\n",
    "    \n",
    "    print(\"\\n\".join(\"\".join(row) for row in char_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10, 3])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAHiCAYAAADf3nSgAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAACDZJREFUeJzt3DGOIzkQRcHKRd3/yjn+jgyhAPUbqiNsGR8EpQc6mt3dCwD4cf/VAwDgtxJhAIiIMABERBgAIiIMABERBoCICANARIQBICLCABC56wGfNjP1hIdO/SOzU8+bn3bqn/Wd+pty6nl/Oy9hAIiIMABERBgAIiIMABERBoCICANARIQBICLCABARYQCIiDAAREQYACIiDAAREQaAiAgDQESEASAiwgAQEWEAiIgwAEREGAAiIgwAEREGgIgIA0BEhAEgIsIAEBFhAIiIMABERBgAIiIMABERBoCICANARIQBICLCABARYQCIiDAAREQYACIiDAAREQaAiAgDQESEASAiwgAQEWEAiIgwAEREGAAiIgwAEREGgMjs7tYj4F8wM/WER3yF4VxewgAQEWEAiIgwAEREGAAiIgwAEREGgIgIA0BEhAEgIsIAEBFhAIiIMABERBgAIiIMABERBoCICANARIQBICLCABARYQCIiDAAREQYACIiDAAREQaAiAgDQESEASAiwgAQEWEAiIgwAEREGAAiIgwAEREGgIgIA0BEhAEgIsIAEBFhAIiIMABERBgAIiIMABERBoCICANARIQBICLCABARYQCIiDAAREQYACIiDACR+90Pzswnd/A/u1tPeOTke3LqmfOzTr3jJ9/vbz5zL2EAiIgwAEREGAAiIgwAEREGgIgIA0BEhAEgIsIAEBFhAIiIMABERBgAIiIMABERBoCICANARIQBICLCABARYQCIiDAAREQYACIiDAAREQaAiAgDQESEASAiwgAQEWEAiIgwAEREGAAiIgwAEREGgIgIA0BEhAEgIsIAEBFhAIiIMABERBgAIiIMABERBoCICANARIQBICLCABARYQCIiDAAREQYACIiDAAREQaAiAgDQOR+94O7+8kdAPDSN/fHSxgAIiIMABERBoCICANARIQBICLCABARYQCIiDAAREQYACIiDAAREQaAiAgDQESEASAiwgAQEWEAiIgwAEREGAAiIgwAEREGgIgIA0BEhAEgIsIAEBFhAIiIMABERBgAIiIMABERBoCICANARIQBICLCABARYQCIiDAAREQYACIiDAAREQaAiAgDQESEASAiwgAQEWEAiIgwAEREGAAiIgwAEREGgIgIA0BEhAEgctcDeG1m6gmP7G49AXjh1N+U6/ru3xUvYQCIiDAAREQYACIiDAAREQaAiAgDQESEASAiwgAQEWEAiIgwAEREGAAiIgwAEREGgIgIA0BEhAEgIsIAEBFhAIiIMABERBgAIiIMABERBoCICANARIQBICLCABARYQCIiDAAREQYACIiDAAREQaAiAgDQESEASAiwgAQEWEAiIgwAEREGAAiIgwAEREGgIgIA0BEhAEgIsIAEBFhAIiIMABERBgAIiIMABERBoDIXQ8AfqeZqSc8srv1hF/nm++KlzAAREQYACIiDAAREQaAiAgDQESEASAiwgAQEWEAiIgwAEREGAAiIgwAEREGgIgIA0BEhAEgIsIAEBFhAIiIMABERBgAIiIMABERBoCICANARIQBICLCABARYQCIiDAAREQYACIiDAAREQaAiAgDQESEASAiwgAQEWEAiIgwAEREGAAiIgwAEREGgIgIA0BEhAEgIsIAEBFhAIiIMABERBgAIiIMABERBoCICANA5K4H8F1mpp7w2O7WEx459cxPPe9TnXpPruu774qXMABERBgAIiIMABERBoCICANARIQBICLCABARYQCIiDAAREQYACIiDAAREQaAiAgDQESEASAiwgAQEWEAiIgwAEREGAAiIgwAEREGgIgIA0BEhAEgIsIAEBFhAIiIMABERBgAIiIMABERBoCICANARIQBICLCABARYQCIiDAAREQYACIiDAAREQaAiAgDQESEASAiwgAQEWEAiIgwAEREGAAiIgwAEREGgIgIA0DkfveDM/PJHR+zu/WER07dDcD7vIQBICLCABARYQCIiDAAREQYACIiDAAREQaAiAgDQESEASAiwgAQEWEAiIgwAEREGAAiIgwAEREGgIgIA0BEhAEgIsIAEBFhAIiIMABERBgAIiIMABERBoCICANARIQBICLCABARYQCIiDAAREQYACIiDAAREQaAiAgDQESEASAiwgAQEWEAiIgwAEREGAAiIgwAEREGgIgIA0BEhAEgIsIAEBFhAIiIMABERBgAIiIMAJG7HsB3mZl6AvDC7tYTeMFLGAAiIgwAEREGgIgIA0BEhAEgIsIAEBFhAIiIMABERBgAIiIMABERBoCICANARIQBICLCABARYQCIiDAAREQYACIiDAAREQaAiAgDQESEASAiwgAQEWEAiIgwAEREGAAiIgwAEREGgIgIA0BEhAEgIsIAEBFhAIiIMABERBgAIiIMABERBoCICANARIQBICLCABARYQCIiDAAREQYACIiDAAREQaAiAgDQESEASAyu7v1CP4219QTnjl09nVd16lfhZmDD/1A7gnveueueAkDQESEASAiwgAQEWEAiIgwAEREGAAiIgwAEREGgIgIA0BEhAEgIsIAEBFhAIiIMABERBgAIiIMABERBoCICANARIQBICLCABARYQCIiDAAREQYACIiDAAREQaAiAgDQESEASAiwgAQEWEAiIgwAEREGAAiIgwAEREGgIgIA0BEhAEgIsIAEBFhAIiIMABERBgAIiIMABERBoCICANARIQBICLCABARYQCIiDAARGZ3tx4BAL+RlzAAREQYACIiDAAREQaAiAgDQESEASAiwgAQEWEAiIgwAET+ANFhVrgycKSxAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_grid_world(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-#S###-#--\n",
      "----#-----\n",
      "##-#---#-#\n",
      "---#---#--\n",
      "--#----##-\n",
      "--#---#--#\n",
      "--##-#--#-\n",
      "#----#--#-\n",
      "#--###----\n",
      "--E#-##-##\n"
     ]
    }
   ],
   "source": [
    "display_grid_text(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from maze_dataset.plotting import MazePlot\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/atul/diffusion-based-environment-generator/generator\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=4):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # 10x10x32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # 5x5x64\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # 3x3x128\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, latent_dim * 2, kernel_size=3, padding=1)  # 3x3x(latent_dim * 2)\n",
    "        )\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def forward(self, x, noise):\n",
    "        x = self.encoder(x)  # (B, latent_dim*2, 3, 3)\n",
    "        \n",
    "        mean, log_var = torch.chunk(x, 2, dim=1)  # Split into two parts\n",
    "        log_var = torch.clamp(log_var, -10, 10)  \n",
    "        var = log_var.exp()\n",
    "        stdev = var.sqrt()\n",
    "\n",
    "        z = mean + stdev * noise  # Reparameterization trick\n",
    "        return mean, log_var, z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=4):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(latent_dim, 128, kernel_size=3, padding=1),  # 3x3x128\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # 6x6\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),  # 6x6x64\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # 12x12\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),  # 12x12x32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, kernel_size=3, padding=1),  # 12x12x3 (Extra padding)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Forward pass through decoder\n",
    "        x = self.decoder(z)  # Shape: (B, 3, 12, 12)\n",
    "\n",
    "        # Crop to 10x10 (assuming output is 12x12)\n",
    "        x = x[:, :, :10, :10]\n",
    "\n",
    "        # Process channels for discrete output\n",
    "        wall = torch.sigmoid(x[:, 0]) < 0.5 # Channel 1: Sigmoid for walls\n",
    "        \n",
    "        # Apply softmax over each pixel (across spatial dimensions) for source and destination\n",
    "        source = F.softmax(x[:, 1], dim=1)  # Shape: (B, 10, 10)\n",
    "        destination = F.softmax(x[:, 2], dim=1)  # Shape: (B, 10, 10)\n",
    "\n",
    "        # Ensure a single position is max for source/destination\n",
    "        source_idx = source.argmax(dim=1, keepdim=True)  # Shape: (B, 1, 10, 10)\n",
    "        destination_idx = destination.argmax(dim=1, keepdim=True)  # Shape: (B, 1, 10, 10)\n",
    "\n",
    "        # Create one-hot encoded source/destination\n",
    "        source_one_hot = torch.zeros_like(source).scatter_(1, source_idx, 1)\n",
    "        destination_one_hot = torch.zeros_like(destination).scatter_(1, destination_idx, 1)\n",
    "\n",
    "        # Stack the channels to return the final output\n",
    "        return torch.stack([wall, source_one_hot, destination_one_hot], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch.nn import functional as F\n",
    "# import math\n",
    "\n",
    "# class SelfAttention(nn.Module):\n",
    "#     def __init__(self, n_heads: int, d_embed: int, in_proj_bias=True, out_proj_bias = True):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias)\n",
    "#         self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
    "\n",
    "#         self.n_heads = n_heads\n",
    "#         self.d_head = d_embed // n_heads\n",
    "\n",
    "#     def forward(self, x: torch.Tensor, casual_mask=False):\n",
    "#         # x (Batch_size, Seq_Len, Dim)\n",
    "#         input_shape = x.shape\n",
    "#         batch_size, seq_len, d_embed = input_shape\n",
    "\n",
    "#         interim_shape = (batch_size, seq_len, self.n_heads, self.d_head)\n",
    "\n",
    "#         q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
    "\n",
    "#         q = q.view(interim_shape).transpose(1, 2)\n",
    "#         k = k.view(interim_shape).transpose(1, 2)\n",
    "#         v = v.view(interim_shape).transpose(1, 2)\n",
    "\n",
    "#         weight = q @ k.transpose(-1, -2)\n",
    "\n",
    "#         if casual_mask:\n",
    "#             mask = torch.ones_like(weight, dtype=torch.bool).triu(1)\n",
    "#             weight.masked_fill_(mask, -torch.inf)\n",
    "\n",
    "#         weight /= math.sqrt(self.d_head)\n",
    "\n",
    "#         weight = F.softmax(weight, dim=-1)\n",
    "\n",
    "#         output = weight @ v\n",
    "\n",
    "#         output = output.transpose(1, 2)\n",
    "\n",
    "#         output = output.reshape(input_shape)\n",
    "#         output = self.out_proj(output)\n",
    "\n",
    "#         return output\n",
    "\n",
    "\n",
    "# class CrossAttention(nn.Module):\n",
    "#     def __init__(self, n_heads: int, d_embed: int, d_cross: int, in_proj_bias=True, out_proj_bias = True):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.q_proj = nn.Linear(d_embed, d_embed, bias=in_proj_bias)\n",
    "#         self.k_proj = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
    "#         self.v_proj = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
    "#         self.out_proj = nn.Linear(d_embed, d_embed, bias = out_proj_bias)\n",
    "#         self.n_heads = n_heads\n",
    "#         self.d_head = d_embed // n_heads\n",
    "\n",
    "#     def forward(self, x, y):\n",
    "#         input_shape = x.shape\n",
    "#         batch_size, seq_len, d_embed = input_shape\n",
    "\n",
    "#         interim_shape = (batch_size, -1, self.n_heads, self.d_head)\n",
    "\n",
    "#         q = self.q_proj(x)\n",
    "#         k = self.k_proj(y)\n",
    "#         v = self.v_proj(y)\n",
    "\n",
    "#         q = q.view(interim_shape).transpose(1, 2)\n",
    "#         k = k.view(interim_shape).transpose(1, 2)\n",
    "#         v = v.view(interim_shape).transpose(1, 2)\n",
    "\n",
    "#         weight = q @ k.transpose(-1, -2)\n",
    "#         weight /= math.sqrt(self.d_head)\n",
    "#         weight = F.softmax(weight, dim=-1)\n",
    "\n",
    "#         output = weight @ v\n",
    "#         output = output.transpose(1, 2).contiguous()\n",
    "#         output = output.view(input_shape)\n",
    "#         output = self.out_proj(output)\n",
    "\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VAE_AttnBlock(nn.Module):\n",
    "#     def __init__(self, channels: int):\n",
    "#         super().__init__()\n",
    "#         self.groupnorm = nn.GroupNorm(32, channels)\n",
    "#         self.attention = SelfAttention(1, channels)\n",
    "\n",
    "#     def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "#         residue = x\n",
    "#         n, c, h, w = x.shape\n",
    "\n",
    "#         x = x.view(n, c, h * w)\n",
    "#         x = x.transpose(-1, -2)\n",
    "\n",
    "#         x = self.attention(x)\n",
    "#         x = x.transpose(-1, -2)\n",
    "        \n",
    "#         x = x.view((n, c, h, w))\n",
    "\n",
    "#         x += residue\n",
    "#         return x\n",
    "        \n",
    "# class VAE_ResBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super().__init__()\n",
    "#         self.groupnorm_1 = nn.GroupNorm(32, in_channels)\n",
    "#         self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "#         self.groupnorm_2 = nn.GroupNorm(32, out_channels)\n",
    "#         self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "#         if in_channels == out_channels:\n",
    "#             self.residual_layer = nn.Identity()\n",
    "#         else:\n",
    "#             self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         # x: (Batch_size, In_channels, H, W)\n",
    "\n",
    "#         residue = x\n",
    "\n",
    "#         x = self.groupnorm_1(x)\n",
    "#         x = F.silu(x)\n",
    "#         x = self.conv_1(x)\n",
    "\n",
    "#         x = self.groupnorm_2(x)\n",
    "#         x = F.silu(x)\n",
    "#         x = self.conv_2(x)\n",
    "\n",
    "#         return x + self.residual_layer(residue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encoder\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch.nn import functional as F\n",
    "\n",
    "# class VAE_Encoder(nn.Sequential):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super().__init__(\n",
    "#             # Initial convolution\n",
    "#             nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            \n",
    "#             # Residual blocks\n",
    "#             VAE_ResBlock(64, 64),\n",
    "#             VAE_ResBlock(64, 64),\n",
    "            \n",
    "#             # First downsampling (32x32 -> 16x16)\n",
    "#             nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=0),\n",
    "            \n",
    "#             # Increased channels\n",
    "#             VAE_ResBlock(64, 128),\n",
    "#             VAE_ResBlock(128, 128),\n",
    "            \n",
    "#             # Second downsampling (16x16 -> 8x8)\n",
    "#             nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=0),\n",
    "            \n",
    "#             # Final processing\n",
    "#             VAE_ResBlock(128, 256),\n",
    "#             VAE_ResBlock(256, 256),\n",
    "#             VAE_AttnBlock(256),\n",
    "            \n",
    "#             # Output projection\n",
    "#             nn.GroupNorm(32, 256),\n",
    "#             nn.SiLU(),\n",
    "#             nn.Conv2d(256, 8, kernel_size=3, padding=1),\n",
    "#             nn.Conv2d(8, 8, kernel_size=1, padding=0)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x: torch.Tensor, noise: torch.Tensor) -> torch.Tensor:\n",
    "#         # x: (Batch_size, Channel, H, W)\n",
    "#         # noise: (Batch_size, out_channels, H/8, W/8)\n",
    "\n",
    "#         for module in self:\n",
    "#             if getattr(module, 'stride', None) == (2, 2):\n",
    "#                 x = F.pad(x, (0, 1, 0, 1))\n",
    "#             x = module(x)\n",
    "            \n",
    "#         # (Batch_size, 8, H, H / 8, W / 8) -> 2 tensors of shape (Batch size, 4, H/8, W/8)\n",
    "#         mean, log_var = torch.chunk(x, 2, dim=1)\n",
    "#         log_var = torch.clamp(log_var, -30, 20)\n",
    "#         var = log_var.exp()\n",
    "#         stdev = var.sqrt()\n",
    "\n",
    "#         print(x.shape)\n",
    "#         print(mean.shape)\n",
    "#         print(stdev.shape)\n",
    "#         print(noise.shape)\n",
    "\n",
    "#         x = mean + stdev * noise\n",
    "\n",
    "#         x *= 0.18215\n",
    "\n",
    "#         # return x\n",
    "#         return mean, log_var, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VAE_Decoder(nn.Sequential):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super().__init__(\n",
    "#             # Input projection\n",
    "#             nn.Conv2d(4, 4, kernel_size=1, padding=0),\n",
    "#             nn.Conv2d(4, 256, kernel_size=3, padding=1),\n",
    "            \n",
    "#             # Residual blocks\n",
    "#             VAE_ResBlock(256, 256),\n",
    "#             VAE_AttnBlock(256),\n",
    "#             VAE_ResBlock(256, 256),\n",
    "            \n",
    "#             # First upsampling (8x8 -> 16x16)\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "#             nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "#             VAE_ResBlock(128, 128),\n",
    "#             VAE_ResBlock(128, 128),\n",
    "            \n",
    "#             # Second upsampling (16x16 -> 32x32)\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "#             nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "#             VAE_ResBlock(64, 64),\n",
    "#             VAE_ResBlock(64, 64),\n",
    "            \n",
    "#             # Output projection\n",
    "#             nn.GroupNorm(32, 64),\n",
    "#             nn.SiLU(),\n",
    "#             nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         # x: (BATCH.SIZE, 4, H / 8, W / 8)\n",
    "\n",
    "#         print(x.shape)\n",
    "#         x /= 0.18215\n",
    "\n",
    "#         for module in self:\n",
    "#             x = module(x)\n",
    "\n",
    "#         print(\"Decoder\")\n",
    "#         print(x.shape)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 300\n",
    "LATENT_CHANNELS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(f'Training dataset size: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "encoder = VAE_Encoder().to(device)\n",
    "decoder = VAE_Decoder().to(device)\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_hat, mean, log_var):\n",
    "    recon_loss = nn.functional.mse_loss(x_hat, x, reduction='sum') / x.size(0)\n",
    "    \n",
    "    kl_loss = 0.5 * (mean.pow(2) + log_var.exp() - log_var - 1).sum(dim=(1, 2, 3)).mean()\n",
    "    \n",
    "    return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1 [00:00<?, ?batch/s]/tmp/ipykernel_421795/1042413475.py:2: UserWarning: Using a target size (torch.Size([1, 3, 10, 10])) that is different to the input size (torch.Size([1, 3, 8, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  recon_loss = nn.functional.mse_loss(x_hat, x, reduction='sum') / x.size(0)\n",
      "Training:   0%|          | 0/1 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 2, 2])\n",
      "torch.Size([1, 4, 2, 2])\n",
      "torch.Size([1, 4, 2, 2])\n",
      "torch.Size([1, 4, 2, 2])\n",
      "Decoder\n",
      "torch.Size([1, 3, 8, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (10) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m mean, log_var, z \u001b[38;5;241m=\u001b[39m encoder(x, noise)\n\u001b[1;32m     16\u001b[0m x_hat \u001b[38;5;241m=\u001b[39m decoder(z)\n\u001b[0;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mvae_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[115], line 2\u001b[0m, in \u001b[0;36mvae_loss\u001b[0;34m(x, x_hat, mean, log_var)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvae_loss\u001b[39m(x, x_hat, mean, log_var):\n\u001b[0;32m----> 2\u001b[0m     recon_loss \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m     kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (mean\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m log_var\u001b[38;5;241m.\u001b[39mexp() \u001b[38;5;241m-\u001b[39m log_var \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m recon_loss \u001b[38;5;241m+\u001b[39m kl_loss\n",
      "File \u001b[0;32m~/diffusion-based-environment-generator/venv/lib/python3.10/site-packages/torch/nn/functional.py:3884\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3882\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3884\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3887\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n",
      "File \u001b[0;32m~/diffusion-based-environment-generator/venv/lib/python3.10/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (10) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, x in enumerate(tqdm(dataloader, desc=\"Training\", unit=\"batch\")):\n",
    "        x = x.to(torch.float32).to(device).permute(0, 3, 1, 2)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        noise = torch.randn(batch_size, LATENT_CHANNELS, 2, 2).to(device)\n",
    "        \n",
    "        mean, log_var, z = encoder(x, noise)\n",
    "        x_hat = decoder(z)\n",
    "        \n",
    "        loss = vae_loss(x, x_hat, mean, log_var)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * batch_size\n",
    "        \n",
    "    train_loss = train_loss / len(dataloader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_loss:.4f}')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, EPOCHS + 1), train_losses, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('VAE Training Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_grid_world(grid):\n",
    "    \"\"\"\n",
    "    Converts a 3-channel grid world into an RGB image for visualization.\n",
    "    - First channel: Wall (0 or 1)\n",
    "    - Second channel: Source (1 if source)\n",
    "    - Third channel: Destination (1 if destination)\n",
    "    \"\"\"\n",
    "    # Extract channels\n",
    "\n",
    "    print(grid[:, :, 0])\n",
    "    wall = grid[:, :, 0] < 0.5\n",
    "    source = grid[:, :, 1] >= 0.5\n",
    "    destination = grid[:, :, 2] >= 1\n",
    "    \n",
    "    # Create an RGB image with a white background (1, 1, 1)\n",
    "    img = np.ones((*wall.shape, 3), dtype=np.float32)  # White background\n",
    "    \n",
    "    # Set walls to black (0, 0, 0)\n",
    "    img[wall] = np.array([0, 0, 0])\n",
    "    \n",
    "    # Set source to blue (0, 0, 1)\n",
    "    img[source] = np.array([0, 0, 1])\n",
    "    \n",
    "    # Set destination to green (0, 1, 0)\n",
    "    img[destination] = np.array([0, 1, 0])\n",
    "    \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, LATENT_CHANNELS, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Encoder and Decoder\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m _, _, z \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m reconstructed \u001b[38;5;241m=\u001b[39m decoder(z)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m original \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/diffusion-based-environment-generator/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/diffusion-based-environment-generator/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[93], line 54\u001b[0m, in \u001b[0;36mVAE_Encoder.forward\u001b[0;34m(self, x, noise)\u001b[0m\n\u001b[1;32m     51\u001b[0m var \u001b[38;5;241m=\u001b[39m log_var\u001b[38;5;241m.\u001b[39mexp()\n\u001b[1;32m     52\u001b[0m stdev \u001b[38;5;241m=\u001b[39m var\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[0;32m---> 54\u001b[0m x \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m+\u001b[39m \u001b[43mstdev\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\n\u001b[1;32m     56\u001b[0m x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.18215\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# return x\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 3"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    fig, axes = plt.subplots(3, 5, figsize=(15, 8))  # Create 3x5 grid of subplots\n",
    "    \n",
    "    for i in range(5):\n",
    "        idx = np.random.randint(len(dataset))\n",
    "        img = dataset[idx]  # Ignore the path length label for reconstruction\n",
    "        img = img.unsqueeze(0).to(torch.float32).to(device).permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Generate noise for reparameterization trick\n",
    "        noise = torch.randn(1, LATENT_CHANNELS, 3, 3).to(device)\n",
    "        \n",
    "        # Encoder and Decoder\n",
    "        _, _, z = encoder(img, noise)\n",
    "        reconstructed = decoder(z).cpu().squeeze(0)\n",
    "        \n",
    "        original = img.cpu().squeeze(0)\n",
    "        \n",
    "        # Convert the grid world to RGB for visualization\n",
    "        original_rgb = visualize_grid_world(original.permute(1, 2, 0).numpy())\n",
    "        reconstructed_rgb = visualize_grid_world(reconstructed.permute(1, 2, 0).numpy())\n",
    "        \n",
    "        # Compute the absolute difference between original and reconstructed\n",
    "        difference = np.abs(original_rgb - reconstructed_rgb)\n",
    "        \n",
    "        # Original Image\n",
    "        axes[0, i].imshow(original_rgb)\n",
    "        axes[0, i].axis('off')\n",
    "        axes[0, i].set_title('Original')\n",
    "        \n",
    "        # Reconstructed Image\n",
    "        axes[1, i].imshow(reconstructed_rgb)\n",
    "        axes[1, i].axis('off')\n",
    "        axes[1, i].set_title('Reconstructed')\n",
    "        \n",
    "        # Difference Image (showing difference in RGB space)\n",
    "        axes[2, i].imshow(difference)\n",
    "        axes[2, i].axis('off')\n",
    "        axes[2, i].set_title('Difference')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
