{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rke55-mHMvQl",
    "outputId": "8efdf510-52eb-44f2-b77b-8137019b7cf0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting maze_dataset\n",
      "  Downloading maze_dataset-1.2.0-py3-none-any.whl (37.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting torch>=1.13.1\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting matplotlib>=3.7.0\n",
      "  Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting jaxtyping>=0.2.19\n",
      "  Downloading jaxtyping-0.2.38-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pandas>=2.2.2\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting tqdm>=4.65.0\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting frozendict>=2.4.4\n",
      "  Downloading frozendict-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting zanj>=0.4.0\n",
      "  Downloading zanj-0.4.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: ipykernel>=6.22.0 in ./.local/lib/python3.10/site-packages (from maze_dataset) (6.29.5)\n",
      "Collecting muutils>=0.8.3\n",
      "  Downloading muutils-0.8.3-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting jupyter>=1.0.0\n",
      "  Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Requirement already satisfied: pyzmq>=24 in ./.local/lib/python3.10/site-packages (from ipykernel>=6.22.0->maze_dataset) (26.3.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./.local/lib/python3.10/site-packages (from ipykernel>=6.22.0->maze_dataset) (0.1.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./.local/lib/python3.10/site-packages (from ipykernel>=6.22.0->maze_dataset) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.local/lib/python3.10/site-packages (from ipykernel>=6.22.0->maze_dataset) (5.7.2)\n",
      "Requirement already satisfied: psutil in ./.local/lib/python3.10/site-packages (from ipykernel>=6.22.0->maze_dataset) (7.0.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in ./.local/lib/python3.10/site-packages (from ipykernel>=6.22.0->maze_dataset) (8.34.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./.local/lib/python3.10/site-packages (from ipykernel>=6.22.0->maze_dataset) (1.8.13)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in ./.local/lib/python3.10/site-packages (from ipykernel>=6.22.0->maze_dataset) (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./.local/lib/python3.10/site-packages (from ipykernel>=6.22.0->maze_dataset) (0.2.2)\n",
      "Requirement already satisfied: nest-asyncio in ./.local/lib/python3.10/site-packages (from ipykernel>=6.22.0->maze_dataset) (1.6.0)\n",
      "Requirement already satisfied: tornado>=6.1 in ./.local/lib/python3.10/site-packages (from ipykernel>=6.22.0->maze_dataset) (6.4.2)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.10/site-packages (from ipykernel>=6.22.0->maze_dataset) (24.2)\n",
      "Collecting wadler-lindig>=0.1.3\n",
      "  Downloading wadler_lindig-0.1.3-py3-none-any.whl (20 kB)\n",
      "Collecting jupyter-console\n",
      "  Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: jupyterlab in ./.local/lib/python3.10/site-packages (from jupyter>=1.0.0->maze_dataset) (4.3.5)\n",
      "Collecting notebook\n",
      "  Downloading notebook-7.3.2-py3-none-any.whl (13.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nbconvert in ./.local/lib/python3.10/site-packages (from jupyter>=1.0.0->maze_dataset) (7.16.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/lib/python3/dist-packages (from matplotlib>=3.7.0->maze_dataset) (4.29.1)\n",
      "Collecting numpy>=1.23\n",
      "  Downloading numpy-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.local/lib/python3.10/site-packages (from matplotlib>=3.7.0->maze_dataset) (2.9.0.post0)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib>=3.7.0->maze_dataset) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.7.0->maze_dataset) (1.3.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib>=3.7.0->maze_dataset) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.7.0->maze_dataset) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=2.2.2->maze_dataset) (2022.1)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting nvidia-curand-cu12==10.3.5.147\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.local/lib/python3.10/site-packages (from torch>=1.13.1->maze_dataset) (4.12.2)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting triton==3.2.0\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nvtx-cu12==12.4.127\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nccl-cu12==2.21.5\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch>=1.13.1->maze_dataset) (3.6.0)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.13.1->maze_dataset) (3.0.3)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.22.0->maze_dataset) (3.0.50)\n",
      "Requirement already satisfied: exceptiongroup in ./.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.22.0->maze_dataset) (1.2.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/lib/python3/dist-packages (from ipython>=7.23.1->ipykernel>=6.22.0->maze_dataset) (4.8.0)\n",
      "Requirement already satisfied: stack_data in ./.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.22.0->maze_dataset) (0.6.3)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/lib/python3/dist-packages (from ipython>=7.23.1->ipykernel>=6.22.0->maze_dataset) (2.11.2)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.22.0->maze_dataset) (0.19.2)\n",
      "Requirement already satisfied: decorator in /usr/lib/python3/dist-packages (from ipython>=7.23.1->ipykernel>=6.22.0->maze_dataset) (4.4.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/lib/python3/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=6.22.0->maze_dataset) (2.5.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->maze_dataset) (1.16.0)\n",
      "Collecting widgetsnbextension~=4.0.12\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "Collecting jupyterlab-widgets~=3.0.12\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in ./.local/lib/python3.10/site-packages (from jupyterlab->jupyter>=1.0.0->maze_dataset) (2.15.0)\n",
      "Requirement already satisfied: httpx>=0.25.0 in ./.local/lib/python3.10/site-packages (from jupyterlab->jupyter>=1.0.0->maze_dataset) (0.28.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from jupyterlab->jupyter>=1.0.0->maze_dataset) (59.6.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in ./.local/lib/python3.10/site-packages (from jupyterlab->jupyter>=1.0.0->maze_dataset) (0.2.4)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in ./.local/lib/python3.10/site-packages (from jupyterlab->jupyter>=1.0.0->maze_dataset) (2.27.3)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in ./.local/lib/python3.10/site-packages (from jupyterlab->jupyter>=1.0.0->maze_dataset) (2.2.5)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in ./.local/lib/python3.10/site-packages (from jupyterlab->jupyter>=1.0.0->maze_dataset) (2.0.4)\n",
      "Requirement already satisfied: tomli>=1.2.2 in ./.local/lib/python3.10/site-packages (from jupyterlab->jupyter>=1.0.0->maze_dataset) (2.2.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3/dist-packages (from nbconvert->jupyter>=1.0.0->maze_dataset) (4.10.0)\n",
      "Requirement already satisfied: bleach[css]!=5.0.0 in ./.local/lib/python3.10/site-packages (from nbconvert->jupyter>=1.0.0->maze_dataset) (6.2.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /usr/lib/python3/dist-packages (from nbconvert->jupyter>=1.0.0->maze_dataset) (2.0.1)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in ./.local/lib/python3.10/site-packages (from nbconvert->jupyter>=1.0.0->maze_dataset) (3.1.2)\n",
      "Requirement already satisfied: defusedxml in /usr/lib/python3/dist-packages (from nbconvert->jupyter>=1.0.0->maze_dataset) (0.7.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in ./.local/lib/python3.10/site-packages (from nbconvert->jupyter>=1.0.0->maze_dataset) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in ./.local/lib/python3.10/site-packages (from nbconvert->jupyter>=1.0.0->maze_dataset) (1.5.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in ./.local/lib/python3.10/site-packages (from nbconvert->jupyter>=1.0.0->maze_dataset) (0.3.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in ./.local/lib/python3.10/site-packages (from nbconvert->jupyter>=1.0.0->maze_dataset) (5.10.4)\n",
      "Requirement already satisfied: webencodings in /usr/lib/python3/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->maze_dataset) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in ./.local/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->maze_dataset) (1.4.0)\n",
      "Requirement already satisfied: anyio in ./.local/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.local/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (3.3)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (2020.6.20)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.local/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.22.0->maze_dataset) (0.8.4)\n",
      "Requirement already satisfied: terminado>=0.8.3 in ./.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (0.18.1)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in ./.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (1.8.3)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in ./.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (0.5.3)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in ./.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (0.21.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in ./.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (1.8.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in ./.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (0.12.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in ./.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (23.1.0)\n",
      "Requirement already satisfied: overrides>=5.0 in ./.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (7.7.0)\n",
      "Requirement already satisfied: requests>=2.31 in ./.local/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->maze_dataset) (2.32.3)\n",
      "Requirement already satisfied: json5>=0.9.0 in ./.local/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->maze_dataset) (0.10.0)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in ./.local/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->maze_dataset) (4.23.0)\n",
      "Requirement already satisfied: babel>=2.10 in ./.local/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->maze_dataset) (2.17.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in ./.local/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert->jupyter>=1.0.0->maze_dataset) (2.21.1)\n",
      "Requirement already satisfied: wcwidth in ./.local/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=6.22.0->maze_dataset) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.local/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.22.0->maze_dataset) (2.2.0)\n",
      "Requirement already satisfied: pure-eval in ./.local/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.22.0->maze_dataset) (0.2.3)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.local/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.22.0->maze_dataset) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.local/lib/python3.10/site-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in ./.local/lib/python3.10/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (21.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.local/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->maze_dataset) (25.2.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.local/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->maze_dataset) (0.23.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.local/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->maze_dataset) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.local/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->maze_dataset) (0.36.2)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in ./.local/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (0.1.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in ./.local/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (3.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /usr/lib/python3/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (5.4.1)\n",
      "Requirement already satisfied: rfc3339-validator in ./.local/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (0.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->maze_dataset) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->maze_dataset) (1.26.5)\n",
      "Requirement already satisfied: ptyprocess in /usr/lib/python3/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (0.7.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in ./.local/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->maze_dataset) (24.11.1)\n",
      "Requirement already satisfied: fqdn in ./.local/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->maze_dataset) (1.5.1)\n",
      "Requirement already satisfied: jsonpointer>1.13 in ./.local/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->maze_dataset) (3.0.0)\n",
      "Requirement already satisfied: isoduration in ./.local/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->maze_dataset) (20.11.0)\n",
      "Requirement already satisfied: uri-template in ./.local/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->maze_dataset) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in ./.local/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (1.17.1)\n",
      "Requirement already satisfied: pycparser in ./.local/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->maze_dataset) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in ./.local/lib/python3.10/site-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->maze_dataset) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in ./.local/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->maze_dataset) (2.9.0.20241206)\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, mpmath, widgetsnbextension, wadler-lindig, tzdata, tqdm, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, muutils, jupyterlab-widgets, fsspec, frozendict, einops, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jaxtyping, contourpy, nvidia-cusolver-cu12, matplotlib, ipywidgets, torch, jupyter-console, zanj, notebook, jupyter, maze_dataset\n",
      "\u001b[33m  WARNING: The scripts proton and proton-viewer are installed in '/home/k/kavishs/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/k/kavishs/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[33m  WARNING: The script isympy is installed in '/home/k/kavishs/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/home/k/kavishs/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[33m  WARNING: The scripts torchfrtrace and torchrun are installed in '/home/k/kavishs/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[33m  WARNING: The script jupyter-console is installed in '/home/k/kavishs/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[33m  WARNING: The script jupyter-notebook is installed in '/home/k/kavishs/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "Successfully installed contourpy-1.3.1 einops-0.8.1 frozendict-2.4.6 fsspec-2025.3.0 ipywidgets-8.1.5 jaxtyping-0.2.38 jupyter-1.1.1 jupyter-console-6.6.3 jupyterlab-widgets-3.0.13 matplotlib-3.10.1 maze_dataset-1.2.0 mpmath-1.3.0 muutils-0.8.3 networkx-3.4.2 notebook-7.3.2 numpy-2.2.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pandas-2.2.3 sympy-1.13.1 torch-2.6.0 tqdm-4.67.1 triton-3.2.0 tzdata-2025.1 wadler-lindig-0.1.3 widgetsnbextension-4.0.13 zanj-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install maze_dataset einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.10/site-packages (0.21.0)\n",
      "Collecting diffusers\n",
      "  Downloading diffusers-0.32.2-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: torch==2.6.0 in ./.local/lib/python3.10/site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from torchvision) (2.2.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch==2.6.0->torchvision) (3.0.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch==2.6.0->torchvision) (3.6.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Collecting huggingface-hub>=0.23.2\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from diffusers) (2.32.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers) (4.6.4)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.23.2->diffusers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (24.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.67.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->diffusers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->diffusers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->diffusers) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->diffusers) (3.4.1)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, diffusers\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/k/kavishs/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[33m  WARNING: The script diffusers-cli is installed in '/home/k/kavishs/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "Successfully installed diffusers-0.32.2 huggingface-hub-0.29.3 regex-2024.11.6 safetensors-0.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision diffusers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.5.0-py3-none-any.whl (345 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.local/lib/python3.10/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./.local/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.local/lib/python3.10/site-packages (from accelerate) (0.29.3)\n",
      "Requirement already satisfied: psutil in ./.local/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.5)\n",
      "Installing collected packages: accelerate\n",
      "\u001b[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory, accelerate-launch and accelerate-merge-weights are installed in '/home/k/kavishs/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "Successfully installed accelerate-1.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: 1.15: No such file or directory\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy<1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fN1v7uBzPW04"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/kavishs/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/k/kavishs/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from maze_dataset import MazeDataset, MazeDatasetConfig\n",
    "from maze_dataset.generation import LatticeMazeGenerators\n",
    "\n",
    "cfg: MazeDatasetConfig = MazeDatasetConfig(\n",
    "\tname=\"test\", # name is only for you to keep track of things\n",
    "\tgrid_n=16, # number of rows/columns in the lattice\n",
    "\tn_mazes=10000, # number of mazes to generate\n",
    "\tmaze_ctor=LatticeMazeGenerators.gen_dfs, # algorithm to generate the maze\n",
    "    maze_ctor_kwargs=dict(do_forks=False), # additional parameters to pass to the maze generation algorithm\n",
    ")\n",
    "dataset: MazeDataset = MazeDataset.from_config(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9nTnACG8VfLE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/kavishs/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from maze_dataset.plotting import MazePlot\n",
    "import importlib\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.quantization\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rf_zCXcpVrNf"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 150\n",
    "LATENT_CHANNELS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 855
    },
    "id": "_X8Ou6fDUSBZ",
    "outputId": "5fbe59fe-a14e-449c-f87e-b808aeb4b81e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbQAAAGwCAYAAADBpZBQAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAbcpJREFUeJzt3Xl8E3X+P/DX5OxBaWk5esl9CrQUCuUoUAQKFTlE5JCjiK5fV1xl2a+r7H7xWNet7ve7ru7CD1eXRVYFvACVFWoptNxnKZfcQoGWXrSkbdokk5nP74+a0PRI0/aT5uj7+XjkIZNMXnk7Teadmcx8RmCMMRBCCCEeTuHqAgghhBAeqKERQgjxCtTQCCGEeAVqaIQQQrwCNTRCCCFegRoaIYQQr0ANjRBCiFdQuboAZ/P394fBYIBSqUTnzp1dXQ4hhJAmKiwshCRJ8PHxgV6vb3A+wdtPrFYqlZBl2dVlEEIIaSGFQgFJkhp83Ou30Go2ND8/P67ZjDFUVVVBqVRCq9VyzzYYDFAoFB6ZLQgCfHx8KPtnVVVVTss2GAwA4HHZzmQwGMAYg4+PDwRB8Jhso9EIWZYpu1a2pYkplUr7MzMvFxERwQAwPz8/7tkFBQUsLCyMzZ8/n3t2cXExi4yMZHPmzGGyLHPNLi0tZd26dWMzZszgnq3T6Vjv3r3Zww8/zD27oqKC9evXjyUmJjJJkrhm6/V6NmjQIJaQkMA922AwsOjoaBYfH89EUeSabTQaWWxsLIuLi2Mmk4lrtiiKbOTIkWzYsGHMaDRyzZZlmRkMBmYwGLi/TyRJYg899BAbOHAg0+v1XLNlWWaJiYmsb9++rLy8nHv2tGnTWK9evZhOp+OePWvWLNa1a1dWUlLCNZsxxubOncsiIiJYcXEx9+wnnniCAWAAWEREhN156aAQQgghXoEaGiGEEK9ADY0QQohXoIZGCCHEK1BDI4QQ4hWooRFCCPEKHtHQ1q5di+7du8PHxwdxcXE4duyYq0sihBDiZtz+xOrPP/8cK1euxAcffIC4uDi89957mDJlCi5dukRDWRHiTkQROH0a+PlkbKsxY4DaJ9vm5UG4dKn632p13cfr07MnEB7Op1bildy+ob377rv4xS9+gSeffBIA8MEHH+A///kP/vWvf+GVV16pM7/RaITRaLROM+8e2YsQ9yCKQFwccOpU/Y+paq1qvvoKmhUrmvYaa9YAy5c3u0Ti/dx6l6PJZMLJkycxadIk630KhQKTJk3C4cOH631OSkoKAgMDrbe8vLzWKpeQtuv06fqbGSGtyK0bWnFxMSRJQpcuXWzu79KlC/Lz8+t9zqpVq6DT6ay3cNpFQYjz1d7NSIgLuP0ux6bSarU2A+7yHiiTEOKAtWuBqKjqf9c3oOycOTANHgwAUKvVjn1Oe/bkWCDxRm7d0Dp27AilUomCggKb+wsKChAaGuqiqgghjYqKAuLjG348PBysY8fqf2s0jh0UQkgj3HqXo0ajwbBhw5Cenm69T5ZlpKenY9SoUS6sjBBCiLtx6y00AFi5ciWSk5MRGxuLESNG4L333oNer7ce9UgI8XKyXH2kpIVaDSjc+rs4cRG3b2jz5s1DUVERXn31VeTn52PIkCHYtWtXnQNFCCFe6tAhYOzY+9P799vfnUnaLLdvaADw/PPP4/nnn3d1GYQQQtyYRzQ0Qoib8/MDHnzQdpqQVkYNjRDSckOHAufPu7oK0sa1mYbGGENhYSHXTMuJ30ajkXt2aWkpZFmG0WhEUVER1+yysjLIsgyTycQ9u6KiApIkOSW7srLSJpvnOYYGgwFmsxmiKKKwsBAKjgcdmEwma3ZRURGU9Z2X1UyiKMJsNkOWZRQVFUFVe4ipFpAkCeLPB2MwxrgPI3fv3j0wxhofk5UxCDaTDLBTi6VWs9mMoqIi+Pr68in452yTyQRJklBUVITKykpu2QCs2cXFxTBwPlndaDRClmUUFxdb/668GAwGa7YkSdyzHSUwLx/sMDIyErm5uQDAfdQQy5taq9WiQ4cOXLNlWUZhYSE0Gg2Cg4O5ZxcVFUGtVjstW6VSITg4mGvTsXxglEol92zGGIqKiqBQKBASEuJR2cXFxQCqz9t0RvbgwYORkZEBtVrNLVuWZSxYsADl5eXYvn07NBpNg/MKBw9CW2P4O+Pu3WBjxtit++GHH8aBAwfQqVMn7sukpKQEZrMZnTp14vrFx5nZAFBSUgJRFJ2WbTKZ0LlzZ+7ZpaWlqKqqAgBERETg9u3bDc7bZrbQlEolxo0bxzXTaDRi586d6Ny5M/fz4kwmE3bt2oWQkBCMsfPhbQ5RFLFr1y4EBQVhbM2jxzgwm83YtWsXAgICMH78eK7ZkiRh165d8Pf3d0r2Dz/8AK1Wyz1blmX88MMPUKvVGDduHPcVbFpaGgRB4J4NVNfeo0cPKJVK7iuqu3fvoqysDIIg2M0Waj2mUCjA7MzPGENCQoJTVq4AkJGRAb1ej/j4eK5bxACwb98+lJWVOSV7//79KC0txZgxY7h+OQGAQ4cOoaioCKNHj7b75aQ5Dh8+jJycHMdmZl4uIiKCAWB+fn7cswsKClhYWBibP38+9+zi4mIWGRnJ5syZw2RZ5ppdWlrKunXrxmbMmME9W6fTsd69e7OHH36Ye3ZFRQXr168fS0xMZJIkcc3W6/Vs0KBBLCEhgXu2wWBg0dHRLD4+nomiyDXbaDSy2NhYFhcXx0wmE9dsWZaZwWBgBoOB+9/SbDaz+Ph4FhUVxaqqquzPvH+/ZSdj9W3/fruzy7LMjEajU+qWZZklJiayvn37svLycu7Z06ZNY7169WI6nY579qxZs1jXrl1ZSUkJ12zGGJs7dy6LiIhgxcXF3LOfeOIJBoABYBEREXbnbTNbaIQQJ7pyBXjnnfvTL78M9OnjunpIm0QNjRDScgUFwPr196eXLqWGRlodjR9DCCHEK1BDI4QQ4hVolyMhxL35+QE/XzvNOk1IPaihEULc29ChwJkzrq6CeAC33+W4b98+TJ8+HeHh4RAEAdu3b3d1SYQQQtyQ22+h6fV6REdHY9myZZg9e7aryyHEO1y6BDQ2NBljECxDJKnVwJAhQECA7Tw6HXD2bOttQV2+DDQ2zBxjEMzm6rPW1GogOhpo3952nrIy25p9fKrn43zCMWldbt/QkpKSkJSU5PD8RqMRRqPROs28e2QvQprnzTeBzz6zO4sAwGbMh2PHgOHDbWc6e9b2WmXO9qc/ARs32p1FAGDTlg4dAmqP5HP+fN26Y2KAo0epqXkwt9/l2FQpKSkIDAy03vLy8lxdEiFtj4+PqytoulOngNOnXV0FaQGva2irVq2CTqez3ngPSEwIaURMTPXuO0/EeYR70rrcfpdjU2m1Wmi1Wus078FaCfEKq1cDzz5rdxbGmPUyI2q1GkL//nVnGjwY2L///rSzf4v63e+Ap5+2Owv7+dIxjLHqugcOrDvTwIHVdZ85Ayxf7pxaSavzuoZGCHFAv37VN3sYAzOZqv+t0QD1fTkMDATi4/nX15C+fatv9jAGJorVv583VHf79q1bN2kVXrfLkRBCSNvk9ltoFRUVuHr1qnX6+vXryM7ORnBwMLp27erCygjxMKdOAU89dX96/frq37sI8RJu39BOnDiBCRMmWKdXrlwJAEhOTsbHH3/soqoI8UB6fXVTqzlNiBdx+4aWkJBA55IRQghpFP2GRgghxCu4/RYaIYQ4TefOQHKy7TTxWG2moTHGcPfuXa6ZpaWlkGUZJpPJqdklJSVcs8vKyiDLMkRR5J5dUVEBSZKckl1ZWWmTzfMcQ4PBAEmSYDabuWebTCabbKVSyS1bFEVIkmTNtLt7njEINpOserxDO8rKymA2m6HRaOzO11SyLMNsNkOSJJSUlNicO8qDKIpQKBTo2LGj/Rn79AE2bLC9z84ysSxfSZJQWlpqM8weD5a/Z2lpqfUcQF5MJhNkWbauW3gyGo3WbN5MllNHHCAwL/+BKjIyErm5uRAEAREREVyzZVlGfn4+fHx8EBwczD27oKAAGo0GISEhXLMZY8jPz4darW78A9+M7IKCAqhUKqdlK5VKdOrUiXt2YWEhFAoF92wAKCgogCAI6OyELYCCggIMGTIEe/bsgUrV8HdU4eBBaCdNsk4bd+8GGzOmwfklScL8+fORnZ3Ns1yrwsJCMMbQpUsXp+T36dMHW7duhQ/HYbgYY5gxYwYyMjLQpUsX7gM3FBcXQxRFhIaGcs++e/cuTCaTU7O7dOkChYLvL1klJSWorKwEAEREROD27dsNzttmttAUCgVGjhzJNdNoNCItLQ0hISGIi4vjmm0ymazZvOsWRRFpaWkICgrinm02m7F79260a9fOKdnp6enw8/Pjni1JEtLT06HVahEXF8f1Ay/LMtLT06FWq52SvXfvXgiCAIVCYXdlItR6TKFQgNmZX5ZlFBUVQa/XY8KECVxXVIwxZGZmQhRFjBgxgvtKMDMz0/olgnfdAODj44Phw4fb/QLRHAcPHkRZWZlTsg8dOoR79+4hNjYWas4juRw5cgR3795FbGws9635Y8eO4ebNm47NzLxcREQEA8D8/PyYLMtcbwUFBSwsLIzNnz+fe3ZxcTGLjIxkc+bM4Z5dWlrKunXrxmbMmME9W6fTsd69e7OHH36Ye3ZFRQXr168fS0xMZJIkcc3W6/Vs0KBBLCEhgXu2wWBg0dHRLD4+nomiyDXbaDSy2NhYFhcXx0wmk/359+2z7GRkDGDyvn125xdFkY0cOZINGzaMGY1GrnWbzWYWHx/PoqKiWFVVFddsSZLYQw89xAYOHMj0ej3XbFmWWWJiIuvbty8rLy/nnj1t2jTWq1cvptPpuGfPmjWLde3alZWUlHDPnjt3LouIiGDFxcXcs5944gkGgAFgERERdtf3bWYLDXDuuI7OHjPSU2unbDfKrvWYIAj1DwvVnOwW4pnNav2KQtnOza7NlePn0mH7hJC2KysLiIq6f8vKcnVFpAXa1BYaIYTYqKysvkhpzWnisWgLjRBCiFegLTRC2opRo4CKivvTnnhVaULsoIZGSFuhVAL+/q6ughCncftdjikpKRg+fDgCAgLQuXNnzJo1C5cuXXJ1WYQQQtyM2ze0zMxMLF++HEeOHEFaWhpEUURiYiL0dOkLQgghNbj9Lsddu3bZTH/88cfo3LkzTp48iXHjxtWZ32g02oyvVvv8C0IIId7J7RtabTqdDgAaHDsxJSUFb7zxRmuWRIhnuHMH+Pbb+9MzZgBhYa6rhxDOPKqhybKMFStWYMyYMRg0aFC986xatcp6VWsAGDBgAPLy8lqrRELc17VrwLPP3p8eOJAaGvEqHtXQli9fjnPnzuHAgQMNzqPVam0uReHKYVgIIYS0Ho9paM8//zx27NiBffv2ITIy0tXlEEIIcTNu39AYY/jVr36Fbdu2ISMjAz169HB1SYQQQtyQ2ze05cuXY9OmTfjmm28QEBCA/Px8AEBgYCB8fX1dXB0hxKONHg0YDPenOV8njLQutz8Pbd26ddDpdEhISEBYWJj19vnnn7u6NEKIp1MoAK32/o3zhUZJ63L7LTQ6j4wQQogj6OsIIYQQr0ANjRBCiFdw+12OvDDGcO/ePa6ZZWVlkGUZoihyz9bpdGCMQRRF6+gonpBdXl5us0x4ngeo1+ut2Tqdjmt2VVUVJEmC2WzGvXv3oOD4W4rRaLRm63Q6KJVKbtmiKEKSJGum3V30jEGwmWSAnfktWZIk4d69e9BoNDxKtmaazWZIkgSdTgdDzQMzWsjyHmGMWW8NyssDtm27P/3oo0B4eIOzW7JkWYZOp4PZbOZVNhhjNstElmVu2QCsy4T3ZwcATCaTNZvn+9uS7SiBefmPVJGRkcjNzYUgCOjevTvXbEmSkJubCx8fH3Tu3Nkp2VqtFl26dOGenZeXB41Gwz1blmXk5uZCrVajS5cuXD84siwjLy8PKpXKadlKpRKhoaFcsxljyMvLgyAICAsLc0p2dHQ09uzZA7Wdo/SEgwehmTjROm1KTwcbM6bB+c1mMyZOnIhTp04hPDyce9137twBY8wp2fn5+ejRowcOHjwIPz+/Budt6jJhjGH69OnIyMhAREQE1y8+jDEUFBRAFEXu2QBQUFAAk8mE8PBw7k2nsLAQBoMBERERTsm2DEYfERGB27dvNzhvm9lCUygUGDx4MNdMk8mEgoICBAUFOSW7sLDQKdmiKKKoqAjt27fnng3Amsn7A+nJ2Zah2pyV3adPHyiVSruNofZjgiAADszv6+uLQYMGca/d2cvkgQcegEql4rpMLLRaLQYNGsR95X306FGUl5dj4MCBUKn4rp6rqqqg0+kwcOBAu198muPEiROQJAkPPvgg1y15ADh58qTDV1dpMw1Nq9Vi+/btXDOLioowZMgQjB07Fps2beKafffuXcTExGDkyJH44osvuGbrdDoMGTIE0dHR3JcJUL2LTaFQcP/QANWNXhAEp2UD4P6BdJvsPn3A/u//rJOqPn3snnelUCggCAJ69+6Nr776invtbrFMajUNlUrV6LloCoUCERER2LJlC/w5XzB1+vTpuHjxIjZv3oyAgACu2bNnz0ZWVhY+++wzBAUFcc2eN28eDh06hE8++QQhISFcsxctWuTw+rXNNDTAueM68s6u95ujk/De3VMzj3d2Tc7ItmR6Wt0OZUdGAr/5TbNeRxAEj1omgiDU+ZvWq4lbaG7zt3Sj7Jp5vN8nTUVHORJCCPEK1NAIIYR4BWpohBBCvAI1NEIIIV6BGhohbcWBA9UHPFhudi6US4gncvuGtm7dOkRFRaF9+/Zo3749Ro0ahZ07d7q6LEIIIW7G7Q/bj4yMxNtvv40+ffqAMYaNGzdi5syZOHXqFAYOHOjq8toGUQROn7a9blR9wsKAnj3r3n/iROPPrS0+vu59t29DuHKl+rBglcqhE2DRuzcQGmp7n9kMHDliex9jEESx4ex27YAhQ+rmX7wIFBfbr+HnbADV5zjFxFTn1aTTAWfPNvq/YyM0FOjVq+79J08CVVV17z9zpmn5TVXf+yQ6Gqh9PpVOB5w5Y7tMGvtbdukC9OlT9/6sLKCy0vY+xiCYzdXDetWXPWaMY+8d4nHcvqFNnz7dZvqtt97CunXrcOTIkXobmtFohNFotE57+chezieKQFwccOpU4/M+9xywZk3d++fPB65dc/w1lcrqplPb5s3QvPyy4zkA8NFHwNNP295XWQmMHWtzlwDA7im4sbHA8eN173/jDWDLFrsl1Mk+eRIYOtR2plOngAkT7ObU8cwzwAcf1L3/iSeAy5ebltVSDb1Pjh0Dhg+3ve/sWQjjxtlf3rU99RTwz3/WvX/xYuDHH23uEgDYPTVaFOucUE28g0f9VSVJwpdffgm9Xo9Ro0bVO09KSgreeOONVq7Mi50+7VgzI57Hx4dflqe+T3r2BN5/33aaeCyPaGhnz57FqFGjYDAY0K5dO2zbtg0PPvhgvfOuWrUKK1eutE4PGDAAeXl5rVWq9+E4CjpxIzEx1bsDefHU90l4OPDCC66ugnDiEQ2tX79+yM7Ohk6nw1dffYXk5GRkZmbW29S0Wi20Wq112pXDsHiltWuBqKj6HwsLq//+LVv4rPAWLIApNhaCIDQ66KxV79517/PzA/bvt7nLcjmdBrNr/+Zl8dprwPLldkuwZAOAWq2G0Ldv3ZliYurU1Kjavw1abNpU/29oFj4+1c3MCeNhWlneJ/37131s8GCwfftsl4kjv6HV55NP6vyGZrkMC2Os/mzOAwoT9+ERDU2j0aD3zyumYcOG4fjx43j//ffxj3/8w8WVtUFRUfUfsGFR32+WsbF8XjsyEqxz5+of9B05kKAhKlXd/wfGwEympmfXt8KuzZINABpN/dmBgfaXq53sOoYNa3oOb/beJz//vza6TBxR+7dIoHp5/3ztrxZlE4/jEQ2tNlmWbQ78IE40aBCQlmY7TQghbsjtG9qqVauQlJSErl27ory8HJs2bUJGRgZSU1NdXVrbEBQETJrk6ioIIaRRbt/QCgsLsWTJEty5cweBgYGIiopCamoqJk+e7OrSCCGEuBG3b2jr1693dQmEEEI8gNsPfUUIIYQ4wu230IiL6fW2o3z06gVwvuw88QKdOgELF9pOE9LK2kxDY4yhvLyca2ZFRQVkWYbZbHZKtuV8moqKCqdkW9gdHiwrC8K4cffn3bfP7uHljDHo9XowxqDRNGlwo0YxxmAymSAIgsdmO3TOVROzm3Q+VxOYzWZIkmR9Hbvvk759q88Jsy3Obn5VVRXMZjM0Gg3380VNJpP1Pch7eZvNZsiyXOdzxDvbGXUzxlBRUQEV56G/xJ9Pk6ioqOD++bG8vx3RZhqawWDA0PrOWWkBSZJQXFyMXbt2OSW7oKAAu3fv5p4tyzLy8vIQFRUFo9Fo94MjiKLNmHuiKN4/f6geFRUVePzxx3Hz5k2nnNRuWYFQtvOzGWPIzc3Fgw8+CJOdv3lzSJKEZ555BllZWR61TAAgNzcXZrMZY8aMgULB91ebvLw8mEwmjB07lnv2nTt3YDQaMX78eCg5n1yen5+PqqoqTJgwwSnZjmozDU0QBPStb4SGFjCZTMjLy0NAQAD3bKB6hBQA3N/YANC3b18MHTq00ezaKwTBci0tO27evImysjIMrz0obQtJkoQDBw7A19cXI0aM4J598OBBaDQaxMXFcV0RyrKMgwcPQqVSYeTIkdyzDx8+DEEQMGrUKO7f6u/evQug+u/OM1sQBOTl5aGwsJB7Y2CM4dixYzAajRgzZgz3Fey9e/dQVVWFPn36cM8uKytDRUUFevfuzX0rqry8HLIso3fv3lBzHiVGr9fDbDajV69e3LfQKisrHd5L1WYamo+PD3bs2ME1s7CwEDExMRg/fjw2bdrENduym0qhUHB/8zUpu9aHSqVS2R0yybLba/jw4fjuu++4rgT1ej1iY2PRrVs37tlVVVWIi4tDx44d8d1333FdwRqNRowcORIBAQH49ttvua4ETSYT4uPjoVQq8c0333BdCZrNZowbNw6iKEKj0XB9HyqVSgiCgB49emDr1q3w4ThQsizLmDx5MgoKCvD111/Dz8+PWzZjDElJSbhx4wa+/PJLtGtoSLRmZs+YMQMXL17EF198gfbt23PLBoBHH30Up06dwpYtW9ChQweu2fPmzcOhQ4ewefNmhISEcM1etGiRw+vXNtPQAP67H2rmOXvMSJfV3owttJrz8v5Wb2+a5+t4St2eml07l3fdtW/O4EnZjDFrnjPqdmZ2U9Bh+4SQljt1qnpcRcvNEy8lQzxem9pCI4Q4iV5v28T0etfVQtos2kIjhBDiFaihEUII8QrU0AghhHgFj2pob7/9NgRBwIoVK1xdCiGEEDfjMQeFHD9+HP/4xz8QFRXl6lKaTxSB06cBg+H+fYIAjBlTd95btyBcuQJBoag+F8yRQ2H79Kl7qXpRBI4etb2PMQiiWH14bX3ZAQFAdLRj/0+EEOImPKKhVVRUYOHChfjoo4/wxz/+0e68RqPR5mrWPMdaaxFRBOLi6h7OrNEA9V19+9NPof3975v2Ghs2AEuX2t5XXg6MHWtzlwDA7rn8I0cChw9X/zskBJgz5/5jnE+aJIQQXjxil+Py5csxbdo0THLgyskpKSkIDAy03vLy8lqhQgecPu2Z5+YMGAB8+eX924ABrq6IEELq5fYNbcuWLcjKykJKSopD869atQo6nc56Cw8Pd3KFDqq5m5EQQgh3br3L8datW3jxxReRlpbm8FhvWq0WWq3WOu3KYVjsWrsWiIpq+LexRYtgjIuDQqGASqVy7P+jT5+69wUEAPv329xlueSIIAj1ZwcEOPg/QQgh7sOtG9rJkydRWFhoc/kUSZKwb98+rFmzBkajkfto160mKsrudcXwwANgnTuDKRTVgwE3tzGr1XVfh7HqS8AIQsuyCSHEjbh1Q5s4cSLOnj1rc9+TTz6J/v374+WXX/bcZkYIIYQ7t25oAQEBGDRokM19/v7+CAkJqXM/IcSFRo0Cal6ziuPlYAhxlFs3NEKIh1AqAX9/V1dB2jiPa2gZGRmuLoEQQogb8riG5rG0WqBbN9tpQggh3FBDay3DhwM3bri6CkII8VptpqExxqDnfNHByspK69BavIfYYoyhqqoKAKDR2B2oqlnZJpMJgiBwz66srIQsy5AkCXq9nut5gHq93iZboeA3LkBVVZXTso1GI2RZhizLqKys5Hp0rslkgizLAKqXvUrF7yNtNput2Ywx++/xO3eAb7+9Pz1jBhAW1uDslixZlqHX6yFJEpeaLZmSJFmzeX42GWM22Tzf3zWzeb9PGGMwm81gjKGyspL7596SrdfrHT5nuCnZjmozDc1gMGDYsGFcMyVJQlFREWRZhslk4pp97949zJ07F/n5+U45OdzyIeedLcsybt26hYKCAsTGxnLPzsnJQW5uLoYPH841mzGGGzdu4MaNG07JzsnJgUKhwIgRI7hn37x5EwAwYsQI7ivYmzdv4sEHH2z0/S1cvAjNL39pnTb17QtmZ9xPSZLAGMOVK1cwevRo7u/DW7duwWw2Iz4+3inZoihi7NixXL/4AMDt27dhMpkwbtw47tm5ubkwGAyYMGEC9+y8vDxUVVVh4sSJ3E+nunPnjsPztpmGJggCHnjgAa6ZJpPJujLh/aFRKpUICwuDWq12SkOTZRmCIDgl+4EHHvC4bFmWkZubC41Gw/19Issy8vLyoFKpEBkZyb3p5OfnA7i/bHh64IEH0Lt3bygUCrvZtR8TBMHuCfuW+dVqNSIjI7mvYAsLCyEIglOyi4qKAACRkZHcV953796FLMtOyS4pKYHZbEZERATXLXmg+gu4yWRCREQE1Go112ydToeysjKH5m0zDc3HxwepqalcM4uKijBkyBAoFAruf8SQkBBs2LDBKdlA9W4wZ2Vbdmd6UnZVVRVGjhyJjh07YufOndx3OY4aNQoBAQHYuXMn1xWVKIqIj4+HUqnE999/z31FZdky0zZ2EFOt11WpVNWj0DRAqVRCEAT07NkTO3bsaDy/CRhjmDx5MgoKCvDtt9/C19eXa3ZSUhJycnLwzTffwJ/zqQozZszAxYsXsW3bNgRwHoJu9uzZOHXqFL7++msEBQVxzZ4/fz4OHTqEL7/8EiGcr8ixePFibNq0yaF520xDA8D9m1rNLYVGvxlfuwa8//796RdfBHr1anB2xpg1n/cWCWPM+o3bGdnOrNtZ2TWzFAoF1/dKzSxBELhm16ybd7Zledf3WvUUUrcuB/8+vOuuuffBmcvEGdkWnlS3JRPg/9lpqjbV0Fzqzh3g73+/Pz13rt2GRgghpGnc/vIxxEu8/jrw5ptNe86bb1Y/jxBCHEANjbQOpRJ49VXHm9qbb1bPTwNQE0IcRLscSetYvbr6v6++ajtdH0sz+8Mf7M9HCCE1UEMjrceRpkbNjBDSTG6/y/H111+vc2Rb//79XV0Waa7Vq6ubVX27H6mZEUJawCO20AYOHIjdu3dbp3mfa+MSZ85U/3fgQKBDB9vH9HogKwuCyQRBoag+x6exQ6BDQoABA+p/ndonJTIGQRSrD7WtLzsuru45RIWFwOXLjf9/1czu2dN2QGaLiROBnJzq5mX578aN1Mxc6eRJ4Oeh1qx+/lsCqP/K5vauuE6IC3hEZ1CpVAgNDXVoXqPRCKPRaJ3mPcYiN8uXV/931y5gyhTbx27cgDBuHJp0qumjjwJbt9a9/7nngIMHbe4SANgdya2wEOjUyfa+H34AFi9utIya2ayh5jR5MlBZWf3v9euBjz8GJImamSs98USdLyyNvk/c9bNF2iyPaGhXrlxBeHg4fHx8MGrUKKSkpKBr1671zpuSkoI33nijlSt0AF3Bt2GSBGg01Mw8WY8ewF/+YjtNSCtz+9/Q4uLi8PHHH2PXrl1Yt24drl+/jrFjx6K8vLze+VetWgWdTme9hYeHt3LFDYiOBmJiXF2Fe1IqAZOp6eepkaa5ehX4r/+6f7t6lV92RASwcuX9W0QEv2xCHOT2W2hJSUnWf0dFRSEuLg7dunXDF198gaeeeqrO/Fqt1mZcOGcMkNssajVw9Chw+jRgMNy/f+DAuvN27w62bx9MJhMUCgVUKlXj/x8NjZ/2//5fnd/QGGMQf/6dq97s+sZ5S0wE9u+3X0Pt7J49658pLQ3417+qdzc+9ZTtb2gAbak5S34+8OGH96cXLwZ6967+96ZNdX5Ds/wtAThtkGxCeHL7hlZbUFAQ+vbti6s8v122FrUacOSSKv7+QHw8mNEIplDU/4O8o6Ki6t7HGJjJVJ3paHbnztW3xtTOrk96enUzq/mbWVPOUyP81XdpJcvfEqjeJUwNjbg5j2toFRUVuHbtGhY7cIACcUP2Ds2npkYIaQG3b2j//d//jenTp6Nbt27Iy8vDa6+9BqVSiQULFri6NNJUjpxnRk2NENJMbt/Qbt++jQULFuDu3bvo1KkT4uPjceTIEXSqfVg5cW9NOWmamhohpBncvqFt2bLF1SUQHpp6npllPklyXk2EEK/i9g2NeInmXAaGtswIIU3g9uehEUIIIY5oM1tojDFU1R6rroUMBgMYY5BlmXs2Y8x6HprEebebM7OB6uHHBEFwWrZKpYJarXbKsGaWvyXPy8gbjUbr+8RgMEDJ8RpvoijaLAe7y4QxCDaTrNHhq0wmE2RZhizLLazUliRJkGUZjDHr54gXWZat+QaDgev5c4wxSJJkrZvn3xKAdVkbDAaoGzrtpZlq1s17fWX5rDsz2xFtpqEZDAaMGDGCa6bZbEZxcTFSU1O5ZwPVb27LFQZ4s6xAPDF75MiRWLNmDdd8k8kExhhOnjyJuLg47ivBa9euQaFQYOTIkdyzf/rpJwwaNMjafBoiiKLN2IyiKN4/z6wekiThmWeewdmzZ7n/LRljuH79OmRZxpgxY7h+gWCM4caNGzCbzRg7dizXbAC4ceMGTCYTxo8fzz07JycHRqMRCQkJ3JtlTk4ODAYDJk6cyD375s2bqKysxKRJk7gPHn/r1i2H520zDU0QBHTs2JFrpiiK+Omnn6DVap2SfeLECbRv3x4D6xtNpAXMZjNOnDgBf39/DB48mGs24NxGLMsy2rdvzz3fkqVWq51yBK3l/eGMZdKxY0f079+/0WVS+zFBEBo9Wfr69eu4efMmhg4dyr0R3759G2azGR07duTeGHJzc8EYc1q2JEkICQnh3hju3LkDURTRsWNH7tn5+fkwmUwICQnh3nQKCgpgNBoREhLCfcuysLDQ4XnbTEPz8fGxuQQND0VFRYiJiUFCQgI+/fRTrtklJSUYOnQo4uLi8Pnnn3PN1ul0GDp0KKKjo7G1vhH6W8hkMkEQBO5v7JrZGo3dceCbzGw2QxAEDBkyBD/88AP3laDp5y0h3nVbsgVBsBnyrV61VmIqlarh0VwAKBQKCIKAvn37YufOnVxrl2UZDz30EMrKyrBz587Ga28CxhgSExNRUFCA//znP/D19eWa/fDDDyMnJwc7duyAv78/t2wAmDlzJi5cuIBvv/0WAQEBXLMfe+wxnDp1Ctu3b0dQfcPbtcCCBQtw6NAhbNu2DcHBwVyzFy9ejM2bNzs0b5tpaAC4f+OxfOCdlQ1Uf4uu+To8sy3/5v3N25LpzGzAeeN0KpVK7rvALHnOWiYWdrOjooC9e+/PGxXl8HBWSqWS+3vcQqFQcM2uuYeAdzZjzLqMnZENwOPqBtAq2Y5oUw2NkDYtMBBISHB1FYQ4DR22TwghxCtQQyOEEOIVqKERQgjxCvQbGiFtRUUFcPny/em+fYF27VxXDyGcuf0WWm5uLhYtWoSQkBD4+vpi8ODBOHHihKvLIsTzZGdXX8jTcsvOdnVFhHDl1ltopaWlGDNmDCZMmICdO3eiU6dOuHLlCjp06ODq0ggBbt8GbtxofD7GIIhi9b8HDADCwmwfN5uBI0ea9trt2gFDhtS9/+JFoLi4/uecOdO01yDEw7h1Q3vnnXfwwAMPYMOGDdb7evToYfc5RqMRRqPROu2M8f4IAQBs3gz89reNziYA1iGn2IcfAr/4he0MlZXA2LFNe+3YWOD48br3v/EGQJdcIm2UW+9y/PbbbxEbG4vHH38cnTt3RkxMDD766CO7z0lJSUFgYKD1lpeX10rVEuJhfHxcXQEhXLl1Q/vpp5+wbt069OnTB6mpqfjlL3+JF154ARs3bmzwOatWrYJOp7PewsPDW7FiQjxETAwQHe3qKgjhyq13OcqyjNjYWPzpT38CAMTExODcuXP44IMPkJycXO9ztFqtzbhwzhoeiRAsWACMGtXobIwxiD//hqYeMKDuDH5+wP79TXvtho5OfO01YPly+8/18aluZk4Ya5MQV3LrhhYWFoYHH3zQ5r4BAwbg66+/dlFFhNQQGVl9awxj9y/TUt8AvyoVEB/Pp6b+/fnkEOKB3HqX45gxY3Dp0iWb+y5fvoxu3bq5qCJCCCHuyq0b2q9//WscOXIEf/rTn3D16lVs2rQJH374IZY3tkuFEEJIm+PWDW348OHYtm0bNm/ejEGDBuHNN9/Ee++9h4ULF7q6NNKWHThQvZvQcjtwwNUVEULg5r+hAcAjjzyCRx55xNVlEGJLklxdASGkFrfeQiOEEEIcRQ2NEEKIV3D7XY481RwSiweTyQTGGGRZ5p5tyZNlGSbLId9OyDYajVzP1WOMwWQyQRAE7sOOOTPb8re0vI7dfMYg2EwyoJF6RFG05jpjeVv+zTPbbDZbazYajVyXuSRJkGXZms2zblmWrTej0QilUskt2/J5t9St5nguX+1snusUxhikn3eT884G4NRsWZYdnrfNNLSqqirExcVxzTSbzSguLsYPP/zAPVuSJBQUFCA9Pd0p2Xl5eSgtLcXIkSO5ZgP3x890xkntzspmjOHatWvo2LEjTCYTFIqGd14IooiaZ5OJonj/PLN6iKKIJ598EpcvX/aoZQIAV65cAWMM8fHx3POvXr0KSZIwbtw47tnXrl2D2WxGQkIC9+yffvoJJpMJEyZMsPs+aY7r16/DYDBg4sSJ3LNv3LiByspKTJ48mWuTB4CcnBzo9XpMmTLFKdmOajMNTRAEtON87SdRFCEIAlQqlUdlm81mKBQKp2QD979R8f5AOjs7JiYGDz74oFOa5eXLl3Hjxg0MGjSI+xbamZ9H0Y+KiuKerVQqIcsy/P39uS5zSzZjzGnZzqgbAJRKJQRBgL+/P/eVt1KphEKhcHq2SsV31W9ZJn5+fly3WgE0Ka/NNDQfHx9kZGRwzSwqKsLQoUMxfvx4fPrpp1yzS0pKMGzYMMTFxWEL59HTdTodhg0bhqioKGzdupVrNgDrbkHeb+zWyq45dFq9ar22Wq2ufwSQn1l2BQ4ePBjp6elcV1SiKGLcuHFQKBRIT0/nuqKSJAnjx4+HKIpIT0/nusxlWcZDDz2E8vJypKWlNb7Mm4AxhilTpqCgoACpqanw9fXlmj1t2jTcuHEDu3btgr+/P7dsAJg5cyYuXryI77//HgEBAVyz58yZg1OnTmHHjh0ICgrimr1gwQIcPnwY3333HYKDg7lmL1myBJs3b3Zo3jbT0AA47VuJZUuKdzZQvWVpeR1PyLZ8O/bkbKCR3Xe1HhMEoc59dR7H/eXN871i2WJ1RrYl1xnZkiRZl4szlomz6rZ8OfHUbID/8gbu7zFx1nvQ4Tq4vjIhhBDiItTQCCGEeAVqaIQQQrxCm/oNjRAufHyAXr1spwkhLkcNjZCmio0Frl51dRWEkFrcfpdj9+7drUf+1LzRJWQIIYTU5PZbaMePH7cOqwIA586dw+TJk/H444+7sKo26NYtwJEz9hmD8PNJ4RgwAAgNtX1cFIGjR5v22gEBQHR03fsvXADu3m1aVkwMUPvcoXv3gLNn79etUtk9DB8AEBZmu9uREOJybt/QOnXqZDP99ttvo1evXhg/fny989ceS4z3mH9t1qefAr/7XaOzCYB1WCj2r38BTz5pO0N5OTB2bNNee+RI4PDhuve/+irw1VdNyzp9GoiKsr3vxAkIkyej4VOj6/Hcc8DatU17bUKIU7n9LseaTCYTPv30UyxbtqzBk+1SUlIQGBhoveXl5bVylYQQQlzBoxra9u3bce/ePSxdurTBeVatWgWdTme9hYeHt16BhBBCXMbtdznWtH79eiQlJdltUlqt1mZcOGeMQN4mLVrk0K5Cxtj9gZUHDKg7Q0AAsH9/0167oTHt/vAH4MUXm5ZV3+9esbFg+/bZDAjd6PsmLKxpr0sIcbpmNbQlS5ZgwoQJGDduHHq10g/jOTk52L17t1MG0yUOeOCB6ltjGKu+lIog1BnEF0D1ffHxfGqqr2E2R1AQEB9vWzd9ESLE4zRrl6NGo0FKSgr69OmDBx54AIsWLcI///lPXLlyhXd9Vhs2bEDnzp0xbdo0p70GIYQQz9WshvbPf/4Tly9fxq1bt/DnP/8Z7dq1w1/+8hf0798fkZGRvGuELMvYsGEDkpOTuY/kTAghxDu06KCQDh06ICQkBB06dEBQUBBUKlWdw+x52L17N27evIlly5ZxzyaNOHgQ0Grv3w4edHVFhBBSr2Zt7vzud79DRkYGTp06hQEDBmD8+PF45ZVXMG7cOHTo0IF3jUhMTKTzyVyFMcBksp0mhBA31KyG9vbbb6NTp0547bXXMHv2bPTt25d3XYQQQkiTNKuhnTp1CpmZmcjIyMBf/vIXaDQajB8/HgkJCUhISKAGRwghpNU1q6FFR0cjOjoaL7zwAgDg9OnT+Otf/4rly5dDlmWbsRfdianmrjMORFEEYwyMMadkA9UHxFj+zTvbwu7uXMYg2EyyRnc7Ws7ncgZPzK75PhFFEbIsOy2b5655SZJssnmSZdkmW6HgN8YDY8yaL4oi98+mM7NrLhNn1A3AY7Md0ayGxhjDqVOnkJGRgYyMDBw4cABlZWWIiopqcIxFVzMYDBgzZgzXTLPZjLt37yItLc0p2QUFBdi7dy/3bEmScOfOHQwePLjRN58gijZjHIqiWH2+VgMqKyvx5JNP4vbt205pDpYVtidlM8Zw5coVKJVKjBs3jnv2hQsXIAgCxo0bx732CxcugDGG8ePHc8++ePEiJEnChAkTuGdfunQJoijioYce4tosAeDy5cswGo2YNGkS9+wrV67AYDBg8uTJUCqVXLOvXr2KyspKTJ06lXv2tWvXUFFRgaSkJO5Hov/0008Oz9usVw4ODkZFRQWio6Mxfvx4/OIXv8DYsWMRFBTUnLhWw/uPaPnmIAgC92zGmPVSOc7IdiaFQgGlUsl9JSVJEs6fPw+tVov+/ftzzZZlGefPn4darUb//v251i7LsvVvqVAouGbXfp84IxuA07Ity4R3Y6i5TJyVbXmf884Gqpe3s7KdVbdleTurbkc0q6F9+umnGDt2LNq3b9+cp7uEj48P9jd1yKVGFBUVYdiwYRg/fjw++eQTrtklJSWIjY1FXFwcNm/ezDVbp9MhNjYWCoUCGk0jY8zXGu1DrVYDdp6j0WjwySefQBCE6nk5qqysxIgRI9CtWzfs2LGD6wq2qqoKo0ePRseOHZGamsp1JWg0GhEfH4927dph9+7dXD/woihi3LhxUCqVyMjI4PrtWJIkJCQkQBRF7Nmzp/H3ShPIsoyJEyeirKwMe/bssRmurqUYY5gyZQoKCwuxe/du+Pr6cs1+5JFHcOPGDaSlpcG/9qWIWmjWrFm4ePEiUlNTEdDQkG/NNGfOHGRnZ2Pnzp3cNz6eeOIJHD58GP/5z38QHBzMNXvJkiXYsmWLQ/M2691fc7SO27dvA4BTTqjmjfcK1jLmnzNW3pYVk8NjCzYj28Judq3HBEGwOywUY8xarzPqtixvlUrFtelYfj+z/C15ZtfcklepVFybTs0tHZVKxfV9WPOCumq1mmu2JElOq1uWZeuWMO9sZy5vZ2db3tO8swG0SrZD8zbnBWRZxh/+8AcEBgaiW7du6NatG4KCgvDmm29y/cGbEEIIcVSzvir+/ve/x/r16/H2229bD1g4cOAAXn/9dRgMBrz11ltciySEEEIa06yGtnHjRvzzn//EjBkzrPdFRUUhIiICzz33HDU0Qgghra5ZDa2kpKTeo8z69++PkpKSFhdF3IhWC3TrZjtNCCFuqFm/oUVHR2PNmjV17l+zZg2io6NbXBRxI8OHAzdu3L8NH+7iggghpH7N2kL73//9Xzz88MPYvXs3Ro0aBQA4fPgwbt26he+//55bcZIk4fXXX8enn36K/Px8hIeHY+nSpfif//kfuhI1IYQQG01uaKIo4o033sD333+PH374ARcuXAAAzJ49G8899xzCw8O5FffOO+9g3bp12LhxIwYOHIgTJ07gySefRGBgoHXYLdKKrl0D7tyxPw9jECxDSKlUwKBBQO0rMOj1wKlT96d9fIDo6PqvcE0IIQ5qckNTq9U4c+YMwsLC8Mc//tEZNVkdOnQIM2fOtJ731r17d2zevBnHjh1r8DlGoxFGo9E6TZed4ej994G//93uLAJgM1QWdu0CpkyxnenGDWDsWNv7YmKAo0epqRFCmq1Zv6EtWrQI69ev511LHaNHj0Z6ejouX74MoHoQ5AMHDiApKanB56SkpCAwMNB6y8vLc3qdhINTp4DTp11dBSHEgzXrNzSz2Yx//etf2L17N4YNG1Zn+Jd3332XS3GvvPIKysrK0L9/fyiVSkiShLfeegsLFy5s8DmrVq3CypUrrdMDBgygpuYpDAZXV0AI8WDNamjnzp3D0KFDAcC69WTB82CNL774Ap999hk2bdqEgQMHIjs7GytWrEB4eDiSk5PrfY5Wq7UZF44OHuHoxReBuXPtzmK59IV16KtBg+rO1L07sH8/cOYMsHy5c2olhLQ5zWpoe/fu5V1HvV566SW88sormD9/PgBg8ODByMnJQUpKSoMNjThRr17VN3sYq768jCBU/x5W3xcKf38gPt45NRJC2iy+11XgrLKyss7AlEqlksaLJIQQUgffK7FxNn36dLz11lvo2rUrBg4ciFOnTuHdd9/FsmXLXF0aIYQQN+PWDe3vf/87Vq9ejeeeew6FhYUIDw/Hf/3Xf+HVV191dWmEEELcjFs3tICAALz33nt47733XF0KIYQQN+fWv6ERQgghjnLrLTTi5UJCgEcftZ0mhJBmalMNzWw2c82TJAmMMTDGuGdb8hhjkCTJY7Ity0KhUECtVtsfeqx/f+Drr2sH2M22/FeSJK5Hu9b+Wzblsu+Nac2/Jc/zLs1ms9OWSe3lzfPzI8uy07ItuZ6WDcD6efG07KYMXygwLx/sMDIyErm5uVAoFBgxYgTXbFEUcebMGbRv3x59+vThmm02m3HmzBm0a9cOffv25ZotSRLOnDkDPz8/9OvXj2s2UP0GHDlyJN555x2uuZWVlRg5ciQKCgowYMAArtmyLOPs2bNQq9V48MEHuWYzxnD27FkolUo8+OCDXJsOYwznzp2DIAgYOHAg9+zz58+DMYZBgwZxH6Tg/PnzkCQJgwcP5p79448/QhRFDB48mGsjBoALFy7AaDQiKiqKe/bFixdRVVWFqKgoKJVKrtmXLl2CXq9HVFQUVCq+2zJXrlxBeXm5U7KvXr2K4uJiAEBERARu377d4LxtZguNMQaTycQ10/INVpZlj8q2fDt2RjZQ3Rx4f0sD7n9Tc1bd/fr1gyAI3LMtWwyWunk3HcvN07Ity8VoNHJtDLXr5t10LHU7O5t3Q7NsRYmiyP1cXss6xWQyOSXbUW2mofn6+uLw4cNcM4uKihAbG4tx48bhk08+4Zp99+5djBgxAiNGjMDmzZu5Zt+7dw8jRozA4MGD8dVXX3FfUVk+jBqNpvEnNIFlSK24uDh899133FeClpW2Wq3mukyMRiPi4+MREBCAtLQ0risqk8mEhIQEKJVK7NmzB2qOVyswm8146KGHIIoiMjMzuf49JUnC5MmTUVZWhszMTPj4+HDLlmUZSUlJKCwsxN69e+Hn58ctmzGGRx55BDk5OUhPT0e7du24Zs+ePRsXL17E7t270b59e67Zc+fORXZ2NlJTU9Gh9iWdWmjhwoU4fPgwdu3ahRDOv4UnJydjy5YtDs3bZhoaAO4rWMuKT6FQcM+25Fl+i+K5gq2ZrdFouDc04P4YmjyzLVmWunk3NMtr8F7ejDEIgmDN5r1LxpKt0Wi4NjSFQmGTzbuhOStblmVr7byzGWNOzXbWMrHUDYB7NoBWyXZoXq6vTEhTnDlTPaaj5XbmjKsrIoR4sDa1hUbcTFkZcPCg7TQhhDQTbaERQgjxCtTQCCGEeAVqaIQQQryC2ze08vJyrFixAt26dYOvry9Gjx6N48ePu7osQgghbsbtDwp5+umnce7cOXzyyScIDw/Hp59+ikmTJuHHH39ERESEq8trO27erL41hjEIP58vhgcfBLp0sX1cFIGjR6v/TUc1EkI4cuuGVlVVha+//hrffPMNxo0bBwB4/fXX8d1332HdunX44x//WOc5RqMRRqPROu3lI3u1nn//G1i9utHZBACWs1DYv/8NLF5sO8O9e8DYsbyrI4QQ997laDabIUlSnVEEfH19ceDAgXqfk5KSgsDAQOstLy+vNUolPHAcLYIQ0va4dUMLCAjAqFGj8OabbyIvLw+SJOHTTz/F4cOHcefOnXqfs2rVKuh0OustPDy8lasmzRITA0RHu7oKQogHc+tdjgDwySefYNmyZYiIiIBSqcTQoUOxYMECnDx5st75tVottFqtdZr3KN5t1pIlQEJCo7MxxqxjLqrqG7U+KAjYv9/2Ph+f6mbGcdgmQkjb4/YNrVevXsjMzIRer0dZWRnCwsIwb9489OzZ09WltS1du1bfGsMYmMkECEL9DUqtrh7mihBCOHPrXY41+fv7IywsDKWlpUhNTcXMmTNdXRIhhBA34vZbaKmpqWCMoV+/frh69Speeukl9O/fH08++aSrSyOEEOJG3H4LTafTYfny5ejfvz+WLFmC+Ph4pKamcr1MBrHj0CHA3//+7dAhV1dECCH1cvsttLlz52Lu3LmuLqPtkmWgstJ2mhBC3JDbb6ERQgghjqCGRgghxCu4/S5HniRJ4p5nGVrLGdlA9XldMufdfLVrtTs8GGMQbCYZ0MhwYpIkQRAEKJXKFlRZf65taXyHNZNl2XreIs9sS5blb8nzvVL7feKsbEmSuGfX/OzwzJZlGYwxp9RtyQX4113zfcI7u2a+p2Y7os00NIPBgAkTJnDNFEURJSUlSE9P555tNptRVFSEzMxM7tmSJCE/Px+DBw+GyWSyO68gitaxGYHq/2dm5zmVlZV49tlnkZeXx/2kdlmWcfPmTTzwwAMwmUxc800mE5577jkEBgbi//7v/7hmi6IIxhjOnj2LSZMmccsFqj/sFy9ehCAImDRpEvdlfv78eTDGkJiYyD377NmzkCQJU6dOdUq2yWRCUlISFAq+O6LOnTsHg8GAadOmcc8+f/48Kisr8cgjj3D/Qvjjjz+ioqICM2bMgErFd9V/4cIFlJWVYdasWdyzL1686PC8baahMcag0+m4ZprNZsiyDFEUuWdLkuT0bGdgjKGiogI6nc4pDc1ZdUuShFOnTiEkJASMMa611/z2yvtvafk2LwgC92VuybZ8dnj/PS3f5p2RbRkHVqfTcW86oihClmWnZpeVlTk1m3eztHxpc1a2o9pMQ/P19cVRy2VLOCkqKsKIESMwbtw4bNy4kWt2SUkJ4uLiMGLECHz22Wdcs3U6HeLi4qBQKKDRaOzPXOv0CLVaDdh5jlqtxubNmyEIAvdTK/R6PUaPHm2tm+dK0NIULNk8VyaWBhkdHY0ffviB6wdeFEVMmDABCoUCe/bs4frt2Gw2Y9KkSRBFEXv37m38vdIEkiQhMTER5eXl2L9/v81wdS0lyzKmTZuGgoIC7Nu3D76+vtyyGWOYMWMGcnJykJGRAX9/f27ZADB79mxcunQJe/bsQUBAANfsuXPn4vTp09i9ezeCgoK4Zi9atAhHjhzBDz/8gODgYK7ZS5cuxeeff+7QvG2moQGoM2p/S2m1WutK0BnZAKBQKKyvw4vBYLCZtptd6zFBEOrcV5ulXrVa7ZSmY1OLEwiCwDXbkmX5W/JsOgqFwvoe1Gq1XL9EmM1m67Lw8fHh3tAstWu1Wq6fH1mWoVAorMuEZzZjzGl1Ozvb8kWKdzaAVsl2BB3lSAghxCtQQyOEEOIVqKERQgjxCm3qNzTSDBoNEBFhO00IIW6IGhqxb8QI4PZtV1dBCCGNcukux3379mH69OkIDw+HIAjYvn27zeOMMbz66qsICwuDr68vJk2ahCtXrrimWEIIIW7NpVtoer0e0dHRWLZsGWbPnl3n8T//+c/429/+ho0bN6JHjx5YvXo1pkyZgh9//JH7oaHEAT/9BOTl2Z+HMQiiWH2oukoFDBwI1D4vpbISyMq6P+3jA0RH13+Fa0IIcZBLG1pSUhKSkpLqfYwxhvfeew//8z//Y7069b///W906dIF27dvx/z58+t9ntFohNFotMkhnKxZA/z1r3ZnEQCbobKwYwcwbZrtTLduAWPH2t4XEwMcPUpNjRDSbG57lOP169eRn59vM/ZdYGAg4uLicPjw4Qafl5KSgsDAQOstr7EtCuIeTp0CTp92dRWEEA/mtg0tPz8fANClSxeb+7t06WJ9rD6rVq2CTqez3sLDw51aJ+Go1ggmhBDSFF53lKNWq7UZF85ZwyO1Sc8/D9TzW2dNjDGIP/+GplKpIAwcWHemBx4A9u8HzpwBli93UrGEkLbGbRtaaGgoAKCgoABhYWHW+wsKCjBkyBAXVdXG9exZfbOHserLywhC9e9h9X2h8PMD4uOdUyMhpM1y212OPXr0QGhoKNLT0633lZWV4ejRoxg1apQLKyOEEOKOXLqFVlFRgatXr1qnr1+/juzsbAQHB6Nr165YsWIF/vjHP6JPnz7Ww/bDw8Mxa9Ys1xVNCCHELbm0oZ04ccLmaswrV64EACQnJ+Pjjz/Gb3/7W+j1ejzzzDO4d+8e4uPjsWvXLjoHjRBCSB0ubWgJCQl2zxMTBAF/+MMf8Ic//KEVqyKEEOKJ3PY3NEIIIaQp3PYoR9IGBAcD06fbThNCSDO1qYYmyzLXPMaYdZcp7+yaeTVfx92zGWOQZRkKhcI63aABA4Bvvqkd0KTXchZnZVuWDy+1/5bOypZl2aOyLe9r3tm1P/O8sy3/5Z1dM99Tsx3RZhqawWDA5MmTuWaaTCaUlJRg79693LNFUURRUREyMzOdkl1QUIBDhw5h0qRJXE8+t3zghw8fjjfffJNrtslksn7YTSaTtWnyIIqitXbe2Za6z549iylTpnBdJrIs49KlSxAEAVOmTOFaN2MMP/74IxhjSEpK4p599uxZSJKEadOmcc/Ozs6GyWTCI488AqVSyTX79OnTqKqqwowZM7hnnzlzBpWVlZg5cyZUKn6rZ8vyrqiowKOPPgo15zFTz507B51Oh8cee4x79vnz5x2et800NMYY7ty5wzVTkiTIsoyqqiqnZEuSBIPBwD1blmVrtr1hxJqb/dNPP8HHx4f7Vk7tPN5blpb/OmOrFahubLyXd82RWQoKCrhmA/cbfX5+PvdRd0wmE2RZdkq20WiEJEkoKChwWnZ+fj7XRmzJNpvNKCgocEq2JEkoLCzknm0wGKzLm2eTB4CqqiqH520zDc3HxwcnTpzgmllUVISRI0di7Nix+Pjjj7lml5SUYOTIkRgxYgQ+/fRTrtk6nQ4jR47E4MGD8cUXX3DNLi8vx+jRo6FQKGyGIOPBsuJWKBTQaDRcV1SSJNlk895iEAQBMTEx2LVrF9cPvMlkwsSJE6FUKrF7926u3+rNZjMmT54MURSxZ88eaDherVySJEydOhVlZWXYt28f1/eKLMt45JFHUFhYiH379sHPz49bNmMMM2fORE5ODvbv3w9/f3+u2XPmzMGlS5eQmZmJgIAAbtkAMH/+fGRnZ2PPnj0ICgrimr148WIcOXIE6enpCOb8W/iyZcvw+eefOzRvm2logiBwfWMDgK+vLwRBgFKp5J5dVVVlzba8Di8mk8lp2Waz2aYZ8MyuneWscToFQXBK3QqFAr6+vlybjkqlgkKhsGbz3N1j+VsqFAr4+flxb2g16+Z5bqksy1Aqlda6eTc0S7avr6/HZQuCwD0bqH4fOiu7KV8A6bB94jpnzwIJCfdvZ8+6uCBCiCdrM1toxA3pdEBmpu00IYQ0E22hEUII8QrU0AghhHgFamiEEEK8gksb2r59+zB9+nSEh4dDEARs377d5vGtW7ciMTERISEhEAQB2dnZLqmTEEKI+3NpQ9Pr9YiOjsbatWsbfDw+Ph7vvPNOK1dGCCHE07j0KMekpCQkJSU1+PjixYsBADdu3HA402g0wmg0WqedOd4fIYQQ9+F1v6GlpKQgMDDQesvLy3N1SYQQQlqB1zW0VatWQafTWW/h4eGuLokQQkgr8LoTq7Varc24cM4aHokQQoh78botNEIIIW0TNTRCCCFewaW7HCsqKnD16lXr9PXr15GdnY3g4GB07doVJSUluHnzpvXAjkuXLgEAQkNDERoa6pKaCUcjRgA1D9oJCXFdLYQQj+fSLbQTJ04gJiYGMTExAICVK1ciJiYGr776KgDg22+/RUxMDKZNmwag+no+MTEx+OCDD1xWM+FIowHCwu7fOF6ehBDS9rh0Cy0hIcHueWJLly7F0qVLW68gQgghHot+QyOEEOIVqKERQgjxCl53Hpo9vIfBYoxZM52RbW+aV7ZL6y4qAnbvvj89aRLQqZNDr1Fz2fPgzOVdO9eZf0vKtn1/OCu79r95Zdf3OryynVW3JdNZ2U3RZhqawWCwO25kc5hMJpSWliIjI4N7tiiKKC4uxv79+7lnm81mFBQU4MiRI3j44Ye5Z+fl5aF3794wmUx2T2wXzp+HeuFC67S4Zw/YmDENzm8ymcAYQ1ZWlvVAIV5kWcaNGzcQEhICk8kEhYLfzgtL3efPn8cjjzzC9WR/xhguX74MQRAwffp07tkXLlwAYwzTp0/nukwsy8NsNmPmzJlcswHg9OnTMJlMmDVrFpRKJdfsrKwsVFVVYfbs2U7JrqiowGOPPQaViu/q+dSpUygrK8Pjjz8OtVrNNfv06dMoLS3FvHnzuGefOXPG4XnbTENjjOH69etcMyVJgtlshl6v96hsWZadmi2KIgAHtnLq2Sqy9xxBENCtWzen/C0ZY9bG46xv3gaDAdevX+fedIxGIwRBcFo2Yww3btzgPuqOwWCwfpHgnV1VVQVJknDjxg3uzbKqqgqiKDolu7KyEmazGTk5OU7N5t2IKyoqIEmS07Id1WYamo+PD06ePMk1s6ioCKNGjcLYsWOxYcMGrtklJSUYPXo0hg8fjk8++YRrtk6nw+jRozF48GBs2bKFa3Z5eTni4+OhUChshiCrV61vcmq12u6h+2q1Gp9//jkYY9BwPsS/qqoKCQkJUCgU0Gg03LdGBEHAkCFDsHPnTq4feFEUMWnSJCgUCqSlpXH9Vm82mzFlyhSIoojdu3dzXeaSJCEpKQnl5eXIyMho/L3SBLIsY8aMGSgsLERmZiZ8fX25ZTPGMGvWLNy8eRMHDhyAv78/1+y5c+fi0qVL2LdvHwICArhlA8CCBQtw+vRpZGRkICgoiGt2cnIyjhw5gj179iA4OJhr9lNPPYUvvvjCoXnbTEMTBAHt2rXjmllZWQmFQgGVSsU92/KtW6VSwd/fn+s3WLPZDEEQoFQquWfLsmzTDOxm13pMEIQ699Xm5+cHQRCgVqu51q1QKKx1C4LANduSZVnePJuOZfeoJZvn7h6z2WxdLu3atePe0JRKJRQKBfz9/eHj48MtW5Zlm2w/Pz9u2YwxqFQqazbPzz1jzKZu3tkqlQqCIHDPBuD0bEfRUY7EdWrv1qNr1xFCWoAaGml99+4B778PLFhge/+CBdX337vniqoIIR6OGhppXampQGQk8Otf247jCFRP//rX1Y+nprqmPkKIx6KGRlpPaiowbRpQVVW9e7G+XY6MVT8+bRo1NUJIk1BDI63j3j3gsceqG5Ys259Xlqvne+wx2v1ICHGYSxvavn37MH36dISHh0MQBGzfvt36mCiKePnllzF48GD4+/sjPDwcS5YssV5KhniYjRuBysrGm5mFLFfP/+9/O7cuQojXcGlD0+v1iI6Oxtq1a+s8VllZiaysLKxevRpZWVnYunUrLl26hBkzZrigUtIijAF//3vznvu3v9HRj4QQh7j0PLSkpKQGh3UKDAxEWlqazX1r1qzBiBEjcPPmTXTt2rXe5xmNRhiNRuu0K8cVIz+7exe4dq3pz2Os+nklJXTxT0JIozzqNzSdTgdBEOye5Z6SkoLAwEDrjXZRuoEmDF1Tr/JyPnUQQryaxzQ0g8GAl19+GQsWLED79u0bnG/VqlXQ6XTWW3h4eCtWSerV0pEDOA8BRAjxTh4x9JUoipg7dy4YY1i3bp3debVarc24cLwHPSXNEBIC9OoF/PRT034PEwSgZ0+A89hwhBDv5PZbaJZmlpOTg7S0NLtbZ8RNCQLwq18177kvvNDo+I6EEAK4eUOzNLMrV65g9+7dCKEDAzxXcjLg5wc4Ooq9QlE9/5Ilzq2LEOI1XLrLsaKiAlevXrVOX79+HdnZ2QgODkZYWBjmzJmDrKws7NixA5IkIT8/HwAQHBzM/fIhxMmCgoCvv64eAUShsH8+mkJRvVW2dWv18wghxAEu3UI7ceIEYmJiEBMTAwBYuXIlYmJi8OqrryI3Nxfffvstbt++jSFDhiAsLMx6O3TokCvLJs01ZQrwn/8Avr7VDav2rkTLfb6+wPffA4mJrqmTEOKRXLqFlpCQYPc8MTqHzAtNmQLcvl09Asjf/mZ7flrPntW/mSUnA4GBrquREOKRPOIoR+JlgoKqG9evflV90nR5efWh+cHBdAAIIaTZqKER1xGE6kP66WAfQggHbaqhOXMXJu/s2nk8852Z3dhr8czzpLo9Mbv2svak96ClXk+r21OzLXm8l3dTtZmGZjQaMX36dK6ZJpMJJSUl1qsG8CSKIoqLi3Hw4EHu2WazGYWFhTh69KhTsvPy8lBWVsY9GwBkWcaQIUPw2muvcT1pXhRF64fRZDJB4ejpBQ4wmUxgjOHHH3/EzJkzudbNGMOVK1cgCAJmzZrFPfvixYtgjGH27NncByn48ccfIYoiZs+ezXV5A8CZM2dgNBoxZ84cKJVKrtlZWVmoqqrC448/zj37+PHjqKiowNy5c6FS8V09nzhxAmVlZZg/fz7UajXX7KysLJSWlmLhwoXcs0+dOuXwvG2mocmyjIsXL3LNlCQJZrMZZWVlTskWRdGp2eXl5dyzZVmGyWRCRUWFU7Jv374NURQhyzL3lWBkZCQ6dOjgtG/1lZWVuHjxIvemYzAYAMAp2VVVVU7LrqyshCzLuHTpklOyzWYzLl26xP19otfrnZZdUVEBk8mEy5cvOyVbFEVcvnyZeyMuKytzaraj2kxD8/HxwcmTJ7lmFhcXY8yYMYiPj8f69eu5ZpeUlGDs2LGIjY3Fxo0buWbrdDqMHTsWgwYNwqZNm7hml5eXY/z48ejduze++OILrtmVlZUYP348FAoFtFot15WgWq3GZ599BoVCAR8fH265QPUKVhAExMTEYMeOHVw/8KIoIjExEUqlErt27eL6rV6SJEydOhWiKCItLY3rN29ZljFt2jSUlZVhz549NsPVtRRjDDNnzkRhYSH27t0LX19frtmPPfYYbt68iczMTPj7+3PLBoB58+bh8uXL2LdvHwI4j2G6cOFCnDlzBpmZmQjkfBTxk08+iaNHj2LPnj0I5jxU3S9+8Qt8+eWXDs3bZhqaIAjc/4hGoxEKhQJqtZp7ttlshiAIUKvVaN++PfdvsIIgQKVScc8WBAEKhcIp2SqVyuZbK+9dYO3atbNm8l4mAKBUKtG+fXuuTcdkMkGpVFqzeTYds9kMpVIJWZbRvn17roMZSJJkUzfPLxGyLEOlUlmz/fz8uGUzxqzvw/bt26NdSwfetpPNc5g/xhjUajUEQUBAQAD39ZUlu3379k7JdpRbD31FCCGEOIoaGiGEEK9ADY0QQohXoIZGCCHEK1BDI4QQ4hVc2tAsJySHh4dDEARs377d5vHXX38d/fv3h7+/Pzp06IBJkybh6NGjrimWEEKIW3NpQ9Pr9YiOjsbatWvrfbxv375Ys2YNzp49iwMHDqB79+5ITExEUVFRK1dKCCHE3bn0PLSkpCQkJSU1+PgTTzxhM/3uu+9i/fr1OHPmDCZOnFjvc4xGI4xGo3WaLkFDCCFtg8f8hmYymfDhhx8iMDAQ0dHRDc6XkpKCwMBA6y0vL68VqySEEOIqbt/QduzYgXbt2sHHxwd//etfkZaWho4dOzY4/6pVq6DT6ay38PDwVqyWEEKIq7h9Q5swYQKys7Nx6NAhTJ06FXPnzkVhYWGD82u1WuuwMbyHXiKEEOK+3L6h+fv7o3fv3hg5ciTWr18PlUrFfSBgQgghns/tG1ptsizbHPRBCCGEAC4+yrGiogJXr161Tl+/fh3Z2dkIDg5GSEgI3nrrLcyYMQNhYWEoLi7G2rVrkZubi8cff9yFVRNCCHFHLm1oJ06cwIQJE6zTK1euBAAkJyfjgw8+wMWLF7Fx40YUFxcjJCQEw4cPx/79+zFw4EBXlUwIIcRNubShJSQk2D1PbOvWra1YDSGEEE/mcb+hEUIIIfWhhkYIIcQruHSXY2tz5jBYvLNr53lS7c7Kbo1lYsn0tLqdnc0Y86i6a+bR39L52TU/N64cbrDNNDSj0YjHHnuMe2ZpaSkOHDjAPdtkMuHu3bs4fPgw92xRFFFUVITjx49zzzabzbhz5w7Ky8u5Z0uShNzcXJSWlmLOnDlcs4HqU0IAQKHgu+NClmXcuHEDKpUKjz/+ONeT/WVZxtWrVyEIAubOncs1mzGGy5cvQ5ZlzJs3j+tyYYzhwoULEEUR8+fP577Mz507B4PBgAULFkCpVHLNzs7ORmVlJZ544gmoVHxXoSdPnkR5eTkWLVrEPfvo0aO4d+8elixZArVazTX72LFjuHv3LpYuXQqNRsM1+/jx4w7PKzAvH703MjISubm5EAQBDzzwANdsWZZx584daLVau8NxuVs2Ywx5eXnQaDTo1KkT9+w7d+5ArVY7LVulUqFz587cs/Pz86FQKNClSxeu2QBw584dp2Xn5+cDAEJDQz0qu6CgALIsIywsjHt2YWEhJElCaGgo99GCCgsLYTabERYWxj27qKgIoig6LdtkMiEsLIz7F4ji4mIYjUanZVdWVgIAIiIicPv27QbnbTNbaD4+PsjOzuaaWVxcjLFjxyI+Ph4fffQR1+ySkhKMHz8esbGx2LBhA9dsnU6HcePGYdCgQfjss8+4ZpeXlyMhIQG9e/fG559/zjW7srISEyZMQGRkJL766iuuH/iqqipMmjQJISEh2L59O9cPpdFoxOTJk9GuXTt89913XLcYRFHE1KlToVAosHPnTq7f6s1mMx5++GGYzWbs2rWL6zdvSZIwffp0lJeXY/fu3dBqtdyyZVnGo48+iqKiIqSnp8PX15dbNmMMc+bMwc2bN5GRkQF/f39u2QAwf/58XL58GZmZmQgICOCavWjRIpw5cwb79u1DUFAQ1+xly5bh2LFjyMjIQHBwMNfsZ555Bl999ZVD87aZhiYIAjp06MA1UxRFKBQKqNVq7tmyLEMQBKjVagQFBXFdeQuCYK2bd7ZSqYRSqXRKtkajsambZ9PRarVQKpVQqVTo0KED94ZmyQ4KCuLadEwmk3WZBwUFcd2VZDaboVKpwBhDhw4duDc0lUplrdvHx4dbtizLUKvV1mw/Pz9u2Ywxm+x27do5Lbt9+/Zcsy2fn6CgIO7rK41GA0EQnJbtKDrKkRBCiFeghkYIIcQrUEMjhBDiFaihEUII8QrU0AghhHgFlza0ffv2Yfr06QgPD4cgCNi+fXuD8z777LMQBAHvvfdeq9VHCCHEc7i0oen1ekRHR2Pt2rV259u2bRuOHDmC8PDwVqqMEEKIp3HpeWhJSUlISkqyO09ubi5+9atfITU1FdOmTWs002g02lzR2ssHQiGEEPIzt/4NTZZlLF68GC+99JLDF/VMSUlBYGCg9ZaXl+fkKgkhhLgDt25o77zzDlQqFV544QWHn7Nq1SrodDrrjXZTEkJI2+C2Q1+dPHkS77//PrKyspo0fJJWq7UZF473AJ+EEELck9tuoe3fvx+FhYXo2rUrVCoVVCoVcnJy8Jvf/Abdu3d3dXmEEELcjNtuoS1evBiTJk2yuW/KlClYvHgxnnzySRdVRQghxF25tKFVVFTg6tWr1unr168jOzsbwcHB6Nq1K0JCQmzmV6vVCA0NRb9+/Vq7VEIIIW7OpQ3txIkTmDBhgnV65cqVAIDk5GR8/PHHLqqKEEKIJ3JpQ0tISGjSeWI3btxwXjGEEEI8mtseFEIIIYQ0BTU0QgghXoEaGiGEEK/gtoft82Y0GjFv3jyumQaDAaWlpTh06BD3bKPRiLt37+LIkSOYP38+12yTyYSioiKcOHEC8+bN43ryuSiKyM/Ph16v555tNpuRl5eHe/fuYf78+dyzb968iYKCAu7ZkiQhJycHKpUKCxYsgELB73ukLMu4du0aBEHAE088wT37ypUrkGUZCxcu5JrNGMPFixchiiIWLVoEpVLJNfvcuXMwGAxYvHgxVCp+qznGGE6fPg29Xo/k5GTu2VlZWSgrK8PSpUuhVqu5Zh8/fhylpaVYtmwZNBoNt2wAOHz4MEpKSvDUU0/ZDGzBw5EjRxyeV2BePnpvZGQkcnNzIQgC92GwZFlGQUEBtFotgoODuWcXFhZCo9Fwz2aMoaCgAGq1us6pEe6eXVhYCKVSiY4dO3pMNgAUFhZCEAR06tTJo7KLiorAGEPnzp09Kru4uBiSJKFz587cRwtyZvbdu3chiiK6dOniUdklJSUwmUzo3Lkz1y8+luyqqioAQEREBG7fvt3gvG1mC83HxwfZ2dlcM4uLi5GQkIAxY8bgH//4B9fskpISTJgwAcOGDcO//vUvrtk6nQ4TJkzAwIED8cknn3DNLi8vx8SJE9GrVy9s2rSJ6wensrISEydORGRkJL744guu2VVVVUhMTERISAi2bt3K9UNpNBoxdepU+Pv745tvvuG6NSKKIh5++GEolUrs2LGD6xaD2WzG9OnTIYoivv/+e67f6iVJwsyZM1FRUYHU1FSu3+plWcZjjz2GoqIipKWlwdfXl1s2Ywxz587FrVu3kJ6eDn9/f67ZCxcuxJUrV7Bnzx4EBARwywaAJUuW4OzZs9i7dy+CgoK4Zj/99NM4duwYMjIyuH8Bf/bZZ/H11187NG+baWiCIHD/5i3LMhQKBTQaDfdsQRCgUCig1WoREhLCdeWtUqmsdfPO1mg0UCqV1mXCM1uv11uzQ0JCuDadyspKqFQqqNVqdOzYkXtDUyqV1q1Wnk3HZDJBpVJBqVQiJCSE624qs9kMlUoFxhg6duzIvaGp1Wpr3T4+PtyyZVmGRqOBSqVCSEgI/Pz8uGUzxqzv8ZCQELRr185p2e3bt+eardVqoVAoEBISgg4dOnDLBmCTzXvPTFO+7NBBIYQQQrwCNTRCCCFegRoaIYQQr0ANjRBCiFeghkYIIcQruLSh7du3D9OnT0d4eDgEQcD27dttHl+6dCkEQbC5TZ061TXFEkIIcWsubWh6vR7R0dFYu3Ztg/NMnToVd+7csd42b97cihUSQgjxFC49Dy0pKQlJSUl259FqtQgNDXU402g0wmg0Wqe9fCAUQgghP3P739AyMjLQuXNn9OvXD7/85S9x9+5du/OnpKQgMDDQesvLy2ulSgkhhLiSWze0qVOn4t///jfS09PxzjvvIDMzE0lJSZAkqcHnrFq1CjqdznrjPX4jIYQQ9+TWQ1/VHGV+8ODBiIqKQq9evZCRkYGJEyfW+xytVmszVArvQTgJIYS4J7feQqutZ8+e6NixI65everqUgghhLgZj2pot2/fxt27dxEWFubqUgghhLgZl+5yrKiosNnaun79OrKzsxEcHIzg4GC88cYbeOyxxxAaGopr167ht7/9LXr37o0pU6a4sGpCCCHuyKUN7cSJE5gwYYJ1euXKlQCA5ORkrFu3DmfOnMHGjRtx7949hIeHIzExEW+++Sb3K6ISQgjxfC5taAkJCXbPE0tNTW3FagghhHgyj/oNjRBCCGkINTRCCCFegRoaIYQQr+DWJ1bzZDQasWjRIu6Z9+7dw5EjR7hnm0wmlJSU4NixY1i8eDHXbFEUUVxcjKysLO7ZZrMZBQUFqKys5J4tSRLu3LmDsrIyLFmyhHv2rVu3UFRUxD1blmXcvHkTKpUKycnJXE/2Z4zh+vXrAO5fnYJn9tWrVyHLMpYuXQqFgt/3X8YYLl26BFEUsWzZMq7ZAHD+/HkYDAY89dRTUCqVXLPPnj2LiooKPP3001Cp+K5Cs7OzodPp8Mwzz3DPPnHiBEpLS/Hss89CrVZzzT569ChKS0vxy1/+EhqNhmv2oUOHHJ5XYF4+eq9Go4Eoiq4ugxBCSAup1WqYTKYGH/f6XY72xn0khBDiORpbn3v9LkcfHx8YDAYolUp07ty50fkZY8jLy7NedJQnyqZsyqZsym56dmFhISRJgo+Pj935vH6XY1OVlZUhMDAQOp0O7du3p2zKpmzKpmwPyfb6XY6EEELaBmpohBBCvAI1tFq0Wi1ee+01p4wXSdmUTdmUTdnOy6bf0AghhHgF2kIjhBDiFaihEUII8QrU0AghhHgFamiEEEK8AjW0WtauXYvu3bvDx8cHcXFxOHbsWIsz9+3bh+nTp1vPjN++fXvLC/1ZSkoKhg8fjoCAAHTu3BmzZs3CpUuXuGSvW7cOUVFRaN++Pdq3b49Ro0Zh586dXLJre/vttyEIAlasWNHirNdffx2CINjc+vfv3/Iif5abm4tFixYhJCQEvr6+GDx4ME6cONHi3O7du9epWxAELF++vMXZkiRh9erV6NGjB3x9fdGrVy+8+eabdi+w2xTl5eVYsWIFunXrBl9fX4wePRrHjx9vck5jnxXGGF599VWEhYXB19cXkyZNwpUrV7hkb926FYmJiQgJCYEgCMjOzuZStyiKePnllzF48GD4+/sjPDwcS5YsQV5eHpe6X3/9dfTv3x/+/v7o0KEDJk2ahKNHj3LJrunZZ5+FIAh47733uGRbBtOueZs6dapD2Q2hhlbD559/jpUrV+K1115DVlYWoqOjMWXKFBQWFrYoV6/XIzo6GmvXruVU6X2ZmZlYvnw5jhw5grS0NIiiiMTEROj1+hZnR0ZG4u2338bJkydx4sQJPPTQQ5g5cybOnz/PofL7jh8/jn/84x+Iioriljlw4EDcuXPHejtw4ACX3NLSUowZMwZqtRo7d+7Ejz/+iL/85S/o0KFDi7OPHz9uU3NaWhoA4PHHH29x9jvvvIN169ZhzZo1uHDhAt555x38+c9/xt///vcWZwPA008/jbS0NHzyySc4e/YsEhMTMWnSJOTm5jYpp7HPyp///Gf87W9/wwcffICjR4/C398fU6ZMgcFgaHG2Xq9HfHw83nnnnSbV3Fh2ZWUlsrKysHr1amRlZWHr1q24dOkSZsyY0eJsAOjbty/WrFmDs2fP4sCBA+jevTsSExNRVFTU4myLbdu24ciRIwgPD3eoZkezp06davOe37x5s8P59WLEasSIEWz58uXWaUmSWHh4OEtJSeH2GgDYtm3buOXVVlhYyACwzMxMp+R36NCB/fOf/+SWV15ezvr06cPS0tLY+PHj2YsvvtjizNdee41FR0e3OKc+L7/8MouPj3dKdm0vvvgi69WrF5NlucVZ06ZNY8uWLbO5b/bs2WzhwoUtzq6srGRKpZLt2LHD5v6hQ4ey3//+983Orf1ZkWWZhYaGsv/93/+13nfv3j2m1WrZ5s2bW5Rd0/Xr1xkAdurUqWZU7dhn/NixYwwAy8nJ4Z6t0+kYALZ7924u2bdv32YRERHs3LlzrFu3buyvf/1rk3Ibyk5OTmYzZ85scpY9tIX2M5PJhJMnT2LSpEnW+xQKBSZNmoTDhw+7sLKm0el0AIDg4GCuuZIkYcuWLdDr9Rg1ahS33OXLl2PatGk2y52HK1euIDw8HD179sTChQtx8+ZNLrnffvstYmNj8fjjj6Nz586IiYnBRx99xCW7JpPJhE8//RTLli3jMjjs6NGjkZ6ejsuXLwMATp8+jQMHDiApKanF2Wazud6BY319fbltGQPA9evXkZ+fb/NeCQwMRFxcnEd9RoHqz6kgCAgKCuKaazKZ8OGHHyIwMBDR0dEtzpNlGYsXL8ZLL72EgQMHcqjQVkZGBjp37ox+/frhl7/8Je7evduiPK8fbd9RxcXFkCQJXbp0sbm/S5cuuHjxoouqahpZlrFixQqMGTMGgwYN4pJ59uxZjBo1CgaDAe3atcO2bdvw4IMPcsnesmULsrKymvVbiz1xcXH4+OOP0a9fP9y5cwdvvPEGxo4di3PnziEgIKBF2T/99BPWrVuHlStX4ne/+x2OHz+OF154ARqNBsnJyZz+D4Dt27fj3r17WLp0KZe8V155BWVlZejfvz+USiUkScJbb72FhQsXtjg7ICAAo0aNwptvvokBAwagS5cu2Lx5Mw4fPozevXtzqL5afn4+ANT7GbU85gkMBgNefvllLFiwgNvgvDt27MD8+fNRWVmJsLAwpKWloWPHji3Ofeedd6BSqfDCCy9wqNLW1KlTMXv2bPTo0QPXrl3D7373OyQlJeHw4cPNvigrNTQvsnz5cpw7d47rt+J+/fpZr6L71VdfITk5GZmZmS1uardu3cKLL76ItLS0Ri8J0VQ1tzqioqIQFxeHbt264YsvvsBTTz3VomxZlhEbG4s//elPAICYmBicO3cOH3zwAdeGtn79eiQlJTXpNwt7vvjiC3z22WfYtGkTBg4ciOzsbKxYsQLh4eFc6v7kk0+wbNkyREREQKlUYujQoViwYAFOnjzJoXrvIYoi5s6dC8YY1q1bxy13woQJyM7ORnFxMT766CPMnTsXR48edeiSWQ05efIk3n//fWRlZXG/hAwAzJ8/3/rvwYMHIyoqCr169UJGRgYmTpzYrEza5fizjh07QqlUoqCgwOb+goIChIaGuqgqxz3//PPYsWMH9u7di8jISG65Go0GvXv3xrBhw5CSkoLo6Gi8//77Lc49efIkCgsLMXToUKhUKqhUKmRmZuJvf/sbVCoV1wuzBgUFoW/fvrh69WqLs8LCwuo08wEDBnDbpQkAOTk52L17N55++mlumS+99BJeeeUVzJ8/H4MHD8bixYvx61//GikpKVzye/XqhczMTFRUVODWrVs4duwYRFFEz549ueQDsH4OPfUzamlmOTk5SEtL43rpFH9/f/Tu3RsjR47E+vXroVKpsH79+hZl7t+/H4WFhejatav1M5qTk4Pf/OY36N69O5/Ca+jZsyc6duzYos8pNbSfaTQaDBs2DOnp6db7ZFlGeno619+MeGOM4fnnn8e2bduwZ88e9OjRw6mvJ8syjEZji3MmTpyIs2fPIjs723qLjY3FwoULkZ2d3exdDvWpqKjAtWvXEBYW1uKsMWPG1Dkt4vLly+jWrVuLsy02bNiAzp07Y9q0adwyKysroVDYftyVSiVkWeb2GkD1ijUsLAylpaVITU3FzJkzuWX36NEDoaGhNp/RsrIyHD161K0/o8D9ZnblyhXs3r0bISEhTn09Hp/TxYsX48yZMzaf0fDwcLz00ktITU3lVOl9t2/fxt27d1v0OaVdjjWsXLkSycnJiI2NxYgRI/Dee+9Br9fjySefbFFuRUWFzbeO69evIzs7G8HBwejatWuLspcvX45Nmzbhm2++QUBAgPW3hMDAQPj6+rYoe9WqVUhKSkLXrl1RXl6OTZs2ISMjg8ubOSAgoM7vfP7+/ggJCWnx73///d//jenTp6Nbt27Iy8vDa6+9BqVSiQULFrQoFwB+/etfY/To0fjTn/6EuXPn4tixY/jwww/x4YcftjgbqF4RbdiwAcnJyVCp+H08p0+fjrfeegtdu3bFwIEDcerUKbz77rtYtmwZl/zU1FQwxtCvXz9cvXoVL730Evr379/kz05jn5UVK1bgj3/8I/r06YMePXpg9erVCA8Px6xZs1qcXVJSgps3b1rPD7N8cQkNDW10C9BedlhYGObMmYOsrCzs2LEDkiRZP6fBwcHQaDTNzg4JCcFbb72FGTNmICwsDMXFxVi7di1yc3MdOt2jsWVSu/Gq1WqEhoaiX79+LcoODg7GG2+8gcceewyhoaG4du0afvvb36J3796YMmVKo9kN4nrMpBf4+9//zrp27co0Gg0bMWIEO3LkSIsz9+7dywDUuSUnJ7c4u75cAGzDhg0tzl62bBnr1q0b02g0rFOnTmzixInshx9+aHFuQ3gdtj9v3jwWFhbGNBoNi4iIYPPmzWNXr15teYE/++6779igQYOYVqtl/fv3Zx9++CG37NTUVAaAXbp0iVsmY4yVlZWxF198kXXt2pX5+Piwnj17st///vfMaDRyyf/8889Zz549mUajYaGhoWz58uXs3r17Tc5p7LMiyzJbvXo169KlC9NqtWzixIkOL6vGsjds2FDv46+99lqLsi2nAdR327t3b4uyq6qq2KOPPsrCw8OZRqNhYWFhbMaMGezYsWNclkltTTls3152ZWUlS0xMZJ06dWJqtZp169aN/eIXv2D5+fkOZTeELh9DCCHEK9BvaIQQQrwCNTRCCCFegRoaIYQQr0ANjRBCiFeghkYIIcQrUEMjhBDiFaihEUII8QrU0AghhHgFamiEeLmlS5c6NDQUIZ6OGhohhBCvQA2NEEKIV6CGRogHkGUZf/7zn9G7d29otVp07doVb731FoDqq4o/9NBD8PX1RUhICJ555hlUVFS4uGJCWh81NEI8wKpVq/D2229j9erV+PHHH7Fp0yZ06dIFer0eU6ZMQYcOHXD8+HF8+eWX2L17N55//nlXl0xIq6PR9glxc+Xl5ejUqRPWrFlT5yrWH330EV5++WXcunUL/v7+AIDvv/8e06dPR15eHrp06YKlS5fi3r172L59uwuqJ6T10BYaIW7uwoULMBqNmDhxYr2PRUdHW5sZUH1VbVmW61xZmxBvRw2NEDfX0iuPE9JWUEMjxM316dMHvr6+SE9Pr/PYgAEDcPr0aej1eut9Bw8ehEKhQL9+/VqzTEJcjhoaIW7Ox8cHL7/8Mn7729/i3//+N65du4YjR45g/fr1WLhwIXx8fJCcnIxz585h7969+NWvfoXFixejS5curi6dkFalcnUBhJDGrV69GiqVCq+++iry8vIQFhaGZ599Fn5+fkhNTcWLL76I4cOHw8/PD4899hjeffddV5dMSKujoxwJIYR4BdrlSAghxCtQQyOEEOIVqKERQgjxCtTQCCGEeAVqaIQQQrwCNTRCCCFegRoaIYQQr0ANjRBCiFeghkYIIcQrUEMjhBDiFaihEUII8Qr/H7U+Wg5Axll7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_image(image, target_size=32):\n",
    "    image = np.array(image)\n",
    "\n",
    "    # scale_factor = target_size // image.shape[0]\n",
    "\n",
    "    # image = np.kron(image, np.ones((scale_factor, scale_factor, 1)))\n",
    "\n",
    "    image = image.astype(np.float32) / 127.5 - 1\n",
    "    image = torch.tensor(image).permute(2, 0, 1)\n",
    "\n",
    "    return image\n",
    "\n",
    "def remove_sol(dataset):\n",
    "    new_dataset = []\n",
    "    for i in dataset:\n",
    "        img = i.as_pixels()\n",
    "        mask = np.all(img == [0, 0, 255], axis=-1)\n",
    "        img[mask] = [255, 255, 255]\n",
    "        img = img[:-1, :-1]\n",
    "        new_dataset.append(preprocess_image(img))\n",
    "\n",
    "        # new_dataset.append(img)\n",
    "    return new_dataset\n",
    "\n",
    "new_dataset = remove_sol(dataset)\n",
    "plt.imshow(new_dataset[0].permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "MazePlot(dataset[0]).plot()\n",
    "\n",
    "class MazeTensorDataset(Dataset):\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx]\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = MazeTensorDataset(new_dataset)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "calibration_indices = random.sample(range(len(dataset)), 500)\n",
    "calibration_subset = Subset(dataset, calibration_indices)\n",
    "\n",
    "# # Calibration loader\n",
    "# calibration_loader = DataLoader(\n",
    "#     calibration_subset,\n",
    "#     batch_size=16,\n",
    "#     shuffle=False,\n",
    "#     num_workers=2\n",
    "# )                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "453560bfc4734c158f8664b80c42ac1b",
      "806f4a41f38f44cd88279aaefc496b56",
      "d010ecd775d54b6b8462a0bc73bf7f14",
      "48b544078905483688a3d6b3787d6dbf",
      "024f107c67954608b3f7545279bddc3f",
      "7961d4426aa34604a7cdee2b81fdc87d",
      "090874af3ec148289902c1b67f96bbf2",
      "b70b5d7d71a2493789b44da0df5b5aa7",
      "c4ccd5e0700e437f8cc019b5ae976c8b",
      "075d902a8232445b98f1c05b3d390385",
      "bbbb1d04501c49bebcc238f14159e2bb"
     ]
    },
    "id": "rSQORjcZ05yd",
    "outputId": "6df01e4c-cc39-4c2e-d358-dbeb0ac97e09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models, transforms\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cbCX6Qdk3bvl"
   },
   "outputs": [],
   "source": [
    "class VGG16Encoder(nn.Module):\n",
    "    def __init__(self, latent_shape=(4, 32, 32), freeze=True):\n",
    "        super(VGG16Encoder, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        self.features = vgg16.features\n",
    "        self.latent_shape = latent_shape\n",
    "        latent_dim = latent_shape[0] * latent_shape[1] * latent_shape[2]\n",
    "\n",
    "        if freeze:\n",
    "            for param in self.features.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        self.fc = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # (batch, 512, H', W')\n",
    "        x = self.pool(x)     # (batch, 512, 4, 4)\n",
    "        x = x.view(x.size(0), -1)  # Flatten: (batch, 512*4*4)\n",
    "        x = self.fc(x)       # (batch, latent_dim)\n",
    "        x = x.view(x.size(0), *self.latent_shape)  # (batch, 4, 32, 32)  --> Match decoder input\n",
    "        return x\n",
    "\n",
    "\n",
    "# class TinyEncoder(nn.Module):\n",
    "#     def __init__(self, latent_shape=(4, 32, 32)):\n",
    "#         super(TinyEncoder, self).__init__()\n",
    "#         self.latent_shape = latent_shape\n",
    "#         latent_dim = int(np.prod(latent_shape))\n",
    "\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv2d(3, 32, 4, 2, 1), nn.ReLU(),  # Downsample 2x\n",
    "#             nn.Conv2d(32, 64, 4, 2, 1), nn.ReLU(), # Downsample 2x\n",
    "#             nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(), # Downsample 2x\n",
    "#             nn.Conv2d(128, 256, 4, 2, 1), nn.ReLU(), # Downsample 2x\n",
    "#         )\n",
    "#         self.fc = nn.Linear(256 * 16 * 16, latent_dim)  # Assuming input 256x256 -> 16x16 feature map\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc(x)\n",
    "#         x = x.view(x.size(0), *self.latent_shape)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16QuantizedEncoder(nn.Module):\n",
    "    def __init__(self, latent_shape=(4, 32, 32)):\n",
    "        super(VGG16QuantizedEncoder, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "        # Keep feature extractor\n",
    "        self.features = vgg16.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4, 4))  # Downsampling\n",
    "\n",
    "        latent_dim = latent_shape[0] * latent_shape[1] * latent_shape[2]\n",
    "        self.fc = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "\n",
    "        self.latent_shape = latent_shape\n",
    "\n",
    "        # Quantization stubs (needed for static quantization)\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)             # Quantize input\n",
    "        x = self.features(x)          # VGG16 features\n",
    "        x = self.pool(x)              # Adaptive pooling\n",
    "        x = x.view(x.size(0), -1)     # Flatten\n",
    "        x = self.fc(x)                # FC layer to latent\n",
    "        x = x.view(x.size(0), *self.latent_shape)  # Reshape to latent shape\n",
    "        x = self.dequant(x)           # Dequantize output\n",
    "        return x\n",
    "\n",
    "    def fuse_model(self):\n",
    "        # Fuse Conv + ReLU to improve quantization efficiency\n",
    "        for idx in range(len(self.features)):\n",
    "            if isinstance(self.features[idx], nn.Conv2d) and isinstance(self.features[idx + 1], nn.ReLU):\n",
    "                torch.quantization.fuse_modules(self.features, [str(idx), str(idx + 1)], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_quantized_encoder(device='cpu'):\n",
    "    # Instantiate and prepare model\n",
    "    encoder = VGG16QuantizedEncoder().to(device)\n",
    "    encoder.eval()\n",
    "\n",
    "    # Fuse Conv + ReLU\n",
    "    encoder.fuse_model()\n",
    "\n",
    "    encoder.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "    # Prepare for quantization\n",
    "    encoder_prepared = torch.quantization.prepare(encoder, inplace=False)\n",
    "\n",
    "    return encoder_prepared\n",
    "\n",
    "def calibrate_encoder(encoder_prepared, calibration_loader, device='cpu'):\n",
    "    encoder_prepared.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in calibration_loader:\n",
    "            x = batch.to(device)\n",
    "            _ = encoder_prepared(x)  # Forward pass to calibrate\n",
    "\n",
    "def convert_quantized_encoder(encoder_prepared):\n",
    "    # Convert to fully quantized model\n",
    "    encoder_int8 = torch.quantization.convert(encoder_prepared, inplace=False)\n",
    "    return encoder_int8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7GG43tOp5_1S"
   },
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "\n",
    "class MazeVAEDecoder(nn.Module):\n",
    "    def __init__(self, pretrained_model='CompVis/stable-diffusion-v1-4', device='cuda'):\n",
    "        super(MazeVAEDecoder, self).__init__()\n",
    "        self.vae = AutoencoderKL.from_pretrained(pretrained_model, subfolder=\"vae\",torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "    def forward(self, latent_vector):\n",
    "        return self.vae.decode(latent_vector).sample  # Output in image space\n",
    "\n",
    "# from diffusers import AutoencoderKL\n",
    "\n",
    "# class MazeVAEDecoder(nn.Module):\n",
    "#     def __init__(self, pretrained_model='CompVis/stable-diffusion-v1-4'):\n",
    "#         super(MazeVAEDecoder, self).__init__()\n",
    "#         self.vae = AutoencoderKL.from_pretrained(\n",
    "#             pretrained_model, \n",
    "#             subfolder=\"vae\", \n",
    "#             torch_dtype=torch.float16\n",
    "#         ).cpu()  # Load to CPU to save VRAM\n",
    "\n",
    "#     def forward(self, latent_vector):\n",
    "#         with torch.no_grad():  # No gradients needed for decoder\n",
    "#             return self.vae.decode(latent_vector).sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309,
     "referenced_widgets": [
      "a4c0efc6bc2945b09cb1f11bb29876af",
      "460f2a5d23f0488cbeb6d97aed65f70b",
      "73b3c5a3490a4ecd87854e642e933850",
      "6cd0d360e04a46938d25d904d2f58246",
      "7b24cf86f57b459b92958b6b797c313b",
      "67425a5431cc42ef958288bf8b0db755",
      "ba2379fe5c6f4834958b055766674647",
      "d4cb7e506f894f358d93023203e8501c",
      "1b8f3862c8a8468ca9ac26424767c5a0",
      "5b33eeed425044b399952811b853f108",
      "2d75247e1da44b3ca2e7819ce8a76f05",
      "680073603b3140549632c746129476fe",
      "c1d23f5433b24901ae863291620f6477",
      "1af5f7700ec449bdbff5d4d0eca8d989",
      "55f1259d8d9d4d9fab1d0291a82c3fa1",
      "50d4ff02c7d44beb9b689dfeab1ef0b1",
      "37da6d18d17b4d8ea096084485cb11ae",
      "2335f1ad270e436a88ab861c72c2a2e4",
      "0a70d34895db433284c272c2e1031bd2",
      "a86a88a1625e4e63bc4b7dd51b375153",
      "84be52f7d2154af3a4ff9dacf6764998",
      "b6a3846c2f8c4b08939e75092ec2ecfa"
     ]
    },
    "id": "BQovmyer6Dsv",
    "outputId": "e8901a10-6e73-46ce-a619-32d5788743ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47099/3107256328.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Encoder and Decoder\n",
    "encoder = VGG16Encoder(latent_shape=(4, 32, 32), freeze=True).to(device)\n",
    "# encoder_prepared = prepare_quantized_encoder(device=device)\n",
    "# #calibrate_encoder(encoder_prepared, calibration_loader, device=device)  # Optional but recommended\n",
    "# encoder_quantized = convert_quantized_encoder(encoder_prepared).to(device)\n",
    "\n",
    "decoder = MazeVAEDecoder(pretrained_model='CompVis/stable-diffusion-v1-4', device=device).to(device)\n",
    "\n",
    "for param in decoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(list(encoder.parameters()), lr=1e-4)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Loss\n",
    "def reconstruction_loss(x, x_hat):\n",
    "    return nn.functional.mse_loss(x_hat, x, reduction='sum') / x.size(0)\n",
    "\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "# import gc\n",
    "\n",
    "# # --- Hyperparams ---\n",
    "# EPOCHS = 20\n",
    "# ACCUMULATION_STEPS = 4  # Effective batch size multiplier\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# latent_shape = (4, 32, 32)\n",
    "\n",
    "# encoder = TinyEncoder(latent_shape=latent_shape).to(device)\n",
    "# decoder = MazeVAEDecoder(pretrained_model='CompVis/stable-diffusion-v1-4')  # On CPU\n",
    "\n",
    "# optimizer = optim.Adam(encoder.parameters(), lr=1e-4)\n",
    "# scaler = GradScaler()\n",
    "\n",
    "# def reconstruction_loss(x, x_hat):\n",
    "#     return nn.functional.mse_loss(x_hat, x, reduction='sum') / x.size(0)\n",
    "\n",
    "# train_losses = []\n",
    "# for epoch in range(EPOCHS):\n",
    "#     encoder.train()\n",
    "#     train_loss = 0.0\n",
    "\n",
    "#     for batch_idx, x in enumerate(dataloader):  # Assuming you have a DataLoader\n",
    "#         x = x.to(device)\n",
    "#         batch_size = x.size(0)\n",
    "\n",
    "#         with autocast(dtype=torch.float16):\n",
    "#             latent = encoder(x)               # Encode\n",
    "#             x_hat = decoder(latent).to(device)  # Decode (move back to GPU for loss)\n",
    "\n",
    "#             loss = reconstruction_loss(x, x_hat) / ACCUMULATION_STEPS  # Normalize for accumulation\n",
    "\n",
    "#         scaler.scale(loss).backward()\n",
    "\n",
    "#         # Gradient accumulation\n",
    "#         if (batch_idx + 1) % ACCUMULATION_STEPS == 0:\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#         train_loss += loss.item() * batch_size * ACCUMULATION_STEPS  # Un-normalize for reporting\n",
    "\n",
    "#     # Average loss\n",
    "#     train_loss /= len(dataloader.dataset)\n",
    "#     train_losses.append(train_loss)\n",
    "#     print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "#     # --- Clean up memory ---\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 13 01:55:27 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-PCIE-16GB           Off |   00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   45C    P0             40W /  250W |    1106MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     47099      C   /usr/bin/python                              1102MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "Q6-_LNbw6I7J",
    "outputId": "1951b144-d0f7-4283-b898-f9bfc46a0952"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47099/3523259055.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Mixed precision\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 254.19 MiB is free. Including non-PyTorch memory, this process has 15.51 GiB memory in use. Of the allocated memory 14.49 GiB is allocated by PyTorch, and 647.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():  \u001b[38;5;66;03m# Mixed precision\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     latent \u001b[38;5;241m=\u001b[39m encoder(x)                \u001b[38;5;66;03m# (batch, 4, 32, 32)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     x_hat \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m)\u001b[49m            \u001b[38;5;66;03m# (batch, 3, 256, 256)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Option 1: Downsample x_hat to match x\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# x_hat_resized = torch.nn.functional.interpolate(x_hat, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=False)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Option 2: Upsample x to match x_hat (Recommended)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     x_resized \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39minterpolate(x, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m, in \u001b[0;36mMazeVAEDecoder.forward\u001b[0;34m(self, latent_vector)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, latent_vector):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_vector\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/utils/accelerate_utils.py:46\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hf_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:327\u001b[0m, in \u001b[0;36mAutoencoderKL.decode\u001b[0;34m(self, z, return_dict, generator)\u001b[0m\n\u001b[1;32m    325\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(decoded_slices)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (decoded,)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:298\u001b[0m, in \u001b[0;36mAutoencoderKL._decode\u001b[0;34m(self, z, return_dict)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_quant_conv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_quant_conv(z)\n\u001b[0;32m--> 298\u001b[0m dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dec,)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/autoencoders/vae.py:350\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, sample, latent_embeds)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# up\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m up_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_blocks:\n\u001b[0;32m--> 350\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[43mup_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# post-process\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m latent_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2813\u001b[0m, in \u001b[0;36mUpDecoderBlock2D.forward\u001b[0;34m(self, hidden_states, temb)\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, temb: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m   2812\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m resnet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnets:\n\u001b[0;32m-> 2813\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2816\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/resnet.py:327\u001b[0m, in \u001b[0;36mResnetBlock2D.forward\u001b[0;34m(self, input_tensor, temb, *args, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m     deprecate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m, deprecation_message)\n\u001b[1;32m    325\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m input_tensor\n\u001b[0;32m--> 327\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnonlinearity(hidden_states)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;66;03m# upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313\u001b[0m, in \u001b[0;36mGroupNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2965\u001b[0m, in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2958\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2959\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2960\u001b[0m     )\n\u001b[1;32m   2961\u001b[0m _verify_batch_size(\n\u001b[1;32m   2962\u001b[0m     [\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_groups, num_groups]\n\u001b[1;32m   2963\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m   2964\u001b[0m )\n\u001b[0;32m-> 2965\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2966\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2967\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 254.19 MiB is free. Including non-PyTorch memory, this process has 15.51 GiB memory in use. Of the allocated memory 14.49 GiB is allocated by PyTorch, and 647.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, x in enumerate(dataloader):\n",
    "        x = x.to(device)\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # Mixed precision\n",
    "            latent = encoder(x)                # (batch, 4, 32, 32)\n",
    "            x_hat = decoder(latent)            # (batch, 3, 256, 256)\n",
    "\n",
    "            # x_hat_resized = torch.nn.functional.interpolate(x_hat, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=False)\n",
    "\n",
    "            x_resized = torch.nn.functional.interpolate(x, size=(256, 256), mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = reconstruction_loss(x_resized, x_hat)\n",
    "\n",
    "        # Backprop with scaler\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item() * batch_size\n",
    "\n",
    "    train_loss /= len(dataloader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "024f107c67954608b3f7545279bddc3f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "075d902a8232445b98f1c05b3d390385": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "090874af3ec148289902c1b67f96bbf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0a70d34895db433284c272c2e1031bd2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1af5f7700ec449bdbff5d4d0eca8d989": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a70d34895db433284c272c2e1031bd2",
      "max": 334643276,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a86a88a1625e4e63bc4b7dd51b375153",
      "value": 334643276
     }
    },
    "1b8f3862c8a8468ca9ac26424767c5a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2335f1ad270e436a88ab861c72c2a2e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2d75247e1da44b3ca2e7819ce8a76f05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "37da6d18d17b4d8ea096084485cb11ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "453560bfc4734c158f8664b80c42ac1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_806f4a41f38f44cd88279aaefc496b56",
       "IPY_MODEL_d010ecd775d54b6b8462a0bc73bf7f14",
       "IPY_MODEL_48b544078905483688a3d6b3787d6dbf"
      ],
      "layout": "IPY_MODEL_024f107c67954608b3f7545279bddc3f"
     }
    },
    "460f2a5d23f0488cbeb6d97aed65f70b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67425a5431cc42ef958288bf8b0db755",
      "placeholder": "​",
      "style": "IPY_MODEL_ba2379fe5c6f4834958b055766674647",
      "value": "config.json: 100%"
     }
    },
    "48b544078905483688a3d6b3787d6dbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_075d902a8232445b98f1c05b3d390385",
      "placeholder": "​",
      "style": "IPY_MODEL_bbbb1d04501c49bebcc238f14159e2bb",
      "value": " 0/0 [00:00&lt;?, ?it/s]"
     }
    },
    "50d4ff02c7d44beb9b689dfeab1ef0b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55f1259d8d9d4d9fab1d0291a82c3fa1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84be52f7d2154af3a4ff9dacf6764998",
      "placeholder": "​",
      "style": "IPY_MODEL_b6a3846c2f8c4b08939e75092ec2ecfa",
      "value": " 335M/335M [00:04&lt;00:00, 87.0MB/s]"
     }
    },
    "5b33eeed425044b399952811b853f108": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67425a5431cc42ef958288bf8b0db755": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "680073603b3140549632c746129476fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c1d23f5433b24901ae863291620f6477",
       "IPY_MODEL_1af5f7700ec449bdbff5d4d0eca8d989",
       "IPY_MODEL_55f1259d8d9d4d9fab1d0291a82c3fa1"
      ],
      "layout": "IPY_MODEL_50d4ff02c7d44beb9b689dfeab1ef0b1"
     }
    },
    "6cd0d360e04a46938d25d904d2f58246": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b33eeed425044b399952811b853f108",
      "placeholder": "​",
      "style": "IPY_MODEL_2d75247e1da44b3ca2e7819ce8a76f05",
      "value": " 551/551 [00:00&lt;00:00, 14.8kB/s]"
     }
    },
    "73b3c5a3490a4ecd87854e642e933850": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4cb7e506f894f358d93023203e8501c",
      "max": 551,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1b8f3862c8a8468ca9ac26424767c5a0",
      "value": 551
     }
    },
    "7961d4426aa34604a7cdee2b81fdc87d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b24cf86f57b459b92958b6b797c313b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "806f4a41f38f44cd88279aaefc496b56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7961d4426aa34604a7cdee2b81fdc87d",
      "placeholder": "​",
      "style": "IPY_MODEL_090874af3ec148289902c1b67f96bbf2",
      "value": ""
     }
    },
    "84be52f7d2154af3a4ff9dacf6764998": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4c0efc6bc2945b09cb1f11bb29876af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_460f2a5d23f0488cbeb6d97aed65f70b",
       "IPY_MODEL_73b3c5a3490a4ecd87854e642e933850",
       "IPY_MODEL_6cd0d360e04a46938d25d904d2f58246"
      ],
      "layout": "IPY_MODEL_7b24cf86f57b459b92958b6b797c313b"
     }
    },
    "a86a88a1625e4e63bc4b7dd51b375153": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b6a3846c2f8c4b08939e75092ec2ecfa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b70b5d7d71a2493789b44da0df5b5aa7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "ba2379fe5c6f4834958b055766674647": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bbbb1d04501c49bebcc238f14159e2bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1d23f5433b24901ae863291620f6477": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37da6d18d17b4d8ea096084485cb11ae",
      "placeholder": "​",
      "style": "IPY_MODEL_2335f1ad270e436a88ab861c72c2a2e4",
      "value": "diffusion_pytorch_model.safetensors: 100%"
     }
    },
    "c4ccd5e0700e437f8cc019b5ae976c8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d010ecd775d54b6b8462a0bc73bf7f14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b70b5d7d71a2493789b44da0df5b5aa7",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c4ccd5e0700e437f8cc019b5ae976c8b",
      "value": 0
     }
    },
    "d4cb7e506f894f358d93023203e8501c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
